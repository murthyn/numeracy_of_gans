{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90 92 98 98 99]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "a= np.random.randint(90, 100, [5])\n",
    "a.sort()\n",
    "print(a)\n",
    "a = np.eye(10)\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/xtrain32.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-02eaa9104440>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/xtrain32.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/ytrain.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/xtrain32.npy'"
     ]
    }
   ],
   "source": [
    "x = np.load(\"data/xtrain32.npy\")\n",
    "y = np.load(\"data/ytrain.npy\")\n",
    "print(x[0])\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(x)\n",
    "y = torch.Tensor(y).to(torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5, dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers [1, 22, 23, 26, 29, 33, 35, 37, 42, 48]\n",
      "[Epoch 0/200] [Batch 0/4688] [D loss: 0.000009] [G loss: -0.000044]\n",
      "[Epoch 0/200] [Batch 1000/4688] [D loss: -58.517069] [G loss: -45.360132]\n",
      "[Epoch 0/200] [Batch 2000/4688] [D loss: -2.980305] [G loss: -35.347118]\n",
      "[Epoch 0/200] [Batch 3000/4688] [D loss: -0.780441] [G loss: -10.823278]\n",
      "[Epoch 0/200] [Batch 4000/4688] [D loss: -0.362927] [G loss: -16.931922]\n",
      "[Epoch 1/200] [Batch 0/4688] [D loss: -0.268649] [G loss: -11.936668]\n",
      "[Epoch 1/200] [Batch 1000/4688] [D loss: -0.359299] [G loss: -17.154287]\n",
      "[Epoch 1/200] [Batch 2000/4688] [D loss: -0.231548] [G loss: -14.114906]\n",
      "[Epoch 1/200] [Batch 3000/4688] [D loss: -0.216784] [G loss: -13.302336]\n",
      "[Epoch 1/200] [Batch 4000/4688] [D loss: -0.254355] [G loss: -13.053647]\n",
      "[Epoch 2/200] [Batch 0/4688] [D loss: -0.227354] [G loss: -8.842430]\n",
      "[Epoch 2/200] [Batch 1000/4688] [D loss: -0.320079] [G loss: -15.349561]\n",
      "[Epoch 2/200] [Batch 2000/4688] [D loss: -0.281836] [G loss: -13.649983]\n",
      "[Epoch 2/200] [Batch 3000/4688] [D loss: -0.312349] [G loss: -11.812738]\n",
      "[Epoch 2/200] [Batch 4000/4688] [D loss: -0.271535] [G loss: -12.381623]\n",
      "[Epoch 3/200] [Batch 0/4688] [D loss: -0.180237] [G loss: -7.914085]\n",
      "[Epoch 3/200] [Batch 1000/4688] [D loss: -0.302066] [G loss: -9.120726]\n",
      "[Epoch 3/200] [Batch 2000/4688] [D loss: -0.306291] [G loss: -9.737240]\n",
      "[Epoch 3/200] [Batch 3000/4688] [D loss: -0.352845] [G loss: -6.791870]\n",
      "[Epoch 3/200] [Batch 4000/4688] [D loss: -0.369999] [G loss: -3.496843]\n",
      "[Epoch 4/200] [Batch 0/4688] [D loss: -0.237643] [G loss: -5.079477]\n",
      "[Epoch 4/200] [Batch 1000/4688] [D loss: -0.384359] [G loss: -9.123808]\n",
      "[Epoch 4/200] [Batch 2000/4688] [D loss: -0.379706] [G loss: -6.420037]\n",
      "[Epoch 4/200] [Batch 3000/4688] [D loss: -0.405134] [G loss: -7.385729]\n",
      "[Epoch 4/200] [Batch 4000/4688] [D loss: -0.509862] [G loss: -1.989799]\n",
      "[Epoch 5/200] [Batch 0/4688] [D loss: -0.303319] [G loss: -0.273950]\n",
      "[Epoch 5/200] [Batch 1000/4688] [D loss: -0.871503] [G loss: 0.846965]\n",
      "[Epoch 5/200] [Batch 2000/4688] [D loss: -1.031323] [G loss: -0.886749]\n",
      "[Epoch 5/200] [Batch 3000/4688] [D loss: -0.616960] [G loss: -0.972566]\n",
      "[Epoch 5/200] [Batch 4000/4688] [D loss: -0.934121] [G loss: 0.633559]\n",
      "[Epoch 6/200] [Batch 0/4688] [D loss: -0.816469] [G loss: 0.073839]\n",
      "[Epoch 6/200] [Batch 1000/4688] [D loss: -0.743724] [G loss: -0.226223]\n",
      "[Epoch 6/200] [Batch 2000/4688] [D loss: -1.227847] [G loss: 0.253932]\n",
      "[Epoch 6/200] [Batch 3000/4688] [D loss: -1.565815] [G loss: -0.947165]\n",
      "[Epoch 6/200] [Batch 4000/4688] [D loss: -0.795605] [G loss: -5.544619]\n",
      "[Epoch 7/200] [Batch 0/4688] [D loss: -0.283509] [G loss: -4.426877]\n",
      "[Epoch 7/200] [Batch 1000/4688] [D loss: -0.714483] [G loss: -5.163910]\n",
      "[Epoch 7/200] [Batch 2000/4688] [D loss: -1.062673] [G loss: -1.702108]\n",
      "[Epoch 7/200] [Batch 3000/4688] [D loss: -0.952787] [G loss: -1.895166]\n",
      "[Epoch 7/200] [Batch 4000/4688] [D loss: -0.898676] [G loss: -2.412816]\n",
      "[Epoch 8/200] [Batch 0/4688] [D loss: -0.616875] [G loss: -2.032873]\n",
      "[Epoch 8/200] [Batch 1000/4688] [D loss: -0.906124] [G loss: -3.046982]\n",
      "[Epoch 8/200] [Batch 2000/4688] [D loss: -1.010804] [G loss: -3.004214]\n",
      "[Epoch 8/200] [Batch 3000/4688] [D loss: -1.111468] [G loss: 0.666315]\n",
      "[Epoch 8/200] [Batch 4000/4688] [D loss: -1.393887] [G loss: 0.053446]\n",
      "[Epoch 9/200] [Batch 0/4688] [D loss: -0.705803] [G loss: -0.238128]\n",
      "[Epoch 9/200] [Batch 1000/4688] [D loss: -0.935701] [G loss: -1.458002]\n",
      "[Epoch 9/200] [Batch 2000/4688] [D loss: -1.042078] [G loss: -0.730082]\n",
      "[Epoch 9/200] [Batch 3000/4688] [D loss: -1.153887] [G loss: -2.932693]\n",
      "[Epoch 9/200] [Batch 4000/4688] [D loss: -1.655519] [G loss: 0.387962]\n",
      "[Epoch 10/200] [Batch 0/4688] [D loss: -0.702632] [G loss: -0.281891]\n",
      "[Epoch 10/200] [Batch 1000/4688] [D loss: -1.243292] [G loss: 3.040914]\n",
      "[Epoch 10/200] [Batch 2000/4688] [D loss: -1.068971] [G loss: 3.084106]\n",
      "[Epoch 10/200] [Batch 3000/4688] [D loss: -1.593932] [G loss: 2.880339]\n",
      "[Epoch 10/200] [Batch 4000/4688] [D loss: -1.138718] [G loss: 1.711756]\n",
      "[Epoch 11/200] [Batch 0/4688] [D loss: -0.803086] [G loss: 0.949719]\n",
      "[Epoch 11/200] [Batch 1000/4688] [D loss: -0.983777] [G loss: 2.504142]\n",
      "[Epoch 11/200] [Batch 2000/4688] [D loss: -1.176825] [G loss: 2.031181]\n",
      "[Epoch 11/200] [Batch 3000/4688] [D loss: -0.441558] [G loss: -4.613623]\n",
      "[Epoch 11/200] [Batch 4000/4688] [D loss: -0.480053] [G loss: -4.603820]\n",
      "[Epoch 12/200] [Batch 0/4688] [D loss: -0.801040] [G loss: -0.680861]\n",
      "[Epoch 12/200] [Batch 1000/4688] [D loss: -0.705145] [G loss: -1.858438]\n",
      "[Epoch 12/200] [Batch 2000/4688] [D loss: -0.970177] [G loss: -2.496143]\n",
      "[Epoch 12/200] [Batch 3000/4688] [D loss: -1.275159] [G loss: -0.562350]\n",
      "[Epoch 12/200] [Batch 4000/4688] [D loss: -0.865887] [G loss: -1.800754]\n",
      "[Epoch 13/200] [Batch 0/4688] [D loss: -0.678379] [G loss: 1.535670]\n",
      "[Epoch 13/200] [Batch 1000/4688] [D loss: -0.891495] [G loss: 3.223611]\n",
      "[Epoch 13/200] [Batch 2000/4688] [D loss: -1.388909] [G loss: 1.320459]\n",
      "[Epoch 13/200] [Batch 3000/4688] [D loss: -0.914680] [G loss: -1.228035]\n",
      "[Epoch 13/200] [Batch 4000/4688] [D loss: -0.810254] [G loss: -3.581350]\n",
      "[Epoch 14/200] [Batch 0/4688] [D loss: -0.729361] [G loss: 0.351965]\n",
      "[Epoch 14/200] [Batch 1000/4688] [D loss: -0.943392] [G loss: -2.193822]\n",
      "[Epoch 14/200] [Batch 2000/4688] [D loss: -1.275581] [G loss: -0.573500]\n",
      "[Epoch 14/200] [Batch 3000/4688] [D loss: -1.075035] [G loss: -0.477154]\n",
      "[Epoch 14/200] [Batch 4000/4688] [D loss: -0.744485] [G loss: -0.533125]\n",
      "[Epoch 15/200] [Batch 0/4688] [D loss: -0.479770] [G loss: -1.622061]\n",
      "[Epoch 15/200] [Batch 1000/4688] [D loss: -1.098917] [G loss: 0.855968]\n",
      "[Epoch 15/200] [Batch 2000/4688] [D loss: -1.057727] [G loss: 2.597544]\n",
      "[Epoch 15/200] [Batch 3000/4688] [D loss: -1.370750] [G loss: 3.008984]\n",
      "[Epoch 15/200] [Batch 4000/4688] [D loss: -0.998864] [G loss: -0.794665]\n",
      "[Epoch 16/200] [Batch 0/4688] [D loss: -0.684107] [G loss: 2.255059]\n",
      "[Epoch 16/200] [Batch 1000/4688] [D loss: -1.257979] [G loss: 2.529027]\n",
      "[Epoch 16/200] [Batch 2000/4688] [D loss: -0.897007] [G loss: 5.841774]\n",
      "[Epoch 16/200] [Batch 3000/4688] [D loss: -1.040663] [G loss: 4.127849]\n",
      "[Epoch 16/200] [Batch 4000/4688] [D loss: -0.908446] [G loss: 1.168634]\n",
      "[Epoch 17/200] [Batch 0/4688] [D loss: -0.532833] [G loss: -0.333503]\n",
      "[Epoch 17/200] [Batch 1000/4688] [D loss: -1.103290] [G loss: 3.293426]\n",
      "[Epoch 17/200] [Batch 2000/4688] [D loss: -1.112497] [G loss: 0.033428]\n",
      "[Epoch 17/200] [Batch 3000/4688] [D loss: -1.119417] [G loss: 0.471061]\n",
      "[Epoch 17/200] [Batch 4000/4688] [D loss: -1.086381] [G loss: -3.502164]\n",
      "[Epoch 18/200] [Batch 0/4688] [D loss: -0.541157] [G loss: -1.296569]\n",
      "[Epoch 18/200] [Batch 1000/4688] [D loss: -1.198031] [G loss: 2.768696]\n",
      "[Epoch 18/200] [Batch 2000/4688] [D loss: -1.735424] [G loss: 2.395808]\n",
      "[Epoch 18/200] [Batch 3000/4688] [D loss: -1.199706] [G loss: -0.567544]\n",
      "[Epoch 18/200] [Batch 4000/4688] [D loss: -1.316295] [G loss: -1.871609]\n",
      "[Epoch 19/200] [Batch 0/4688] [D loss: -0.960912] [G loss: 1.113855]\n",
      "[Epoch 19/200] [Batch 1000/4688] [D loss: -1.163901] [G loss: 1.518483]\n",
      "[Epoch 19/200] [Batch 2000/4688] [D loss: -0.927399] [G loss: 1.967312]\n",
      "[Epoch 19/200] [Batch 3000/4688] [D loss: -0.990396] [G loss: 0.794212]\n",
      "[Epoch 19/200] [Batch 4000/4688] [D loss: -1.033772] [G loss: -4.056763]\n",
      "[Epoch 20/200] [Batch 0/4688] [D loss: -0.560808] [G loss: -1.038345]\n",
      "[Epoch 20/200] [Batch 1000/4688] [D loss: -1.355772] [G loss: 4.454495]\n",
      "[Epoch 20/200] [Batch 2000/4688] [D loss: -1.153809] [G loss: -1.743113]\n",
      "[Epoch 20/200] [Batch 3000/4688] [D loss: -1.548655] [G loss: 0.207598]\n",
      "[Epoch 20/200] [Batch 4000/4688] [D loss: -1.312062] [G loss: -1.571691]\n",
      "[Epoch 21/200] [Batch 0/4688] [D loss: -1.235296] [G loss: 0.237884]\n",
      "[Epoch 21/200] [Batch 1000/4688] [D loss: -1.374875] [G loss: 0.410281]\n",
      "[Epoch 21/200] [Batch 2000/4688] [D loss: -1.782457] [G loss: 0.386184]\n",
      "[Epoch 21/200] [Batch 3000/4688] [D loss: -1.344293] [G loss: 0.038990]\n",
      "[Epoch 21/200] [Batch 4000/4688] [D loss: -1.076516] [G loss: -2.923292]\n",
      "[Epoch 22/200] [Batch 0/4688] [D loss: -0.757756] [G loss: 0.269976]\n",
      "[Epoch 22/200] [Batch 1000/4688] [D loss: -1.321107] [G loss: 1.759622]\n",
      "[Epoch 22/200] [Batch 2000/4688] [D loss: -1.233707] [G loss: -0.610834]\n",
      "[Epoch 22/200] [Batch 3000/4688] [D loss: -1.626737] [G loss: -0.064157]\n",
      "[Epoch 22/200] [Batch 4000/4688] [D loss: -1.672777] [G loss: 0.868041]\n",
      "[Epoch 23/200] [Batch 0/4688] [D loss: -0.736544] [G loss: -0.357174]\n",
      "[Epoch 23/200] [Batch 1000/4688] [D loss: -1.716544] [G loss: -1.072421]\n",
      "[Epoch 23/200] [Batch 2000/4688] [D loss: -1.846017] [G loss: 3.050114]\n",
      "[Epoch 23/200] [Batch 3000/4688] [D loss: -1.433616] [G loss: 4.909820]\n",
      "[Epoch 23/200] [Batch 4000/4688] [D loss: -1.464732] [G loss: -0.050789]\n",
      "[Epoch 24/200] [Batch 0/4688] [D loss: -0.737910] [G loss: -0.020500]\n",
      "[Epoch 24/200] [Batch 1000/4688] [D loss: -1.286395] [G loss: 0.514210]\n",
      "[Epoch 24/200] [Batch 2000/4688] [D loss: -1.410349] [G loss: 0.778147]\n",
      "[Epoch 24/200] [Batch 3000/4688] [D loss: -1.384113] [G loss: -2.418554]\n",
      "[Epoch 24/200] [Batch 4000/4688] [D loss: -0.829530] [G loss: -3.993148]\n",
      "[Epoch 25/200] [Batch 0/4688] [D loss: -0.725802] [G loss: -1.022486]\n",
      "[Epoch 25/200] [Batch 1000/4688] [D loss: -1.050957] [G loss: -2.403102]\n",
      "[Epoch 25/200] [Batch 2000/4688] [D loss: -1.310565] [G loss: 0.035768]\n",
      "[Epoch 25/200] [Batch 3000/4688] [D loss: -1.481466] [G loss: 0.393041]\n",
      "[Epoch 25/200] [Batch 4000/4688] [D loss: -1.142028] [G loss: -1.414333]\n",
      "[Epoch 26/200] [Batch 0/4688] [D loss: -1.081956] [G loss: -1.078220]\n",
      "[Epoch 26/200] [Batch 1000/4688] [D loss: -1.799345] [G loss: 0.415359]\n",
      "[Epoch 26/200] [Batch 2000/4688] [D loss: -1.021631] [G loss: 4.514153]\n",
      "[Epoch 26/200] [Batch 3000/4688] [D loss: -1.246603] [G loss: -0.392155]\n",
      "[Epoch 26/200] [Batch 4000/4688] [D loss: -1.594193] [G loss: 1.295355]\n",
      "[Epoch 27/200] [Batch 0/4688] [D loss: -0.841960] [G loss: 0.810073]\n",
      "[Epoch 27/200] [Batch 1000/4688] [D loss: -0.972891] [G loss: 3.029568]\n",
      "[Epoch 27/200] [Batch 2000/4688] [D loss: -1.172450] [G loss: 1.341221]\n",
      "[Epoch 27/200] [Batch 3000/4688] [D loss: -1.516216] [G loss: 0.673509]\n",
      "[Epoch 27/200] [Batch 4000/4688] [D loss: -1.122957] [G loss: 0.769319]\n",
      "[Epoch 28/200] [Batch 0/4688] [D loss: -0.850994] [G loss: -2.009996]\n",
      "[Epoch 28/200] [Batch 1000/4688] [D loss: -1.772973] [G loss: 1.237941]\n",
      "[Epoch 28/200] [Batch 2000/4688] [D loss: -1.349357] [G loss: 1.333175]\n",
      "[Epoch 28/200] [Batch 3000/4688] [D loss: -1.180861] [G loss: 1.809976]\n",
      "[Epoch 28/200] [Batch 4000/4688] [D loss: -1.321120] [G loss: -0.677320]\n",
      "[Epoch 29/200] [Batch 0/4688] [D loss: -0.778423] [G loss: 1.155705]\n",
      "[Epoch 29/200] [Batch 1000/4688] [D loss: -1.189339] [G loss: 2.869970]\n",
      "[Epoch 29/200] [Batch 2000/4688] [D loss: -1.189536] [G loss: -0.050971]\n",
      "[Epoch 29/200] [Batch 3000/4688] [D loss: -1.294018] [G loss: -0.717361]\n",
      "[Epoch 29/200] [Batch 4000/4688] [D loss: -1.292777] [G loss: -2.077321]\n",
      "[Epoch 30/200] [Batch 0/4688] [D loss: -0.937952] [G loss: 0.171259]\n",
      "[Epoch 30/200] [Batch 1000/4688] [D loss: -1.108795] [G loss: -2.575473]\n",
      "[Epoch 30/200] [Batch 2000/4688] [D loss: -1.386627] [G loss: 1.657228]\n",
      "[Epoch 30/200] [Batch 3000/4688] [D loss: -1.631491] [G loss: 1.739039]\n",
      "[Epoch 30/200] [Batch 4000/4688] [D loss: -1.152182] [G loss: 0.357798]\n",
      "[Epoch 31/200] [Batch 0/4688] [D loss: -0.896378] [G loss: 2.113372]\n",
      "[Epoch 31/200] [Batch 1000/4688] [D loss: -1.016552] [G loss: -3.678355]\n",
      "[Epoch 31/200] [Batch 2000/4688] [D loss: -1.102605] [G loss: 1.819825]\n",
      "[Epoch 31/200] [Batch 3000/4688] [D loss: -1.196705] [G loss: -1.573995]\n",
      "[Epoch 31/200] [Batch 4000/4688] [D loss: -0.756735] [G loss: -6.732177]\n",
      "[Epoch 32/200] [Batch 0/4688] [D loss: -0.747743] [G loss: -2.191456]\n",
      "[Epoch 32/200] [Batch 1000/4688] [D loss: -1.418852] [G loss: 1.194321]\n",
      "[Epoch 32/200] [Batch 2000/4688] [D loss: -1.576253] [G loss: 2.458798]\n",
      "[Epoch 32/200] [Batch 3000/4688] [D loss: -1.026979] [G loss: -0.291233]\n",
      "[Epoch 32/200] [Batch 4000/4688] [D loss: -0.994565] [G loss: -1.504388]\n",
      "[Epoch 33/200] [Batch 0/4688] [D loss: -0.849786] [G loss: -0.867660]\n",
      "[Epoch 33/200] [Batch 1000/4688] [D loss: -1.013153] [G loss: -0.903737]\n",
      "[Epoch 33/200] [Batch 2000/4688] [D loss: -0.895819] [G loss: -2.891956]\n",
      "[Epoch 33/200] [Batch 3000/4688] [D loss: -1.016760] [G loss: -1.896038]\n",
      "[Epoch 33/200] [Batch 4000/4688] [D loss: -1.391989] [G loss: 3.795565]\n",
      "[Epoch 34/200] [Batch 0/4688] [D loss: -0.778553] [G loss: 0.500944]\n",
      "[Epoch 34/200] [Batch 1000/4688] [D loss: -0.676722] [G loss: -4.504568]\n",
      "[Epoch 34/200] [Batch 2000/4688] [D loss: -0.788706] [G loss: -4.917565]\n",
      "[Epoch 34/200] [Batch 3000/4688] [D loss: -1.010452] [G loss: -3.919660]\n",
      "[Epoch 34/200] [Batch 4000/4688] [D loss: -0.688512] [G loss: -6.424630]\n",
      "[Epoch 35/200] [Batch 0/4688] [D loss: -0.710641] [G loss: -1.788596]\n",
      "[Epoch 35/200] [Batch 1000/4688] [D loss: -1.142674] [G loss: 1.069415]\n",
      "[Epoch 35/200] [Batch 2000/4688] [D loss: -1.281160] [G loss: 2.791106]\n",
      "[Epoch 35/200] [Batch 3000/4688] [D loss: -1.315578] [G loss: 0.004909]\n",
      "[Epoch 35/200] [Batch 4000/4688] [D loss: -1.108950] [G loss: 1.432328]\n",
      "[Epoch 36/200] [Batch 0/4688] [D loss: -0.661985] [G loss: -2.518317]\n",
      "[Epoch 36/200] [Batch 1000/4688] [D loss: -1.159280] [G loss: -0.626692]\n",
      "[Epoch 36/200] [Batch 2000/4688] [D loss: -0.838442] [G loss: 0.171578]\n",
      "[Epoch 36/200] [Batch 3000/4688] [D loss: -0.850934] [G loss: -0.142209]\n",
      "[Epoch 36/200] [Batch 4000/4688] [D loss: -0.906987] [G loss: -2.796111]\n",
      "[Epoch 37/200] [Batch 0/4688] [D loss: -0.540136] [G loss: -2.370722]\n",
      "[Epoch 37/200] [Batch 1000/4688] [D loss: -0.915262] [G loss: 1.578059]\n",
      "[Epoch 37/200] [Batch 2000/4688] [D loss: -0.943946] [G loss: 1.449505]\n",
      "[Epoch 37/200] [Batch 3000/4688] [D loss: -1.006221] [G loss: -0.554450]\n",
      "[Epoch 37/200] [Batch 4000/4688] [D loss: -1.143364] [G loss: 0.353471]\n",
      "[Epoch 38/200] [Batch 0/4688] [D loss: -0.683072] [G loss: 1.036896]\n",
      "[Epoch 38/200] [Batch 1000/4688] [D loss: -1.018829] [G loss: -1.241360]\n",
      "[Epoch 38/200] [Batch 2000/4688] [D loss: -0.991062] [G loss: 2.057517]\n",
      "[Epoch 38/200] [Batch 3000/4688] [D loss: -1.207803] [G loss: 2.822526]\n",
      "[Epoch 38/200] [Batch 4000/4688] [D loss: -1.376370] [G loss: 1.678526]\n",
      "[Epoch 39/200] [Batch 0/4688] [D loss: -0.460077] [G loss: 1.603294]\n",
      "[Epoch 39/200] [Batch 1000/4688] [D loss: -1.148681] [G loss: 1.994402]\n",
      "[Epoch 39/200] [Batch 2000/4688] [D loss: -0.939355] [G loss: 3.731952]\n",
      "[Epoch 39/200] [Batch 3000/4688] [D loss: -1.063264] [G loss: 0.411217]\n",
      "[Epoch 39/200] [Batch 4000/4688] [D loss: -1.152925] [G loss: 1.688451]\n",
      "[Epoch 40/200] [Batch 0/4688] [D loss: -0.764761] [G loss: 2.081468]\n",
      "[Epoch 40/200] [Batch 1000/4688] [D loss: -1.160632] [G loss: 2.214380]\n",
      "[Epoch 40/200] [Batch 2000/4688] [D loss: -1.251761] [G loss: 3.270902]\n",
      "[Epoch 40/200] [Batch 3000/4688] [D loss: -1.259994] [G loss: 2.491143]\n",
      "[Epoch 40/200] [Batch 4000/4688] [D loss: -0.816731] [G loss: -0.581382]\n",
      "[Epoch 41/200] [Batch 0/4688] [D loss: -0.789918] [G loss: 1.181082]\n",
      "[Epoch 41/200] [Batch 1000/4688] [D loss: -0.666926] [G loss: -3.367133]\n",
      "[Epoch 41/200] [Batch 2000/4688] [D loss: -1.028856] [G loss: -2.025580]\n",
      "[Epoch 41/200] [Batch 3000/4688] [D loss: -1.117404] [G loss: 3.279777]\n",
      "[Epoch 41/200] [Batch 4000/4688] [D loss: -1.105344] [G loss: 1.384095]\n",
      "[Epoch 42/200] [Batch 0/4688] [D loss: -0.615183] [G loss: 3.409416]\n",
      "[Epoch 42/200] [Batch 1000/4688] [D loss: -1.179425] [G loss: 4.985908]\n",
      "[Epoch 42/200] [Batch 2000/4688] [D loss: -0.957212] [G loss: -0.190400]\n",
      "[Epoch 42/200] [Batch 3000/4688] [D loss: -0.772784] [G loss: -0.903771]\n",
      "[Epoch 42/200] [Batch 4000/4688] [D loss: -0.943404] [G loss: 0.216876]\n",
      "[Epoch 43/200] [Batch 0/4688] [D loss: -0.617864] [G loss: 0.694823]\n",
      "[Epoch 43/200] [Batch 1000/4688] [D loss: -0.781715] [G loss: -1.243739]\n",
      "[Epoch 43/200] [Batch 2000/4688] [D loss: -0.847480] [G loss: -1.290502]\n",
      "[Epoch 43/200] [Batch 3000/4688] [D loss: -1.106782] [G loss: 0.207779]\n",
      "[Epoch 43/200] [Batch 4000/4688] [D loss: -1.095363] [G loss: 4.389180]\n",
      "[Epoch 44/200] [Batch 0/4688] [D loss: -0.507654] [G loss: 1.598679]\n",
      "[Epoch 44/200] [Batch 1000/4688] [D loss: -0.801514] [G loss: 1.204493]\n",
      "[Epoch 44/200] [Batch 2000/4688] [D loss: -0.827225] [G loss: 4.085444]\n",
      "[Epoch 44/200] [Batch 3000/4688] [D loss: -0.937951] [G loss: 1.929578]\n",
      "[Epoch 44/200] [Batch 4000/4688] [D loss: -0.655637] [G loss: -3.101557]\n",
      "[Epoch 45/200] [Batch 0/4688] [D loss: -0.690580] [G loss: 0.332037]\n",
      "[Epoch 45/200] [Batch 1000/4688] [D loss: -0.543682] [G loss: 7.625082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 45/200] [Batch 2000/4688] [D loss: -0.656619] [G loss: 1.945401]\n",
      "[Epoch 45/200] [Batch 3000/4688] [D loss: -0.929629] [G loss: 1.930957]\n",
      "[Epoch 45/200] [Batch 4000/4688] [D loss: -1.096417] [G loss: 0.776754]\n",
      "[Epoch 46/200] [Batch 0/4688] [D loss: -0.529594] [G loss: 2.170484]\n",
      "[Epoch 46/200] [Batch 1000/4688] [D loss: -0.922741] [G loss: 1.746680]\n",
      "[Epoch 46/200] [Batch 2000/4688] [D loss: -0.937242] [G loss: 1.620752]\n",
      "[Epoch 46/200] [Batch 3000/4688] [D loss: -0.834447] [G loss: 1.460701]\n",
      "[Epoch 46/200] [Batch 4000/4688] [D loss: -0.856994] [G loss: 2.247192]\n",
      "[Epoch 47/200] [Batch 0/4688] [D loss: -0.569196] [G loss: 0.286404]\n",
      "[Epoch 47/200] [Batch 1000/4688] [D loss: -0.708755] [G loss: -1.319800]\n",
      "[Epoch 47/200] [Batch 2000/4688] [D loss: -0.609410] [G loss: -5.483584]\n",
      "[Epoch 47/200] [Batch 3000/4688] [D loss: -0.456409] [G loss: -9.822556]\n",
      "[Epoch 47/200] [Batch 4000/4688] [D loss: -0.533039] [G loss: -12.884999]\n",
      "[Epoch 48/200] [Batch 0/4688] [D loss: -0.327561] [G loss: -10.507765]\n",
      "[Epoch 48/200] [Batch 1000/4688] [D loss: -0.570738] [G loss: -11.344036]\n",
      "[Epoch 48/200] [Batch 2000/4688] [D loss: -0.691643] [G loss: -4.922881]\n",
      "[Epoch 48/200] [Batch 3000/4688] [D loss: -0.922542] [G loss: -0.636945]\n",
      "[Epoch 48/200] [Batch 4000/4688] [D loss: -0.991295] [G loss: -0.686211]\n",
      "[Epoch 49/200] [Batch 0/4688] [D loss: -0.557897] [G loss: -1.926349]\n",
      "[Epoch 49/200] [Batch 1000/4688] [D loss: -1.169170] [G loss: -2.844411]\n",
      "[Epoch 49/200] [Batch 2000/4688] [D loss: -0.952839] [G loss: -0.715739]\n",
      "[Epoch 49/200] [Batch 3000/4688] [D loss: -0.828219] [G loss: -1.380240]\n",
      "[Epoch 49/200] [Batch 4000/4688] [D loss: -0.997310] [G loss: 3.498767]\n",
      "[Epoch 50/200] [Batch 0/4688] [D loss: -0.504313] [G loss: -1.168465]\n",
      "[Epoch 50/200] [Batch 1000/4688] [D loss: -0.814927] [G loss: -2.257531]\n",
      "[Epoch 50/200] [Batch 2000/4688] [D loss: -0.842363] [G loss: -2.010481]\n",
      "[Epoch 50/200] [Batch 3000/4688] [D loss: -0.783133] [G loss: 3.107249]\n",
      "[Epoch 50/200] [Batch 4000/4688] [D loss: -0.940029] [G loss: -0.249567]\n",
      "[Epoch 51/200] [Batch 0/4688] [D loss: -0.391584] [G loss: 0.799223]\n",
      "[Epoch 51/200] [Batch 1000/4688] [D loss: -0.783222] [G loss: 1.473585]\n",
      "[Epoch 51/200] [Batch 2000/4688] [D loss: -0.920716] [G loss: -1.359618]\n",
      "[Epoch 51/200] [Batch 3000/4688] [D loss: -0.906354] [G loss: 3.265882]\n",
      "[Epoch 51/200] [Batch 4000/4688] [D loss: -0.904524] [G loss: 4.179991]\n",
      "[Epoch 52/200] [Batch 0/4688] [D loss: -0.564595] [G loss: -2.053783]\n",
      "[Epoch 52/200] [Batch 1000/4688] [D loss: -0.810495] [G loss: 1.204481]\n",
      "[Epoch 52/200] [Batch 2000/4688] [D loss: -0.647312] [G loss: 4.790797]\n",
      "[Epoch 52/200] [Batch 3000/4688] [D loss: -0.787547] [G loss: 2.155262]\n",
      "[Epoch 52/200] [Batch 4000/4688] [D loss: -0.864923] [G loss: 0.043194]\n",
      "[Epoch 53/200] [Batch 0/4688] [D loss: -0.426445] [G loss: -1.855289]\n",
      "[Epoch 53/200] [Batch 1000/4688] [D loss: -0.703259] [G loss: -1.030911]\n",
      "[Epoch 53/200] [Batch 2000/4688] [D loss: -0.650786] [G loss: -0.089643]\n",
      "[Epoch 53/200] [Batch 3000/4688] [D loss: -0.886861] [G loss: 1.001592]\n",
      "[Epoch 53/200] [Batch 4000/4688] [D loss: -0.799420] [G loss: -2.104889]\n",
      "[Epoch 54/200] [Batch 0/4688] [D loss: -0.247363] [G loss: -6.171953]\n",
      "[Epoch 54/200] [Batch 1000/4688] [D loss: -1.058912] [G loss: -3.164871]\n",
      "[Epoch 54/200] [Batch 2000/4688] [D loss: -0.762932] [G loss: -7.174147]\n",
      "[Epoch 54/200] [Batch 3000/4688] [D loss: -0.667067] [G loss: -1.661638]\n",
      "[Epoch 54/200] [Batch 4000/4688] [D loss: -0.545269] [G loss: -4.018992]\n",
      "[Epoch 55/200] [Batch 0/4688] [D loss: -0.571338] [G loss: -3.660695]\n",
      "[Epoch 55/200] [Batch 1000/4688] [D loss: -0.774400] [G loss: -0.794742]\n",
      "[Epoch 55/200] [Batch 2000/4688] [D loss: -0.660020] [G loss: -1.963398]\n",
      "[Epoch 55/200] [Batch 3000/4688] [D loss: -0.614277] [G loss: -6.492582]\n",
      "[Epoch 55/200] [Batch 4000/4688] [D loss: -0.589099] [G loss: -9.040094]\n",
      "[Epoch 56/200] [Batch 0/4688] [D loss: -0.767135] [G loss: -3.058397]\n",
      "[Epoch 56/200] [Batch 1000/4688] [D loss: -0.724739] [G loss: -3.588281]\n",
      "[Epoch 56/200] [Batch 2000/4688] [D loss: -0.640016] [G loss: 1.583067]\n",
      "[Epoch 56/200] [Batch 3000/4688] [D loss: -0.662103] [G loss: -4.198116]\n",
      "[Epoch 56/200] [Batch 4000/4688] [D loss: -0.443826] [G loss: -8.176508]\n",
      "[Epoch 57/200] [Batch 0/4688] [D loss: -0.831478] [G loss: 1.248688]\n",
      "[Epoch 57/200] [Batch 1000/4688] [D loss: -0.689433] [G loss: -2.976728]\n",
      "[Epoch 57/200] [Batch 2000/4688] [D loss: -0.638041] [G loss: -4.463857]\n",
      "[Epoch 57/200] [Batch 3000/4688] [D loss: -0.661926] [G loss: -0.599443]\n",
      "[Epoch 57/200] [Batch 4000/4688] [D loss: -0.654652] [G loss: 2.106939]\n",
      "[Epoch 58/200] [Batch 0/4688] [D loss: -0.412061] [G loss: -0.916955]\n",
      "[Epoch 58/200] [Batch 1000/4688] [D loss: -0.502037] [G loss: -2.397282]\n",
      "[Epoch 58/200] [Batch 2000/4688] [D loss: -0.592118] [G loss: -2.995183]\n",
      "[Epoch 58/200] [Batch 3000/4688] [D loss: -0.541523] [G loss: 1.023060]\n",
      "[Epoch 58/200] [Batch 4000/4688] [D loss: -0.732912] [G loss: -4.022244]\n",
      "[Epoch 59/200] [Batch 0/4688] [D loss: -0.376443] [G loss: 0.826107]\n",
      "[Epoch 59/200] [Batch 1000/4688] [D loss: -0.666905] [G loss: -2.216483]\n",
      "[Epoch 59/200] [Batch 2000/4688] [D loss: -0.611121] [G loss: -0.117763]\n",
      "[Epoch 59/200] [Batch 3000/4688] [D loss: -0.524355] [G loss: -0.786915]\n",
      "[Epoch 59/200] [Batch 4000/4688] [D loss: -0.572493] [G loss: -3.869950]\n",
      "[Epoch 60/200] [Batch 0/4688] [D loss: -0.471822] [G loss: 0.397685]\n",
      "[Epoch 60/200] [Batch 1000/4688] [D loss: -0.551649] [G loss: -3.844211]\n",
      "[Epoch 60/200] [Batch 2000/4688] [D loss: -0.350519] [G loss: -3.850144]\n",
      "[Epoch 60/200] [Batch 3000/4688] [D loss: -0.572051] [G loss: -2.683537]\n",
      "[Epoch 60/200] [Batch 4000/4688] [D loss: -0.573620] [G loss: -2.347773]\n",
      "[Epoch 61/200] [Batch 0/4688] [D loss: -0.395709] [G loss: 0.338235]\n",
      "[Epoch 61/200] [Batch 1000/4688] [D loss: -0.597132] [G loss: -1.731556]\n",
      "[Epoch 61/200] [Batch 2000/4688] [D loss: -0.579270] [G loss: -0.226073]\n",
      "[Epoch 61/200] [Batch 3000/4688] [D loss: -0.745037] [G loss: 0.036934]\n",
      "[Epoch 61/200] [Batch 4000/4688] [D loss: -0.478171] [G loss: -5.635261]\n",
      "[Epoch 62/200] [Batch 0/4688] [D loss: -0.267606] [G loss: -4.776246]\n",
      "[Epoch 62/200] [Batch 1000/4688] [D loss: -0.677550] [G loss: 1.434776]\n",
      "[Epoch 62/200] [Batch 2000/4688] [D loss: -0.570877] [G loss: -0.106943]\n",
      "[Epoch 62/200] [Batch 3000/4688] [D loss: -0.597072] [G loss: -2.136296]\n",
      "[Epoch 62/200] [Batch 4000/4688] [D loss: -0.531285] [G loss: -0.496636]\n",
      "[Epoch 63/200] [Batch 0/4688] [D loss: -0.307999] [G loss: 0.674624]\n",
      "[Epoch 63/200] [Batch 1000/4688] [D loss: -0.640076] [G loss: 2.353846]\n",
      "[Epoch 63/200] [Batch 2000/4688] [D loss: -0.567939] [G loss: -1.717825]\n",
      "[Epoch 63/200] [Batch 3000/4688] [D loss: -0.701008] [G loss: -2.957368]\n",
      "[Epoch 63/200] [Batch 4000/4688] [D loss: -0.578483] [G loss: -3.227386]\n",
      "[Epoch 64/200] [Batch 0/4688] [D loss: -0.265025] [G loss: -3.300621]\n",
      "[Epoch 64/200] [Batch 1000/4688] [D loss: -0.410753] [G loss: -4.355068]\n",
      "[Epoch 64/200] [Batch 2000/4688] [D loss: -0.540520] [G loss: -5.959862]\n",
      "[Epoch 64/200] [Batch 3000/4688] [D loss: -0.551051] [G loss: 2.002782]\n",
      "[Epoch 64/200] [Batch 4000/4688] [D loss: -0.855035] [G loss: 0.111502]\n",
      "[Epoch 65/200] [Batch 0/4688] [D loss: -0.394275] [G loss: -3.721688]\n",
      "[Epoch 65/200] [Batch 1000/4688] [D loss: -0.369764] [G loss: -2.335426]\n",
      "[Epoch 65/200] [Batch 2000/4688] [D loss: -0.372854] [G loss: -2.489976]\n",
      "[Epoch 65/200] [Batch 3000/4688] [D loss: -0.454829] [G loss: 1.826659]\n",
      "[Epoch 65/200] [Batch 4000/4688] [D loss: -0.628236] [G loss: -3.823630]\n",
      "[Epoch 66/200] [Batch 0/4688] [D loss: -0.406834] [G loss: 2.821988]\n",
      "[Epoch 66/200] [Batch 1000/4688] [D loss: -0.579211] [G loss: -0.295393]\n",
      "[Epoch 66/200] [Batch 2000/4688] [D loss: -0.453727] [G loss: -1.622456]\n",
      "[Epoch 66/200] [Batch 3000/4688] [D loss: -0.627261] [G loss: -1.189686]\n",
      "[Epoch 66/200] [Batch 4000/4688] [D loss: -0.559650] [G loss: -0.124881]\n",
      "[Epoch 67/200] [Batch 0/4688] [D loss: -0.324526] [G loss: -3.037964]\n",
      "[Epoch 67/200] [Batch 1000/4688] [D loss: -0.582382] [G loss: -2.636379]\n",
      "[Epoch 67/200] [Batch 2000/4688] [D loss: -0.723786] [G loss: -1.911556]\n",
      "[Epoch 67/200] [Batch 3000/4688] [D loss: -0.612327] [G loss: -1.078493]\n",
      "[Epoch 67/200] [Batch 4000/4688] [D loss: -0.468381] [G loss: -3.054840]\n",
      "[Epoch 68/200] [Batch 0/4688] [D loss: -0.387201] [G loss: -4.538864]\n",
      "[Epoch 68/200] [Batch 1000/4688] [D loss: -0.489555] [G loss: -6.497621]\n",
      "[Epoch 68/200] [Batch 2000/4688] [D loss: -0.370237] [G loss: -8.606224]\n",
      "[Epoch 68/200] [Batch 3000/4688] [D loss: -0.432366] [G loss: -5.242358]\n",
      "[Epoch 68/200] [Batch 4000/4688] [D loss: -0.745883] [G loss: 1.421215]\n",
      "[Epoch 69/200] [Batch 0/4688] [D loss: -0.417121] [G loss: -0.552268]\n",
      "[Epoch 69/200] [Batch 1000/4688] [D loss: -0.648214] [G loss: -1.314674]\n",
      "[Epoch 69/200] [Batch 2000/4688] [D loss: -0.689376] [G loss: -1.070840]\n",
      "[Epoch 69/200] [Batch 3000/4688] [D loss: -0.522253] [G loss: -2.913554]\n",
      "[Epoch 69/200] [Batch 4000/4688] [D loss: -0.502011] [G loss: -6.552867]\n",
      "[Epoch 70/200] [Batch 0/4688] [D loss: -0.614881] [G loss: -2.196129]\n",
      "[Epoch 70/200] [Batch 1000/4688] [D loss: -0.809036] [G loss: 2.742057]\n",
      "[Epoch 70/200] [Batch 2000/4688] [D loss: -0.515406] [G loss: -0.936701]\n",
      "[Epoch 70/200] [Batch 3000/4688] [D loss: -0.613628] [G loss: 2.880585]\n",
      "[Epoch 70/200] [Batch 4000/4688] [D loss: -0.784483] [G loss: 0.160821]\n",
      "[Epoch 71/200] [Batch 0/4688] [D loss: -0.395170] [G loss: -0.775076]\n",
      "[Epoch 71/200] [Batch 1000/4688] [D loss: -0.734938] [G loss: 2.279859]\n",
      "[Epoch 71/200] [Batch 2000/4688] [D loss: -0.690274] [G loss: 0.433377]\n",
      "[Epoch 71/200] [Batch 3000/4688] [D loss: -0.609062] [G loss: -4.182689]\n",
      "[Epoch 71/200] [Batch 4000/4688] [D loss: -0.739476] [G loss: -3.754901]\n",
      "[Epoch 72/200] [Batch 0/4688] [D loss: -0.500572] [G loss: -1.347583]\n",
      "[Epoch 72/200] [Batch 1000/4688] [D loss: -0.456150] [G loss: -6.541374]\n",
      "[Epoch 72/200] [Batch 2000/4688] [D loss: -0.837134] [G loss: 1.393170]\n",
      "[Epoch 72/200] [Batch 3000/4688] [D loss: -0.736509] [G loss: -1.457940]\n",
      "[Epoch 72/200] [Batch 4000/4688] [D loss: -0.585949] [G loss: -2.124614]\n",
      "[Epoch 73/200] [Batch 0/4688] [D loss: -0.507094] [G loss: -0.420288]\n",
      "[Epoch 73/200] [Batch 1000/4688] [D loss: -0.762332] [G loss: -0.317848]\n",
      "[Epoch 73/200] [Batch 2000/4688] [D loss: -0.577752] [G loss: 0.476516]\n",
      "[Epoch 73/200] [Batch 3000/4688] [D loss: -0.660964] [G loss: -0.793274]\n",
      "[Epoch 73/200] [Batch 4000/4688] [D loss: -0.763711] [G loss: -2.088956]\n",
      "[Epoch 74/200] [Batch 0/4688] [D loss: -0.334705] [G loss: -1.732876]\n",
      "[Epoch 74/200] [Batch 1000/4688] [D loss: -0.595285] [G loss: -1.230966]\n",
      "[Epoch 74/200] [Batch 2000/4688] [D loss: -0.532051] [G loss: 0.835713]\n",
      "[Epoch 74/200] [Batch 3000/4688] [D loss: -0.778725] [G loss: 0.749279]\n",
      "[Epoch 74/200] [Batch 4000/4688] [D loss: -0.732604] [G loss: 0.744418]\n",
      "[Epoch 75/200] [Batch 0/4688] [D loss: -0.547872] [G loss: 0.451597]\n",
      "[Epoch 75/200] [Batch 1000/4688] [D loss: -0.627805] [G loss: -0.558484]\n",
      "[Epoch 75/200] [Batch 2000/4688] [D loss: -0.676227] [G loss: 0.451890]\n",
      "[Epoch 75/200] [Batch 3000/4688] [D loss: -0.745189] [G loss: -0.797793]\n",
      "[Epoch 75/200] [Batch 4000/4688] [D loss: -0.795937] [G loss: -0.150859]\n",
      "[Epoch 76/200] [Batch 0/4688] [D loss: -0.593172] [G loss: -0.580923]\n",
      "[Epoch 76/200] [Batch 1000/4688] [D loss: -0.806814] [G loss: -0.499258]\n",
      "[Epoch 76/200] [Batch 2000/4688] [D loss: -0.545314] [G loss: -2.045682]\n",
      "[Epoch 76/200] [Batch 3000/4688] [D loss: -0.712425] [G loss: 0.738043]\n",
      "[Epoch 76/200] [Batch 4000/4688] [D loss: -0.521465] [G loss: -2.054618]\n",
      "[Epoch 77/200] [Batch 0/4688] [D loss: -0.416457] [G loss: 0.549330]\n",
      "[Epoch 77/200] [Batch 1000/4688] [D loss: -0.473348] [G loss: -3.378316]\n",
      "[Epoch 77/200] [Batch 2000/4688] [D loss: -0.534634] [G loss: -3.405854]\n",
      "[Epoch 77/200] [Batch 3000/4688] [D loss: -0.862737] [G loss: 2.548544]\n",
      "[Epoch 77/200] [Batch 4000/4688] [D loss: -0.695345] [G loss: 0.798821]\n",
      "[Epoch 78/200] [Batch 0/4688] [D loss: -0.322397] [G loss: -0.972492]\n",
      "[Epoch 78/200] [Batch 1000/4688] [D loss: -0.725778] [G loss: 0.263166]\n",
      "[Epoch 78/200] [Batch 2000/4688] [D loss: -0.555119] [G loss: 1.276139]\n",
      "[Epoch 78/200] [Batch 3000/4688] [D loss: -0.495729] [G loss: 0.188433]\n",
      "[Epoch 78/200] [Batch 4000/4688] [D loss: -0.829269] [G loss: 2.207551]\n",
      "[Epoch 79/200] [Batch 0/4688] [D loss: -0.468823] [G loss: -1.785166]\n",
      "[Epoch 79/200] [Batch 1000/4688] [D loss: -0.965232] [G loss: -0.529970]\n",
      "[Epoch 79/200] [Batch 2000/4688] [D loss: -0.917234] [G loss: -0.179617]\n",
      "[Epoch 79/200] [Batch 3000/4688] [D loss: -0.482172] [G loss: -2.392687]\n",
      "[Epoch 79/200] [Batch 4000/4688] [D loss: -0.386030] [G loss: -4.274048]\n",
      "[Epoch 80/200] [Batch 0/4688] [D loss: -0.440964] [G loss: 0.732186]\n",
      "[Epoch 80/200] [Batch 1000/4688] [D loss: -0.373641] [G loss: -4.622266]\n",
      "[Epoch 80/200] [Batch 2000/4688] [D loss: -0.327872] [G loss: -5.056115]\n",
      "[Epoch 80/200] [Batch 3000/4688] [D loss: -0.353585] [G loss: -6.234391]\n",
      "[Epoch 80/200] [Batch 4000/4688] [D loss: -0.247260] [G loss: -5.989250]\n",
      "[Epoch 81/200] [Batch 0/4688] [D loss: -0.226545] [G loss: -3.036816]\n",
      "[Epoch 81/200] [Batch 1000/4688] [D loss: -0.269660] [G loss: -6.128565]\n",
      "[Epoch 81/200] [Batch 2000/4688] [D loss: -0.231989] [G loss: -4.381858]\n",
      "[Epoch 81/200] [Batch 3000/4688] [D loss: -0.286705] [G loss: -3.264494]\n",
      "[Epoch 81/200] [Batch 4000/4688] [D loss: -0.522607] [G loss: -1.811198]\n",
      "[Epoch 82/200] [Batch 0/4688] [D loss: -0.285917] [G loss: -1.546171]\n",
      "[Epoch 82/200] [Batch 1000/4688] [D loss: -0.374287] [G loss: -4.436743]\n",
      "[Epoch 82/200] [Batch 2000/4688] [D loss: -0.659643] [G loss: -4.036922]\n",
      "[Epoch 82/200] [Batch 3000/4688] [D loss: -0.552778] [G loss: -0.293764]\n",
      "[Epoch 82/200] [Batch 4000/4688] [D loss: -0.455220] [G loss: -2.909735]\n",
      "[Epoch 83/200] [Batch 0/4688] [D loss: -0.432632] [G loss: -2.040950]\n",
      "[Epoch 83/200] [Batch 1000/4688] [D loss: -0.477870] [G loss: -5.240598]\n",
      "[Epoch 83/200] [Batch 2000/4688] [D loss: -0.309693] [G loss: -6.776288]\n",
      "[Epoch 83/200] [Batch 3000/4688] [D loss: -0.279103] [G loss: -8.294135]\n",
      "[Epoch 83/200] [Batch 4000/4688] [D loss: -0.218057] [G loss: -7.796460]\n",
      "[Epoch 84/200] [Batch 0/4688] [D loss: -0.171801] [G loss: -5.265994]\n",
      "[Epoch 84/200] [Batch 1000/4688] [D loss: -0.264796] [G loss: -7.277634]\n",
      "[Epoch 84/200] [Batch 2000/4688] [D loss: -0.298038] [G loss: -6.362039]\n",
      "[Epoch 84/200] [Batch 3000/4688] [D loss: -0.238105] [G loss: -6.837984]\n",
      "[Epoch 84/200] [Batch 4000/4688] [D loss: -0.325050] [G loss: -3.725648]\n",
      "[Epoch 85/200] [Batch 0/4688] [D loss: -0.232157] [G loss: -1.757526]\n",
      "[Epoch 85/200] [Batch 1000/4688] [D loss: -0.389483] [G loss: -2.107748]\n",
      "[Epoch 85/200] [Batch 2000/4688] [D loss: -0.518846] [G loss: -1.896015]\n",
      "[Epoch 85/200] [Batch 3000/4688] [D loss: -0.412699] [G loss: -1.355350]\n",
      "[Epoch 85/200] [Batch 4000/4688] [D loss: -0.499738] [G loss: -1.076395]\n",
      "[Epoch 86/200] [Batch 0/4688] [D loss: -0.348659] [G loss: -1.104740]\n",
      "[Epoch 86/200] [Batch 1000/4688] [D loss: -0.408739] [G loss: -2.125472]\n",
      "[Epoch 86/200] [Batch 2000/4688] [D loss: -0.417140] [G loss: -1.556023]\n",
      "[Epoch 86/200] [Batch 3000/4688] [D loss: -0.351656] [G loss: -1.327390]\n",
      "[Epoch 86/200] [Batch 4000/4688] [D loss: -0.539579] [G loss: 0.417196]\n",
      "[Epoch 87/200] [Batch 0/4688] [D loss: -0.405084] [G loss: -0.400755]\n",
      "[Epoch 87/200] [Batch 1000/4688] [D loss: -0.571347] [G loss: 0.159350]\n",
      "[Epoch 87/200] [Batch 2000/4688] [D loss: -0.409035] [G loss: -0.371638]\n",
      "[Epoch 87/200] [Batch 3000/4688] [D loss: -0.424588] [G loss: -3.782018]\n",
      "[Epoch 87/200] [Batch 4000/4688] [D loss: -0.483780] [G loss: -0.392701]\n",
      "[Epoch 88/200] [Batch 0/4688] [D loss: -0.213630] [G loss: -3.348868]\n",
      "[Epoch 88/200] [Batch 1000/4688] [D loss: -0.724731] [G loss: 0.053329]\n",
      "[Epoch 88/200] [Batch 2000/4688] [D loss: -0.470519] [G loss: -0.002046]\n",
      "[Epoch 88/200] [Batch 3000/4688] [D loss: -0.420892] [G loss: 2.388300]\n",
      "[Epoch 88/200] [Batch 4000/4688] [D loss: -0.417366] [G loss: 2.471677]\n",
      "[Epoch 89/200] [Batch 0/4688] [D loss: -0.319900] [G loss: 1.790074]\n",
      "[Epoch 89/200] [Batch 1000/4688] [D loss: -0.383475] [G loss: 2.236909]\n",
      "[Epoch 89/200] [Batch 2000/4688] [D loss: -0.397911] [G loss: 2.958569]\n",
      "[Epoch 89/200] [Batch 3000/4688] [D loss: -0.401988] [G loss: 1.893155]\n",
      "[Epoch 89/200] [Batch 4000/4688] [D loss: -0.444448] [G loss: 0.698744]\n",
      "[Epoch 90/200] [Batch 0/4688] [D loss: -0.330553] [G loss: -0.032312]\n",
      "[Epoch 90/200] [Batch 1000/4688] [D loss: -0.313924] [G loss: 0.453502]\n",
      "[Epoch 90/200] [Batch 2000/4688] [D loss: -0.230129] [G loss: -2.818593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 90/200] [Batch 3000/4688] [D loss: -0.409737] [G loss: 2.462104]\n",
      "[Epoch 90/200] [Batch 4000/4688] [D loss: -0.353880] [G loss: 2.435538]\n",
      "[Epoch 91/200] [Batch 0/4688] [D loss: -0.364661] [G loss: 1.818955]\n",
      "[Epoch 91/200] [Batch 1000/4688] [D loss: -0.384806] [G loss: 1.067616]\n",
      "[Epoch 91/200] [Batch 2000/4688] [D loss: -0.424580] [G loss: 0.105927]\n",
      "[Epoch 91/200] [Batch 3000/4688] [D loss: -0.453497] [G loss: 3.381473]\n",
      "[Epoch 91/200] [Batch 4000/4688] [D loss: -0.433049] [G loss: 3.188691]\n",
      "[Epoch 92/200] [Batch 0/4688] [D loss: -0.289220] [G loss: 2.200481]\n",
      "[Epoch 92/200] [Batch 1000/4688] [D loss: -0.564208] [G loss: 2.777920]\n",
      "[Epoch 92/200] [Batch 2000/4688] [D loss: -0.477192] [G loss: 2.207238]\n",
      "[Epoch 92/200] [Batch 3000/4688] [D loss: -0.416857] [G loss: 2.831530]\n",
      "[Epoch 92/200] [Batch 4000/4688] [D loss: -0.412143] [G loss: 1.043558]\n",
      "[Epoch 93/200] [Batch 0/4688] [D loss: -0.205535] [G loss: 1.604242]\n",
      "[Epoch 93/200] [Batch 1000/4688] [D loss: -0.382034] [G loss: 2.199224]\n",
      "[Epoch 93/200] [Batch 2000/4688] [D loss: -0.453218] [G loss: 2.350610]\n",
      "[Epoch 93/200] [Batch 3000/4688] [D loss: -0.355118] [G loss: 1.716169]\n",
      "[Epoch 93/200] [Batch 4000/4688] [D loss: -0.310243] [G loss: -0.940772]\n",
      "[Epoch 94/200] [Batch 0/4688] [D loss: -0.152535] [G loss: -1.512925]\n",
      "[Epoch 94/200] [Batch 1000/4688] [D loss: -0.221877] [G loss: -2.056830]\n",
      "[Epoch 94/200] [Batch 2000/4688] [D loss: -0.227479] [G loss: -3.088084]\n",
      "[Epoch 94/200] [Batch 3000/4688] [D loss: -0.395982] [G loss: -1.790162]\n",
      "[Epoch 94/200] [Batch 4000/4688] [D loss: -0.306601] [G loss: 0.572551]\n",
      "[Epoch 95/200] [Batch 0/4688] [D loss: -0.260370] [G loss: 0.085306]\n",
      "[Epoch 95/200] [Batch 1000/4688] [D loss: -0.560787] [G loss: -0.037078]\n",
      "[Epoch 95/200] [Batch 2000/4688] [D loss: -0.388756] [G loss: -0.512633]\n",
      "[Epoch 95/200] [Batch 3000/4688] [D loss: -0.645624] [G loss: -2.560287]\n",
      "[Epoch 95/200] [Batch 4000/4688] [D loss: -0.760761] [G loss: -5.201034]\n",
      "[Epoch 96/200] [Batch 0/4688] [D loss: -0.247448] [G loss: -3.073963]\n",
      "[Epoch 96/200] [Batch 1000/4688] [D loss: -0.232376] [G loss: -3.588710]\n",
      "[Epoch 96/200] [Batch 2000/4688] [D loss: -0.266345] [G loss: -4.144649]\n",
      "[Epoch 96/200] [Batch 3000/4688] [D loss: -0.137654] [G loss: -3.388765]\n",
      "[Epoch 96/200] [Batch 4000/4688] [D loss: -0.122624] [G loss: -4.083007]\n",
      "[Epoch 97/200] [Batch 0/4688] [D loss: -0.114710] [G loss: -2.156673]\n",
      "[Epoch 97/200] [Batch 1000/4688] [D loss: -0.327939] [G loss: -3.660295]\n",
      "[Epoch 97/200] [Batch 2000/4688] [D loss: -0.177032] [G loss: -4.347652]\n",
      "[Epoch 97/200] [Batch 3000/4688] [D loss: -0.165040] [G loss: -5.003624]\n",
      "[Epoch 97/200] [Batch 4000/4688] [D loss: -0.130368] [G loss: -4.733303]\n",
      "[Epoch 98/200] [Batch 0/4688] [D loss: -0.116057] [G loss: -2.782558]\n",
      "[Epoch 98/200] [Batch 1000/4688] [D loss: -0.206099] [G loss: -5.665299]\n",
      "[Epoch 98/200] [Batch 2000/4688] [D loss: -0.172882] [G loss: -4.697022]\n",
      "[Epoch 98/200] [Batch 3000/4688] [D loss: -0.180474] [G loss: -3.894408]\n",
      "[Epoch 98/200] [Batch 4000/4688] [D loss: -0.192093] [G loss: -3.127196]\n",
      "[Epoch 99/200] [Batch 0/4688] [D loss: -0.156184] [G loss: -2.035379]\n",
      "[Epoch 99/200] [Batch 1000/4688] [D loss: -0.246898] [G loss: -2.169059]\n",
      "[Epoch 99/200] [Batch 2000/4688] [D loss: -0.444261] [G loss: -1.799818]\n",
      "[Epoch 99/200] [Batch 3000/4688] [D loss: -0.310458] [G loss: -3.067428]\n",
      "[Epoch 99/200] [Batch 4000/4688] [D loss: -0.235495] [G loss: -2.405881]\n",
      "[Epoch 100/200] [Batch 0/4688] [D loss: -0.199801] [G loss: -3.175781]\n",
      "[Epoch 100/200] [Batch 1000/4688] [D loss: -0.239466] [G loss: -3.179080]\n",
      "[Epoch 100/200] [Batch 2000/4688] [D loss: -0.201420] [G loss: -4.080516]\n",
      "[Epoch 100/200] [Batch 3000/4688] [D loss: -0.157823] [G loss: -2.900124]\n",
      "[Epoch 100/200] [Batch 4000/4688] [D loss: -0.191277] [G loss: -3.696297]\n",
      "[Epoch 101/200] [Batch 0/4688] [D loss: -0.126901] [G loss: -2.756435]\n",
      "[Epoch 101/200] [Batch 1000/4688] [D loss: -0.233847] [G loss: -4.012481]\n",
      "[Epoch 101/200] [Batch 2000/4688] [D loss: -0.207138] [G loss: -3.612582]\n",
      "[Epoch 101/200] [Batch 3000/4688] [D loss: -0.193088] [G loss: -3.097836]\n",
      "[Epoch 101/200] [Batch 4000/4688] [D loss: -0.187379] [G loss: -3.296061]\n",
      "[Epoch 102/200] [Batch 0/4688] [D loss: -0.158559] [G loss: -2.614321]\n",
      "[Epoch 102/200] [Batch 1000/4688] [D loss: -0.254678] [G loss: -4.729145]\n",
      "[Epoch 102/200] [Batch 2000/4688] [D loss: -0.254064] [G loss: -5.853291]\n",
      "[Epoch 102/200] [Batch 3000/4688] [D loss: -0.198290] [G loss: -5.379592]\n",
      "[Epoch 102/200] [Batch 4000/4688] [D loss: -0.175591] [G loss: -5.196101]\n",
      "[Epoch 103/200] [Batch 0/4688] [D loss: -0.113419] [G loss: -3.498019]\n",
      "[Epoch 103/200] [Batch 1000/4688] [D loss: -0.197139] [G loss: -4.865540]\n",
      "[Epoch 103/200] [Batch 2000/4688] [D loss: -0.187213] [G loss: -3.532464]\n",
      "[Epoch 103/200] [Batch 3000/4688] [D loss: -0.276686] [G loss: -3.072425]\n",
      "[Epoch 103/200] [Batch 4000/4688] [D loss: -0.229026] [G loss: -4.488287]\n",
      "[Epoch 104/200] [Batch 0/4688] [D loss: -0.149682] [G loss: -2.662838]\n",
      "[Epoch 104/200] [Batch 1000/4688] [D loss: -0.321412] [G loss: -6.074872]\n",
      "[Epoch 104/200] [Batch 2000/4688] [D loss: -0.188361] [G loss: -5.176824]\n",
      "[Epoch 104/200] [Batch 3000/4688] [D loss: -0.306945] [G loss: -3.584339]\n",
      "[Epoch 104/200] [Batch 4000/4688] [D loss: -0.240571] [G loss: -3.066745]\n",
      "[Epoch 105/200] [Batch 0/4688] [D loss: -0.168095] [G loss: -1.263348]\n",
      "[Epoch 105/200] [Batch 1000/4688] [D loss: -0.247167] [G loss: -1.941441]\n",
      "[Epoch 105/200] [Batch 2000/4688] [D loss: -0.248568] [G loss: -4.261795]\n",
      "[Epoch 105/200] [Batch 3000/4688] [D loss: -0.234892] [G loss: -4.072351]\n",
      "[Epoch 105/200] [Batch 4000/4688] [D loss: -0.231053] [G loss: -5.700732]\n",
      "[Epoch 106/200] [Batch 0/4688] [D loss: -0.136148] [G loss: -2.667084]\n",
      "[Epoch 106/200] [Batch 1000/4688] [D loss: -0.280524] [G loss: -3.274615]\n",
      "[Epoch 106/200] [Batch 2000/4688] [D loss: -0.312562] [G loss: -3.128960]\n",
      "[Epoch 106/200] [Batch 3000/4688] [D loss: -0.239368] [G loss: -5.244470]\n",
      "[Epoch 106/200] [Batch 4000/4688] [D loss: -0.298937] [G loss: -3.695038]\n",
      "[Epoch 107/200] [Batch 0/4688] [D loss: -0.169528] [G loss: -3.364852]\n",
      "[Epoch 107/200] [Batch 1000/4688] [D loss: -0.309538] [G loss: -6.954102]\n",
      "[Epoch 107/200] [Batch 2000/4688] [D loss: -0.253885] [G loss: -7.038697]\n",
      "[Epoch 107/200] [Batch 3000/4688] [D loss: -0.229116] [G loss: -5.717815]\n",
      "[Epoch 107/200] [Batch 4000/4688] [D loss: -0.235213] [G loss: -3.289281]\n",
      "[Epoch 108/200] [Batch 0/4688] [D loss: -0.187763] [G loss: -2.395321]\n",
      "[Epoch 108/200] [Batch 1000/4688] [D loss: -0.267643] [G loss: -1.458925]\n",
      "[Epoch 108/200] [Batch 2000/4688] [D loss: -0.402238] [G loss: -1.625683]\n",
      "[Epoch 108/200] [Batch 3000/4688] [D loss: -0.369630] [G loss: -3.316660]\n",
      "[Epoch 108/200] [Batch 4000/4688] [D loss: -0.446154] [G loss: 0.141781]\n",
      "[Epoch 109/200] [Batch 0/4688] [D loss: -0.156343] [G loss: 0.150427]\n",
      "[Epoch 109/200] [Batch 1000/4688] [D loss: -0.276932] [G loss: -1.735779]\n",
      "[Epoch 109/200] [Batch 2000/4688] [D loss: -0.280629] [G loss: 0.098911]\n",
      "[Epoch 109/200] [Batch 3000/4688] [D loss: -0.228557] [G loss: -2.556734]\n",
      "[Epoch 109/200] [Batch 4000/4688] [D loss: -0.348640] [G loss: -2.633285]\n",
      "[Epoch 110/200] [Batch 0/4688] [D loss: -0.283514] [G loss: -1.332406]\n",
      "[Epoch 110/200] [Batch 1000/4688] [D loss: -0.366345] [G loss: -0.495985]\n",
      "[Epoch 110/200] [Batch 2000/4688] [D loss: -0.296689] [G loss: -1.550452]\n",
      "[Epoch 110/200] [Batch 3000/4688] [D loss: -0.385160] [G loss: 0.296941]\n",
      "[Epoch 110/200] [Batch 4000/4688] [D loss: -0.260633] [G loss: -0.140862]\n",
      "[Epoch 111/200] [Batch 0/4688] [D loss: -0.228660] [G loss: -2.736937]\n",
      "[Epoch 111/200] [Batch 1000/4688] [D loss: -0.315448] [G loss: -4.214344]\n",
      "[Epoch 111/200] [Batch 2000/4688] [D loss: -0.608204] [G loss: 0.375351]\n",
      "[Epoch 111/200] [Batch 3000/4688] [D loss: -0.425636] [G loss: 0.591143]\n",
      "[Epoch 111/200] [Batch 4000/4688] [D loss: -0.343715] [G loss: -2.214943]\n",
      "[Epoch 112/200] [Batch 0/4688] [D loss: -0.262700] [G loss: -0.557751]\n",
      "[Epoch 112/200] [Batch 1000/4688] [D loss: -0.411000] [G loss: -0.221297]\n",
      "[Epoch 112/200] [Batch 2000/4688] [D loss: -0.387995] [G loss: -0.046433]\n",
      "[Epoch 112/200] [Batch 3000/4688] [D loss: -0.392778] [G loss: -0.620764]\n",
      "[Epoch 112/200] [Batch 4000/4688] [D loss: -0.279686] [G loss: -0.274251]\n",
      "[Epoch 113/200] [Batch 0/4688] [D loss: -0.203889] [G loss: -0.955663]\n",
      "[Epoch 113/200] [Batch 1000/4688] [D loss: -0.328065] [G loss: -3.626974]\n",
      "[Epoch 113/200] [Batch 2000/4688] [D loss: -0.438870] [G loss: -2.900153]\n",
      "[Epoch 113/200] [Batch 3000/4688] [D loss: -0.431152] [G loss: 1.315758]\n",
      "[Epoch 113/200] [Batch 4000/4688] [D loss: -0.343746] [G loss: -1.495752]\n",
      "[Epoch 114/200] [Batch 0/4688] [D loss: -0.245543] [G loss: -1.322953]\n",
      "[Epoch 114/200] [Batch 1000/4688] [D loss: -0.588908] [G loss: -2.386341]\n",
      "[Epoch 114/200] [Batch 2000/4688] [D loss: -0.415274] [G loss: -2.155655]\n",
      "[Epoch 114/200] [Batch 3000/4688] [D loss: -0.414296] [G loss: -3.739086]\n",
      "[Epoch 114/200] [Batch 4000/4688] [D loss: -0.338547] [G loss: -3.756481]\n",
      "[Epoch 115/200] [Batch 0/4688] [D loss: -0.336584] [G loss: -0.342881]\n",
      "[Epoch 115/200] [Batch 1000/4688] [D loss: -0.395556] [G loss: -2.299239]\n",
      "[Epoch 115/200] [Batch 2000/4688] [D loss: -0.308168] [G loss: 1.321927]\n",
      "[Epoch 115/200] [Batch 3000/4688] [D loss: -0.436429] [G loss: 1.746447]\n",
      "[Epoch 115/200] [Batch 4000/4688] [D loss: -0.411350] [G loss: 1.516001]\n",
      "[Epoch 116/200] [Batch 0/4688] [D loss: -0.315186] [G loss: 1.471087]\n",
      "[Epoch 116/200] [Batch 1000/4688] [D loss: -0.531384] [G loss: 0.968601]\n",
      "[Epoch 116/200] [Batch 2000/4688] [D loss: -0.357701] [G loss: -0.657153]\n",
      "[Epoch 116/200] [Batch 3000/4688] [D loss: -0.269307] [G loss: 0.912763]\n",
      "[Epoch 116/200] [Batch 4000/4688] [D loss: -0.330378] [G loss: -0.255724]\n",
      "[Epoch 117/200] [Batch 0/4688] [D loss: -0.310460] [G loss: -0.464353]\n",
      "[Epoch 117/200] [Batch 1000/4688] [D loss: -0.515965] [G loss: -2.033786]\n",
      "[Epoch 117/200] [Batch 2000/4688] [D loss: -0.368560] [G loss: -0.939470]\n",
      "[Epoch 117/200] [Batch 3000/4688] [D loss: -0.245887] [G loss: -0.044864]\n",
      "[Epoch 117/200] [Batch 4000/4688] [D loss: -0.432785] [G loss: -1.388678]\n",
      "[Epoch 118/200] [Batch 0/4688] [D loss: -0.192679] [G loss: 1.425565]\n",
      "[Epoch 118/200] [Batch 1000/4688] [D loss: -0.144604] [G loss: 0.942398]\n",
      "[Epoch 118/200] [Batch 2000/4688] [D loss: -0.240775] [G loss: 1.275058]\n",
      "[Epoch 118/200] [Batch 3000/4688] [D loss: -0.417476] [G loss: 0.440914]\n",
      "[Epoch 118/200] [Batch 4000/4688] [D loss: -0.423109] [G loss: 1.539120]\n",
      "[Epoch 119/200] [Batch 0/4688] [D loss: -0.166226] [G loss: 0.696726]\n",
      "[Epoch 119/200] [Batch 1000/4688] [D loss: -0.282875] [G loss: 0.227567]\n",
      "[Epoch 119/200] [Batch 2000/4688] [D loss: -0.270392] [G loss: 0.437628]\n",
      "[Epoch 119/200] [Batch 3000/4688] [D loss: -0.436477] [G loss: -2.159846]\n",
      "[Epoch 119/200] [Batch 4000/4688] [D loss: -0.367863] [G loss: 0.346245]\n",
      "[Epoch 120/200] [Batch 0/4688] [D loss: -0.224424] [G loss: -0.838998]\n",
      "[Epoch 120/200] [Batch 1000/4688] [D loss: -0.356620] [G loss: 0.806686]\n",
      "[Epoch 120/200] [Batch 2000/4688] [D loss: -0.305594] [G loss: 1.575201]\n",
      "[Epoch 120/200] [Batch 3000/4688] [D loss: -0.263110] [G loss: 2.275753]\n",
      "[Epoch 120/200] [Batch 4000/4688] [D loss: -0.608854] [G loss: -0.921517]\n",
      "[Epoch 121/200] [Batch 0/4688] [D loss: -0.242319] [G loss: 0.153417]\n",
      "[Epoch 121/200] [Batch 1000/4688] [D loss: -0.568379] [G loss: -1.963767]\n",
      "[Epoch 121/200] [Batch 2000/4688] [D loss: -0.432526] [G loss: 1.186477]\n",
      "[Epoch 121/200] [Batch 3000/4688] [D loss: -0.337198] [G loss: 0.235126]\n",
      "[Epoch 121/200] [Batch 4000/4688] [D loss: -0.443487] [G loss: 0.831277]\n",
      "[Epoch 122/200] [Batch 0/4688] [D loss: -0.171783] [G loss: -1.191387]\n",
      "[Epoch 122/200] [Batch 1000/4688] [D loss: -0.297312] [G loss: 0.055451]\n",
      "[Epoch 122/200] [Batch 2000/4688] [D loss: -0.615769] [G loss: -0.221092]\n",
      "[Epoch 122/200] [Batch 3000/4688] [D loss: -0.327439] [G loss: -3.163750]\n",
      "[Epoch 122/200] [Batch 4000/4688] [D loss: -0.825625] [G loss: -2.131914]\n",
      "[Epoch 123/200] [Batch 0/4688] [D loss: -0.294886] [G loss: -0.129242]\n",
      "[Epoch 123/200] [Batch 1000/4688] [D loss: -0.368333] [G loss: 1.396890]\n",
      "[Epoch 123/200] [Batch 2000/4688] [D loss: -0.395379] [G loss: 1.362285]\n",
      "[Epoch 123/200] [Batch 3000/4688] [D loss: -0.291656] [G loss: -1.632359]\n",
      "[Epoch 123/200] [Batch 4000/4688] [D loss: -0.422952] [G loss: 0.198914]\n",
      "[Epoch 124/200] [Batch 0/4688] [D loss: -0.261035] [G loss: 1.583297]\n",
      "[Epoch 124/200] [Batch 1000/4688] [D loss: -0.330771] [G loss: 1.700387]\n",
      "[Epoch 124/200] [Batch 2000/4688] [D loss: -0.404183] [G loss: -1.863356]\n",
      "[Epoch 124/200] [Batch 3000/4688] [D loss: -0.584060] [G loss: -3.337988]\n",
      "[Epoch 124/200] [Batch 4000/4688] [D loss: -0.481347] [G loss: -2.794022]\n",
      "[Epoch 125/200] [Batch 0/4688] [D loss: -0.245995] [G loss: -0.333999]\n",
      "[Epoch 125/200] [Batch 1000/4688] [D loss: -0.366664] [G loss: -3.659147]\n",
      "[Epoch 125/200] [Batch 2000/4688] [D loss: -0.353224] [G loss: -4.592528]\n",
      "[Epoch 125/200] [Batch 3000/4688] [D loss: -0.675950] [G loss: -1.247484]\n",
      "[Epoch 125/200] [Batch 4000/4688] [D loss: -0.389635] [G loss: -0.356525]\n",
      "[Epoch 126/200] [Batch 0/4688] [D loss: -0.213676] [G loss: -1.547108]\n",
      "[Epoch 126/200] [Batch 1000/4688] [D loss: -0.382815] [G loss: 1.377212]\n",
      "[Epoch 126/200] [Batch 2000/4688] [D loss: -0.393462] [G loss: 0.275610]\n",
      "[Epoch 126/200] [Batch 3000/4688] [D loss: -0.435016] [G loss: -0.575361]\n",
      "[Epoch 126/200] [Batch 4000/4688] [D loss: -0.394242] [G loss: -0.130084]\n",
      "[Epoch 127/200] [Batch 0/4688] [D loss: -0.257493] [G loss: -0.069157]\n",
      "[Epoch 127/200] [Batch 1000/4688] [D loss: -0.454279] [G loss: -4.546409]\n",
      "[Epoch 127/200] [Batch 2000/4688] [D loss: -0.472378] [G loss: -4.590932]\n",
      "[Epoch 127/200] [Batch 3000/4688] [D loss: -0.506401] [G loss: -4.184131]\n",
      "[Epoch 127/200] [Batch 4000/4688] [D loss: -0.507009] [G loss: -5.154587]\n",
      "[Epoch 128/200] [Batch 0/4688] [D loss: -0.259587] [G loss: -4.505435]\n",
      "[Epoch 128/200] [Batch 1000/4688] [D loss: -0.385123] [G loss: -4.579312]\n",
      "[Epoch 128/200] [Batch 2000/4688] [D loss: -0.478684] [G loss: -6.304129]\n",
      "[Epoch 128/200] [Batch 3000/4688] [D loss: -0.551168] [G loss: -6.444145]\n",
      "[Epoch 128/200] [Batch 4000/4688] [D loss: -0.559267] [G loss: -7.368471]\n",
      "[Epoch 129/200] [Batch 0/4688] [D loss: -0.248865] [G loss: -4.382956]\n",
      "[Epoch 129/200] [Batch 1000/4688] [D loss: -0.474760] [G loss: -3.947435]\n",
      "[Epoch 129/200] [Batch 2000/4688] [D loss: -0.549594] [G loss: -3.039525]\n",
      "[Epoch 129/200] [Batch 3000/4688] [D loss: -0.537931] [G loss: -4.377590]\n",
      "[Epoch 129/200] [Batch 4000/4688] [D loss: -0.406040] [G loss: -4.237099]\n",
      "[Epoch 130/200] [Batch 0/4688] [D loss: -0.330968] [G loss: -2.255502]\n",
      "[Epoch 130/200] [Batch 1000/4688] [D loss: -0.543897] [G loss: -4.916893]\n",
      "[Epoch 130/200] [Batch 2000/4688] [D loss: -0.594806] [G loss: -5.096350]\n",
      "[Epoch 130/200] [Batch 3000/4688] [D loss: -0.468003] [G loss: -3.631756]\n",
      "[Epoch 130/200] [Batch 4000/4688] [D loss: -0.458699] [G loss: -3.959772]\n",
      "[Epoch 131/200] [Batch 0/4688] [D loss: -0.401169] [G loss: -1.585163]\n",
      "[Epoch 131/200] [Batch 1000/4688] [D loss: -0.540534] [G loss: -0.919725]\n",
      "[Epoch 131/200] [Batch 2000/4688] [D loss: -0.584922] [G loss: -2.738777]\n",
      "[Epoch 131/200] [Batch 3000/4688] [D loss: -0.555993] [G loss: -3.212421]\n",
      "[Epoch 131/200] [Batch 4000/4688] [D loss: -0.503813] [G loss: -5.299710]\n",
      "[Epoch 132/200] [Batch 0/4688] [D loss: -0.321578] [G loss: -2.599707]\n",
      "[Epoch 132/200] [Batch 1000/4688] [D loss: -0.495133] [G loss: -3.470938]\n",
      "[Epoch 132/200] [Batch 2000/4688] [D loss: -0.386706] [G loss: -3.172138]\n",
      "[Epoch 132/200] [Batch 3000/4688] [D loss: -0.398212] [G loss: -2.100242]\n",
      "[Epoch 132/200] [Batch 4000/4688] [D loss: -0.406249] [G loss: -2.955277]\n",
      "[Epoch 133/200] [Batch 0/4688] [D loss: -0.392588] [G loss: -0.768563]\n",
      "[Epoch 133/200] [Batch 1000/4688] [D loss: -0.582946] [G loss: -3.399529]\n",
      "[Epoch 133/200] [Batch 2000/4688] [D loss: -0.317703] [G loss: -4.190697]\n",
      "[Epoch 133/200] [Batch 3000/4688] [D loss: -0.500373] [G loss: 0.158665]\n",
      "[Epoch 133/200] [Batch 4000/4688] [D loss: -0.531474] [G loss: -3.057018]\n",
      "[Epoch 134/200] [Batch 0/4688] [D loss: -0.338436] [G loss: -0.980805]\n",
      "[Epoch 134/200] [Batch 1000/4688] [D loss: -0.303063] [G loss: 1.485574]\n",
      "[Epoch 134/200] [Batch 2000/4688] [D loss: -0.479903] [G loss: -2.276318]\n",
      "[Epoch 134/200] [Batch 3000/4688] [D loss: -0.375899] [G loss: -2.726768]\n",
      "[Epoch 134/200] [Batch 4000/4688] [D loss: -0.523376] [G loss: -2.227461]\n",
      "[Epoch 135/200] [Batch 0/4688] [D loss: -0.384358] [G loss: 2.171583]\n",
      "[Epoch 135/200] [Batch 1000/4688] [D loss: -0.291602] [G loss: 4.499844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 135/200] [Batch 2000/4688] [D loss: -0.314845] [G loss: 0.103336]\n",
      "[Epoch 135/200] [Batch 3000/4688] [D loss: -0.516158] [G loss: -1.672189]\n",
      "[Epoch 135/200] [Batch 4000/4688] [D loss: -0.423268] [G loss: -1.883338]\n",
      "[Epoch 136/200] [Batch 0/4688] [D loss: -0.218471] [G loss: -0.414915]\n",
      "[Epoch 136/200] [Batch 1000/4688] [D loss: -0.430220] [G loss: -0.358059]\n",
      "[Epoch 136/200] [Batch 2000/4688] [D loss: -0.448017] [G loss: -2.645470]\n",
      "[Epoch 136/200] [Batch 3000/4688] [D loss: -0.410012] [G loss: -1.637460]\n",
      "[Epoch 136/200] [Batch 4000/4688] [D loss: -0.312750] [G loss: -1.378755]\n",
      "[Epoch 137/200] [Batch 0/4688] [D loss: -0.286157] [G loss: -2.548769]\n",
      "[Epoch 137/200] [Batch 1000/4688] [D loss: -0.505851] [G loss: -1.202388]\n",
      "[Epoch 137/200] [Batch 2000/4688] [D loss: -0.361435] [G loss: -1.921941]\n",
      "[Epoch 137/200] [Batch 3000/4688] [D loss: -0.391054] [G loss: -3.446549]\n",
      "[Epoch 137/200] [Batch 4000/4688] [D loss: -0.363261] [G loss: -2.901456]\n",
      "[Epoch 138/200] [Batch 0/4688] [D loss: -0.361591] [G loss: -2.116814]\n",
      "[Epoch 138/200] [Batch 1000/4688] [D loss: -0.383026] [G loss: -3.260592]\n",
      "[Epoch 138/200] [Batch 2000/4688] [D loss: -0.436784] [G loss: -3.016372]\n",
      "[Epoch 138/200] [Batch 3000/4688] [D loss: -0.415259] [G loss: -3.214727]\n",
      "[Epoch 138/200] [Batch 4000/4688] [D loss: -0.531729] [G loss: -3.574267]\n",
      "[Epoch 139/200] [Batch 0/4688] [D loss: -0.233073] [G loss: -2.134278]\n",
      "[Epoch 139/200] [Batch 1000/4688] [D loss: -0.399527] [G loss: -3.708177]\n",
      "[Epoch 139/200] [Batch 2000/4688] [D loss: -0.395505] [G loss: -2.363583]\n",
      "[Epoch 139/200] [Batch 3000/4688] [D loss: -0.640727] [G loss: -1.295332]\n",
      "[Epoch 139/200] [Batch 4000/4688] [D loss: -0.588178] [G loss: 0.049856]\n",
      "[Epoch 140/200] [Batch 0/4688] [D loss: -0.560964] [G loss: -1.099365]\n",
      "[Epoch 140/200] [Batch 1000/4688] [D loss: -0.415275] [G loss: -0.766245]\n",
      "[Epoch 140/200] [Batch 2000/4688] [D loss: -0.294579] [G loss: 0.376869]\n",
      "[Epoch 140/200] [Batch 3000/4688] [D loss: -0.277152] [G loss: -1.989871]\n",
      "[Epoch 140/200] [Batch 4000/4688] [D loss: -0.439706] [G loss: -3.346772]\n",
      "[Epoch 141/200] [Batch 0/4688] [D loss: -0.384088] [G loss: -3.071030]\n",
      "[Epoch 141/200] [Batch 1000/4688] [D loss: -0.650410] [G loss: 0.236638]\n",
      "[Epoch 141/200] [Batch 2000/4688] [D loss: -0.391229] [G loss: -1.136512]\n",
      "[Epoch 141/200] [Batch 3000/4688] [D loss: -0.430980] [G loss: -3.265410]\n",
      "[Epoch 141/200] [Batch 4000/4688] [D loss: -0.295563] [G loss: -2.792955]\n",
      "[Epoch 142/200] [Batch 0/4688] [D loss: -0.355628] [G loss: -1.588947]\n",
      "[Epoch 142/200] [Batch 1000/4688] [D loss: -0.380245] [G loss: -0.361298]\n",
      "[Epoch 142/200] [Batch 2000/4688] [D loss: -0.451826] [G loss: 0.015179]\n",
      "[Epoch 142/200] [Batch 3000/4688] [D loss: -0.391291] [G loss: -0.887997]\n",
      "[Epoch 142/200] [Batch 4000/4688] [D loss: -0.411580] [G loss: 1.123872]\n",
      "[Epoch 143/200] [Batch 0/4688] [D loss: -0.097132] [G loss: 1.412511]\n",
      "[Epoch 143/200] [Batch 1000/4688] [D loss: -0.180940] [G loss: 0.953987]\n",
      "[Epoch 143/200] [Batch 2000/4688] [D loss: -0.307334] [G loss: -0.743598]\n",
      "[Epoch 143/200] [Batch 3000/4688] [D loss: -0.297097] [G loss: 0.151570]\n",
      "[Epoch 143/200] [Batch 4000/4688] [D loss: -0.483239] [G loss: 0.840355]\n",
      "[Epoch 144/200] [Batch 0/4688] [D loss: -0.451009] [G loss: -0.110503]\n",
      "[Epoch 144/200] [Batch 1000/4688] [D loss: -0.807016] [G loss: -2.004372]\n",
      "[Epoch 144/200] [Batch 2000/4688] [D loss: -0.405971] [G loss: -0.361886]\n",
      "[Epoch 144/200] [Batch 3000/4688] [D loss: -0.375937] [G loss: -0.411626]\n",
      "[Epoch 144/200] [Batch 4000/4688] [D loss: -0.387839] [G loss: -0.808121]\n",
      "[Epoch 145/200] [Batch 0/4688] [D loss: -0.195985] [G loss: -0.818513]\n",
      "[Epoch 145/200] [Batch 1000/4688] [D loss: -0.336375] [G loss: 0.196037]\n",
      "[Epoch 145/200] [Batch 2000/4688] [D loss: -0.248144] [G loss: 0.671601]\n",
      "[Epoch 145/200] [Batch 3000/4688] [D loss: -0.330326] [G loss: -0.022606]\n",
      "[Epoch 145/200] [Batch 4000/4688] [D loss: -0.460468] [G loss: 0.296600]\n",
      "[Epoch 146/200] [Batch 0/4688] [D loss: -0.305875] [G loss: 0.379825]\n",
      "[Epoch 146/200] [Batch 1000/4688] [D loss: -0.421386] [G loss: 0.019361]\n",
      "[Epoch 146/200] [Batch 2000/4688] [D loss: -0.304373] [G loss: -2.156488]\n",
      "[Epoch 146/200] [Batch 3000/4688] [D loss: -0.450425] [G loss: -4.914781]\n",
      "[Epoch 146/200] [Batch 4000/4688] [D loss: -0.455135] [G loss: -1.681741]\n",
      "[Epoch 147/200] [Batch 0/4688] [D loss: -0.294092] [G loss: -1.587380]\n",
      "[Epoch 147/200] [Batch 1000/4688] [D loss: -0.349995] [G loss: -5.811655]\n",
      "[Epoch 147/200] [Batch 2000/4688] [D loss: -0.370910] [G loss: -5.582180]\n",
      "[Epoch 147/200] [Batch 3000/4688] [D loss: -0.257278] [G loss: -5.270646]\n",
      "[Epoch 147/200] [Batch 4000/4688] [D loss: -0.382382] [G loss: -3.232796]\n",
      "[Epoch 148/200] [Batch 0/4688] [D loss: -0.261502] [G loss: -0.004042]\n",
      "[Epoch 148/200] [Batch 1000/4688] [D loss: -0.292996] [G loss: -2.959306]\n",
      "[Epoch 148/200] [Batch 2000/4688] [D loss: -0.335121] [G loss: -3.656773]\n",
      "[Epoch 148/200] [Batch 3000/4688] [D loss: -0.586354] [G loss: -4.191750]\n",
      "[Epoch 148/200] [Batch 4000/4688] [D loss: -0.458805] [G loss: -5.349318]\n",
      "[Epoch 149/200] [Batch 0/4688] [D loss: -0.251024] [G loss: -3.102925]\n",
      "[Epoch 149/200] [Batch 1000/4688] [D loss: -0.330919] [G loss: -1.720448]\n",
      "[Epoch 149/200] [Batch 2000/4688] [D loss: -0.343030] [G loss: -3.442749]\n",
      "[Epoch 149/200] [Batch 3000/4688] [D loss: -0.460877] [G loss: -2.897895]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-52cd5dad27bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-db8e96cda090>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \"\"\"\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[1;34m(pic, mode)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input type {} is not supported'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2670\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   2606\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m  \u001b[1;31m# may change to (mode, 0, 1) post-1.1.6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2607\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_MAPMODES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2608\u001b[1;33m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2609\u001b[0m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2610\u001b[0m             \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadonly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mnew\u001b[1;34m(mode, size, color)\u001b[0m\n\u001b[0;32m   2507\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImagePalette\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2508\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2509\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(\"images/cwganweb_disc5_rmsprop\", exist_ok=True)\n",
    "\n",
    "\n",
    "digit_embeddings = np.load(\"digit_embeddings.npy\")\n",
    "\n",
    "class Arguments():\n",
    "  def __init__(self):\n",
    "    self.n_epochs = 200\n",
    "    self.batch_size = 64\n",
    "    self.lr = 0.00005\n",
    "    self.b1 = 0.5\n",
    "    self.b2 = 0.999\n",
    "    self.n_cpu = 8\n",
    "    self.latent_dim = 100\n",
    "    self.n_classes = 50\n",
    "    self.img_size = 32\n",
    "    self.channels = 1\n",
    "    self.sample_interval = 1000\n",
    "    self.embedding_size = 50\n",
    "    self.n_generator = 5\n",
    "    self.clip_value = 0.01\n",
    "\n",
    "opt = Arguments()\n",
    "\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size*2)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Linear(opt.embedding_size, opt.embedding_size)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.embedding_size, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Linear(opt.embedding_size, opt.embedding_size)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.embedding_size + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n",
    "\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Configure data loader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    CustomTensorDataset(tensors = (x, y), transform = transforms.Compose(\n",
    "            [transforms.ToPILImage(), transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "       )),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "#\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=opt.lr)#, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=opt.lr)#, betas=(opt.b1, opt.b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "#numbers = np.random.randint(0, 49, [10])\n",
    "#numbers = np.sort(numbers)\n",
    "numbers = [1, 22, 23, 26, 29, 33, 35, 37, 42, 48]\n",
    "print(\"Numbers\", numbers)\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    \"\"\"does not use n_row\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (10*10, opt.latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(10) for num in numbers])\n",
    "    gen_labels = Variable(FloatTensor(digit_embeddings[labels]))\n",
    "    gen_imgs = generator(z, gen_labels)\n",
    "    save_image(gen_imgs.data, \"images/cwganweb_disc5_rmsprop/%d.png\" % batches_done, nrow=10, normalize=True)\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "total_d_loss = 0.0\n",
    "total_g_loss = 0.0\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(FloatTensor(digit_embeddings[labels]))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        if i%opt.n_generator==0:\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Sample noise and labels as generator input\n",
    "            z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
    "            gen_labels = Variable(FloatTensor(digit_embeddings[np.random.randint(0, opt.n_classes, batch_size)]))\n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            validity = discriminator(gen_imgs, gen_labels)\n",
    "            g_loss = -torch.mean(validity)\n",
    "            total_g_loss += g_loss.item()\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        validity_real = discriminator(real_imgs, labels)\n",
    "\n",
    "        # Loss for fake images\n",
    "        validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = -torch.mean(validity_real)+torch.mean(validity_fake) \n",
    "\n",
    "        total_d_loss += d_loss.item()\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        # Clip weights of discriminator\n",
    "        for p in discriminator.parameters():\n",
    "            p.data.clamp_(-opt.clip_value, opt.clip_value)\n",
    "            \n",
    "            \n",
    "        if i%1000==0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, opt.n_epochs, i, len(dataloader), total_d_loss/1000, total_g_loss/1000)\n",
    "            )\n",
    "            total_d_loss = 0.0\n",
    "            total_g_loss = 0.0\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            sample_image(n_row=20, batches_done=batches_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

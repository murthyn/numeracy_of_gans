{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(300000, 28, 56)\n",
      "(300000,)\n"
     ]
    }
   ],
   "source": [
    "x = np.load(\"data/xtrain32.npy\")\n",
    "\n",
    "y = np.load(\"data/ytrain.npy\")\n",
    "print(x[0])\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[250004])\n",
    "print(y[250004])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(x[:60000, :, :]).to(torch.int8)\n",
    "y = torch.Tensor(y[:60000]).to(torch.int8)\n",
    "\n",
    "#x = np.array(x[:60000, :, 28:], dtype=np.uint8)\n",
    "#y = torch.Tensor(np.array(y[:60000], dtype=np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5, dtype=torch.int8)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0])\n",
    "print(y[0])\n",
    "#print(torch.max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "        x = Image.fromarray(x.numpy(), mode='L')\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[Epoch 0/200] [Batch 0/938] [D loss: 0.000507] [G loss: 0.001020]\n",
      "[Epoch 1/200] [Batch 0/938] [D loss: 0.205733] [G loss: 0.349204]\n",
      "[Epoch 2/200] [Batch 0/938] [D loss: 0.209261] [G loss: 0.326962]\n",
      "[Epoch 3/200] [Batch 0/938] [D loss: 0.176583] [G loss: 0.401426]\n",
      "[Epoch 4/200] [Batch 0/938] [D loss: 0.168161] [G loss: 0.418049]\n",
      "[Epoch 5/200] [Batch 0/938] [D loss: 0.174809] [G loss: 0.399600]\n",
      "[Epoch 6/200] [Batch 0/938] [D loss: 0.185143] [G loss: 0.375236]\n",
      "[Epoch 7/200] [Batch 0/938] [D loss: 0.190745] [G loss: 0.360368]\n",
      "[Epoch 8/200] [Batch 0/938] [D loss: 0.193477] [G loss: 0.352444]\n",
      "[Epoch 9/200] [Batch 0/938] [D loss: 0.194708] [G loss: 0.349400]\n",
      "[Epoch 10/200] [Batch 0/938] [D loss: 0.193299] [G loss: 0.353592]\n",
      "[Epoch 11/200] [Batch 0/938] [D loss: 0.199947] [G loss: 0.338290]\n",
      "[Epoch 12/200] [Batch 0/938] [D loss: 0.207449] [G loss: 0.318437]\n",
      "[Epoch 13/200] [Batch 0/938] [D loss: 0.210672] [G loss: 0.311471]\n",
      "[Epoch 14/200] [Batch 0/938] [D loss: 0.214776] [G loss: 0.301860]\n",
      "[Epoch 15/200] [Batch 0/938] [D loss: 0.217269] [G loss: 0.294686]\n",
      "[Epoch 16/200] [Batch 0/938] [D loss: 0.219126] [G loss: 0.288273]\n",
      "[Epoch 17/200] [Batch 0/938] [D loss: 0.221448] [G loss: 0.282402]\n",
      "[Epoch 18/200] [Batch 0/938] [D loss: 0.223138] [G loss: 0.278551]\n",
      "[Epoch 19/200] [Batch 0/938] [D loss: 0.223913] [G loss: 0.275697]\n",
      "[Epoch 20/200] [Batch 0/938] [D loss: 0.224786] [G loss: 0.273657]\n",
      "[Epoch 21/200] [Batch 0/938] [D loss: 0.224945] [G loss: 0.272115]\n",
      "[Epoch 22/200] [Batch 0/938] [D loss: 0.226380] [G loss: 0.268440]\n",
      "[Epoch 23/200] [Batch 0/938] [D loss: 0.226041] [G loss: 0.268755]\n",
      "[Epoch 24/200] [Batch 0/938] [D loss: 0.226904] [G loss: 0.266378]\n",
      "[Epoch 25/200] [Batch 0/938] [D loss: 0.227791] [G loss: 0.264656]\n",
      "[Epoch 26/200] [Batch 0/938] [D loss: 0.227585] [G loss: 0.263614]\n",
      "[Epoch 27/200] [Batch 0/938] [D loss: 0.228516] [G loss: 0.262018]\n",
      "[Epoch 28/200] [Batch 0/938] [D loss: 0.226612] [G loss: 0.266569]\n",
      "[Epoch 29/200] [Batch 0/938] [D loss: 0.227729] [G loss: 0.262725]\n",
      "[Epoch 30/200] [Batch 0/938] [D loss: 0.228374] [G loss: 0.262144]\n",
      "[Epoch 31/200] [Batch 0/938] [D loss: 0.227186] [G loss: 0.263843]\n",
      "[Epoch 32/200] [Batch 0/938] [D loss: 0.227759] [G loss: 0.262108]\n",
      "[Epoch 33/200] [Batch 0/938] [D loss: 0.228592] [G loss: 0.259368]\n",
      "[Epoch 34/200] [Batch 0/938] [D loss: 0.228463] [G loss: 0.259931]\n",
      "[Epoch 35/200] [Batch 0/938] [D loss: 0.229338] [G loss: 0.257464]\n",
      "[Epoch 36/200] [Batch 0/938] [D loss: 0.230981] [G loss: 0.254150]\n",
      "[Epoch 37/200] [Batch 0/938] [D loss: 0.228391] [G loss: 0.259624]\n",
      "[Epoch 38/200] [Batch 0/938] [D loss: 0.229669] [G loss: 0.256194]\n",
      "[Epoch 39/200] [Batch 0/938] [D loss: 0.230313] [G loss: 0.253544]\n",
      "[Epoch 40/200] [Batch 0/938] [D loss: 0.230530] [G loss: 0.254217]\n",
      "[Epoch 41/200] [Batch 0/938] [D loss: 0.229270] [G loss: 0.256825]\n",
      "[Epoch 42/200] [Batch 0/938] [D loss: 0.229852] [G loss: 0.254946]\n",
      "[Epoch 43/200] [Batch 0/938] [D loss: 0.230859] [G loss: 0.251287]\n",
      "[Epoch 44/200] [Batch 0/938] [D loss: 0.230718] [G loss: 0.255125]\n",
      "[Epoch 45/200] [Batch 0/938] [D loss: 0.230432] [G loss: 0.251768]\n",
      "[Epoch 46/200] [Batch 0/938] [D loss: 0.231753] [G loss: 0.249552]\n",
      "[Epoch 47/200] [Batch 0/938] [D loss: 0.231112] [G loss: 0.251364]\n",
      "[Epoch 48/200] [Batch 0/938] [D loss: 0.231336] [G loss: 0.251091]\n",
      "[Epoch 49/200] [Batch 0/938] [D loss: 0.230324] [G loss: 0.253907]\n",
      "[Epoch 50/200] [Batch 0/938] [D loss: 0.230690] [G loss: 0.253543]\n",
      "[Epoch 51/200] [Batch 0/938] [D loss: 0.230326] [G loss: 0.252713]\n",
      "[Epoch 52/200] [Batch 0/938] [D loss: 0.230821] [G loss: 0.251932]\n",
      "[Epoch 53/200] [Batch 0/938] [D loss: 0.231910] [G loss: 0.249368]\n",
      "[Epoch 54/200] [Batch 0/938] [D loss: 0.230997] [G loss: 0.252201]\n",
      "[Epoch 55/200] [Batch 0/938] [D loss: 0.231503] [G loss: 0.249704]\n",
      "[Epoch 56/200] [Batch 0/938] [D loss: 0.230917] [G loss: 0.251044]\n",
      "[Epoch 57/200] [Batch 0/938] [D loss: 0.231435] [G loss: 0.249886]\n",
      "[Epoch 58/200] [Batch 0/938] [D loss: 0.231725] [G loss: 0.249048]\n",
      "[Epoch 59/200] [Batch 0/938] [D loss: 0.231260] [G loss: 0.250787]\n",
      "[Epoch 60/200] [Batch 0/938] [D loss: 0.231705] [G loss: 0.249843]\n",
      "[Epoch 61/200] [Batch 0/938] [D loss: 0.234797] [G loss: 0.242927]\n",
      "[Epoch 62/200] [Batch 0/938] [D loss: 0.231826] [G loss: 0.248649]\n",
      "[Epoch 63/200] [Batch 0/938] [D loss: 0.233577] [G loss: 0.244784]\n",
      "[Epoch 64/200] [Batch 0/938] [D loss: 0.231142] [G loss: 0.249893]\n",
      "[Epoch 65/200] [Batch 0/938] [D loss: 0.231360] [G loss: 0.250618]\n",
      "[Epoch 66/200] [Batch 0/938] [D loss: 0.231968] [G loss: 0.247902]\n",
      "[Epoch 67/200] [Batch 0/938] [D loss: 0.231979] [G loss: 0.248057]\n",
      "[Epoch 68/200] [Batch 0/938] [D loss: 0.232156] [G loss: 0.247629]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-73f0f8cd446f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mg_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0moptimizer_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;31m# ---------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(\"images/big10\", exist_ok=True)\n",
    "\n",
    "\n",
    "digit_embeddings = np.load(\"digit_embeddings.npy\")\n",
    "\n",
    "class Arguments():\n",
    "  def __init__(self):\n",
    "    self.n_epochs = 200\n",
    "    self.batch_size = 64\n",
    "    self.lr = 0.0002\n",
    "    self.b1 = 0.5\n",
    "    self.b2 = 0.999\n",
    "    self.n_cpu = 8\n",
    "    self.latent_dim = 100\n",
    "    self.n_classes = 10\n",
    "    self.img_size = 32\n",
    "    self.channels = 1\n",
    "    self.sample_interval = 1000\n",
    "    self.embedding_size = 50\n",
    "    self.n_discriminator = 1\n",
    "\n",
    "opt = Arguments()\n",
    "\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size*2)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Linear(opt.embedding_size, opt.embedding_size)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.embedding_size, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Linear(opt.embedding_size, opt.embedding_size)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.embedding_size + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n",
    "\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Configure data loader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    CustomTensorDataset(tensors = (x, y), transform = transforms.Compose(\n",
    "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "       )),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "total_d_loss = 0.0\n",
    "total_g_loss = 0.0\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "#numbers = np.random.randint(0, 49, [10])\n",
    "#numbers = np.sort(numbers)\n",
    "numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "print(\"Numbers\", numbers)\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    \"\"\"does not use n_row\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (10*10, opt.latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(10) for num in numbers])\n",
    "    gen_labels = Variable(FloatTensor(digit_embeddings[labels]))\n",
    "    gen_imgs = generator(z, gen_labels)\n",
    "    save_image(gen_imgs.data, \"images/big10/%d.png\" % batches_done, nrow=10, normalize=True)\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        #print('Max', torch.max(real_imgs))\n",
    "        #print('Min', torch.min(real_imgs))\n",
    "        labels = Variable(FloatTensor(digit_embeddings[labels]))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
    "        gen_labels = Variable(FloatTensor(digit_embeddings[np.random.randint(0, opt.n_classes, batch_size)]))\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "        \n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "        total_g_loss += g_loss.item()\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        if i%opt.n_discriminator==0:\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Loss for real images\n",
    "            validity_real = discriminator(real_imgs, labels)\n",
    "            d_real_loss = adversarial_loss(validity_real, valid)\n",
    "\n",
    "            # Loss for fake images\n",
    "            validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
    "            d_fake_loss = adversarial_loss(validity_fake, fake)\n",
    "\n",
    "            # Total discriminator loss\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "            total_d_loss += d_loss.item()\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "        if i%1000==0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, opt.n_epochs, i, len(dataloader), total_d_loss/(1000/opt.n_discriminator), total_g_loss/1000)\n",
    "            )\n",
    "            total_d_loss = 0.0\n",
    "            total_g_loss = 0.0\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            sample_image(n_row=20, batches_done=batches_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

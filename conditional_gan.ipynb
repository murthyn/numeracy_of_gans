{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/938] [D loss: 0.563877] [G loss: 1.049378]\n",
      "[Epoch 0/200] [Batch 300/938] [D loss: 0.099793] [G loss: 0.505118]\n",
      "[Epoch 0/200] [Batch 600/938] [D loss: 0.406350] [G loss: 1.504236]\n",
      "[Epoch 0/200] [Batch 900/938] [D loss: 0.354785] [G loss: 1.628829]\n",
      "[Epoch 1/200] [Batch 0/938] [D loss: 0.096207] [G loss: 0.617486]\n",
      "[Epoch 1/200] [Batch 300/938] [D loss: 0.081843] [G loss: 0.748146]\n",
      "[Epoch 1/200] [Batch 600/938] [D loss: 0.109547] [G loss: 0.579923]\n",
      "[Epoch 1/200] [Batch 900/938] [D loss: 0.115510] [G loss: 0.650354]\n",
      "[Epoch 2/200] [Batch 0/938] [D loss: 0.101971] [G loss: 0.581561]\n",
      "[Epoch 2/200] [Batch 300/938] [D loss: 0.099785] [G loss: 0.528440]\n",
      "[Epoch 2/200] [Batch 600/938] [D loss: 0.113738] [G loss: 0.386501]\n",
      "[Epoch 2/200] [Batch 900/938] [D loss: 0.097983] [G loss: 0.556454]\n",
      "[Epoch 3/200] [Batch 0/938] [D loss: 0.135327] [G loss: 0.586107]\n",
      "[Epoch 3/200] [Batch 300/938] [D loss: 0.137688] [G loss: 0.447845]\n",
      "[Epoch 3/200] [Batch 600/938] [D loss: 0.121836] [G loss: 0.501477]\n",
      "[Epoch 3/200] [Batch 900/938] [D loss: 0.135105] [G loss: 0.347387]\n",
      "[Epoch 4/200] [Batch 0/938] [D loss: 0.136255] [G loss: 0.536963]\n",
      "[Epoch 4/200] [Batch 300/938] [D loss: 0.174470] [G loss: 0.273365]\n",
      "[Epoch 4/200] [Batch 600/938] [D loss: 0.120475] [G loss: 0.606069]\n",
      "[Epoch 4/200] [Batch 900/938] [D loss: 0.164001] [G loss: 0.342470]\n",
      "[Epoch 5/200] [Batch 0/938] [D loss: 0.113934] [G loss: 0.548050]\n",
      "[Epoch 5/200] [Batch 300/938] [D loss: 0.203305] [G loss: 0.282600]\n",
      "[Epoch 5/200] [Batch 600/938] [D loss: 0.127884] [G loss: 0.601013]\n",
      "[Epoch 5/200] [Batch 900/938] [D loss: 0.137355] [G loss: 0.549618]\n",
      "[Epoch 6/200] [Batch 0/938] [D loss: 0.166606] [G loss: 0.324099]\n",
      "[Epoch 6/200] [Batch 300/938] [D loss: 0.173775] [G loss: 0.382849]\n",
      "[Epoch 6/200] [Batch 600/938] [D loss: 0.183850] [G loss: 0.706772]\n",
      "[Epoch 6/200] [Batch 900/938] [D loss: 0.192579] [G loss: 0.464879]\n",
      "[Epoch 7/200] [Batch 0/938] [D loss: 0.185511] [G loss: 0.399286]\n",
      "[Epoch 7/200] [Batch 300/938] [D loss: 0.172464] [G loss: 0.575565]\n",
      "[Epoch 7/200] [Batch 600/938] [D loss: 0.211640] [G loss: 0.528461]\n",
      "[Epoch 7/200] [Batch 900/938] [D loss: 0.200076] [G loss: 0.367770]\n",
      "[Epoch 8/200] [Batch 0/938] [D loss: 0.197336] [G loss: 0.645004]\n",
      "[Epoch 8/200] [Batch 300/938] [D loss: 0.163897] [G loss: 0.504297]\n",
      "[Epoch 8/200] [Batch 600/938] [D loss: 0.172790] [G loss: 0.503748]\n",
      "[Epoch 8/200] [Batch 900/938] [D loss: 0.180198] [G loss: 0.436914]\n",
      "[Epoch 9/200] [Batch 0/938] [D loss: 0.176495] [G loss: 0.685904]\n",
      "[Epoch 9/200] [Batch 300/938] [D loss: 0.170332] [G loss: 0.362198]\n",
      "[Epoch 9/200] [Batch 600/938] [D loss: 0.189062] [G loss: 0.320736]\n",
      "[Epoch 9/200] [Batch 900/938] [D loss: 0.181320] [G loss: 0.351393]\n",
      "[Epoch 10/200] [Batch 0/938] [D loss: 0.236137] [G loss: 0.156809]\n",
      "[Epoch 10/200] [Batch 300/938] [D loss: 0.176377] [G loss: 0.488760]\n",
      "[Epoch 10/200] [Batch 600/938] [D loss: 0.201137] [G loss: 0.289061]\n",
      "[Epoch 10/200] [Batch 900/938] [D loss: 0.180101] [G loss: 0.394452]\n",
      "[Epoch 11/200] [Batch 0/938] [D loss: 0.192368] [G loss: 0.249269]\n",
      "[Epoch 11/200] [Batch 300/938] [D loss: 0.199247] [G loss: 0.408495]\n",
      "[Epoch 11/200] [Batch 600/938] [D loss: 0.189703] [G loss: 0.504992]\n",
      "[Epoch 11/200] [Batch 900/938] [D loss: 0.206311] [G loss: 0.327727]\n",
      "[Epoch 12/200] [Batch 0/938] [D loss: 0.217414] [G loss: 0.394934]\n",
      "[Epoch 12/200] [Batch 300/938] [D loss: 0.224001] [G loss: 0.502006]\n",
      "[Epoch 12/200] [Batch 600/938] [D loss: 0.231199] [G loss: 0.255072]\n",
      "[Epoch 12/200] [Batch 900/938] [D loss: 0.190944] [G loss: 0.329973]\n",
      "[Epoch 13/200] [Batch 0/938] [D loss: 0.196316] [G loss: 0.359615]\n",
      "[Epoch 13/200] [Batch 300/938] [D loss: 0.286401] [G loss: 0.172700]\n",
      "[Epoch 13/200] [Batch 600/938] [D loss: 0.213929] [G loss: 0.494974]\n",
      "[Epoch 13/200] [Batch 900/938] [D loss: 0.213076] [G loss: 0.314370]\n",
      "[Epoch 14/200] [Batch 0/938] [D loss: 0.186825] [G loss: 0.403112]\n",
      "[Epoch 14/200] [Batch 300/938] [D loss: 0.224548] [G loss: 0.425371]\n",
      "[Epoch 14/200] [Batch 600/938] [D loss: 0.176539] [G loss: 0.373690]\n",
      "[Epoch 14/200] [Batch 900/938] [D loss: 0.187294] [G loss: 0.398358]\n",
      "[Epoch 15/200] [Batch 0/938] [D loss: 0.208379] [G loss: 0.314596]\n",
      "[Epoch 15/200] [Batch 300/938] [D loss: 0.177530] [G loss: 0.428885]\n",
      "[Epoch 15/200] [Batch 600/938] [D loss: 0.238107] [G loss: 0.332525]\n",
      "[Epoch 15/200] [Batch 900/938] [D loss: 0.196379] [G loss: 0.418249]\n",
      "[Epoch 16/200] [Batch 0/938] [D loss: 0.207374] [G loss: 0.471364]\n",
      "[Epoch 16/200] [Batch 300/938] [D loss: 0.202075] [G loss: 0.445673]\n",
      "[Epoch 16/200] [Batch 600/938] [D loss: 0.185443] [G loss: 0.412295]\n",
      "[Epoch 16/200] [Batch 900/938] [D loss: 0.193541] [G loss: 0.377529]\n",
      "[Epoch 17/200] [Batch 0/938] [D loss: 0.216623] [G loss: 0.556315]\n",
      "[Epoch 17/200] [Batch 300/938] [D loss: 0.199115] [G loss: 0.333325]\n",
      "[Epoch 17/200] [Batch 600/938] [D loss: 0.225032] [G loss: 0.413429]\n",
      "[Epoch 17/200] [Batch 900/938] [D loss: 0.215622] [G loss: 0.304272]\n",
      "[Epoch 18/200] [Batch 0/938] [D loss: 0.203104] [G loss: 0.399111]\n",
      "[Epoch 18/200] [Batch 300/938] [D loss: 0.164571] [G loss: 0.429847]\n",
      "[Epoch 18/200] [Batch 600/938] [D loss: 0.211603] [G loss: 0.406308]\n",
      "[Epoch 18/200] [Batch 900/938] [D loss: 0.219477] [G loss: 0.389358]\n",
      "[Epoch 19/200] [Batch 0/938] [D loss: 0.222629] [G loss: 0.398354]\n",
      "[Epoch 19/200] [Batch 300/938] [D loss: 0.219105] [G loss: 0.360046]\n",
      "[Epoch 19/200] [Batch 600/938] [D loss: 0.260251] [G loss: 0.301374]\n",
      "[Epoch 19/200] [Batch 900/938] [D loss: 0.187745] [G loss: 0.373950]\n",
      "[Epoch 20/200] [Batch 0/938] [D loss: 0.218764] [G loss: 0.358921]\n",
      "[Epoch 20/200] [Batch 300/938] [D loss: 0.223850] [G loss: 0.305736]\n",
      "[Epoch 20/200] [Batch 600/938] [D loss: 0.231160] [G loss: 0.328640]\n",
      "[Epoch 20/200] [Batch 900/938] [D loss: 0.174418] [G loss: 0.433411]\n",
      "[Epoch 21/200] [Batch 0/938] [D loss: 0.218558] [G loss: 0.486626]\n",
      "[Epoch 21/200] [Batch 300/938] [D loss: 0.181350] [G loss: 0.360157]\n",
      "[Epoch 21/200] [Batch 600/938] [D loss: 0.277651] [G loss: 0.517550]\n",
      "[Epoch 21/200] [Batch 900/938] [D loss: 0.211387] [G loss: 0.283772]\n",
      "[Epoch 22/200] [Batch 0/938] [D loss: 0.179781] [G loss: 0.457367]\n",
      "[Epoch 22/200] [Batch 300/938] [D loss: 0.246294] [G loss: 0.189528]\n",
      "[Epoch 22/200] [Batch 600/938] [D loss: 0.206827] [G loss: 0.324806]\n",
      "[Epoch 22/200] [Batch 900/938] [D loss: 0.236753] [G loss: 0.567302]\n",
      "[Epoch 23/200] [Batch 0/938] [D loss: 0.206122] [G loss: 0.405169]\n",
      "[Epoch 23/200] [Batch 300/938] [D loss: 0.222996] [G loss: 0.455319]\n",
      "[Epoch 23/200] [Batch 600/938] [D loss: 0.290294] [G loss: 0.139988]\n",
      "[Epoch 23/200] [Batch 900/938] [D loss: 0.212999] [G loss: 0.477221]\n",
      "[Epoch 24/200] [Batch 0/938] [D loss: 0.217206] [G loss: 0.269056]\n",
      "[Epoch 24/200] [Batch 300/938] [D loss: 0.193649] [G loss: 0.395066]\n",
      "[Epoch 24/200] [Batch 600/938] [D loss: 0.214374] [G loss: 0.387734]\n",
      "[Epoch 24/200] [Batch 900/938] [D loss: 0.241857] [G loss: 0.263720]\n",
      "[Epoch 25/200] [Batch 0/938] [D loss: 0.191691] [G loss: 0.351185]\n",
      "[Epoch 25/200] [Batch 300/938] [D loss: 0.193625] [G loss: 0.335001]\n",
      "[Epoch 25/200] [Batch 600/938] [D loss: 0.201289] [G loss: 0.477522]\n",
      "[Epoch 25/200] [Batch 900/938] [D loss: 0.246315] [G loss: 0.226350]\n",
      "[Epoch 26/200] [Batch 0/938] [D loss: 0.203746] [G loss: 0.515600]\n",
      "[Epoch 26/200] [Batch 300/938] [D loss: 0.222228] [G loss: 0.383372]\n",
      "[Epoch 26/200] [Batch 600/938] [D loss: 0.210053] [G loss: 0.466235]\n",
      "[Epoch 26/200] [Batch 900/938] [D loss: 0.234359] [G loss: 0.507445]\n",
      "[Epoch 27/200] [Batch 0/938] [D loss: 0.222519] [G loss: 0.539850]\n",
      "[Epoch 27/200] [Batch 300/938] [D loss: 0.206144] [G loss: 0.379695]\n",
      "[Epoch 27/200] [Batch 600/938] [D loss: 0.233628] [G loss: 0.232038]\n",
      "[Epoch 27/200] [Batch 900/938] [D loss: 0.191627] [G loss: 0.437068]\n",
      "[Epoch 28/200] [Batch 0/938] [D loss: 0.244027] [G loss: 0.196086]\n",
      "[Epoch 28/200] [Batch 300/938] [D loss: 0.220507] [G loss: 0.455814]\n",
      "[Epoch 28/200] [Batch 600/938] [D loss: 0.208357] [G loss: 0.296874]\n",
      "[Epoch 28/200] [Batch 900/938] [D loss: 0.217785] [G loss: 0.314059]\n",
      "[Epoch 29/200] [Batch 0/938] [D loss: 0.188081] [G loss: 0.319053]\n",
      "[Epoch 29/200] [Batch 300/938] [D loss: 0.201442] [G loss: 0.299139]\n",
      "[Epoch 29/200] [Batch 600/938] [D loss: 0.201215] [G loss: 0.389043]\n",
      "[Epoch 29/200] [Batch 900/938] [D loss: 0.213995] [G loss: 0.487964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30/200] [Batch 0/938] [D loss: 0.212085] [G loss: 0.327978]\n",
      "[Epoch 30/200] [Batch 300/938] [D loss: 0.192269] [G loss: 0.510927]\n",
      "[Epoch 30/200] [Batch 600/938] [D loss: 0.205106] [G loss: 0.380007]\n",
      "[Epoch 30/200] [Batch 900/938] [D loss: 0.199391] [G loss: 0.465925]\n",
      "[Epoch 31/200] [Batch 0/938] [D loss: 0.213559] [G loss: 0.269654]\n",
      "[Epoch 31/200] [Batch 300/938] [D loss: 0.207497] [G loss: 0.539645]\n",
      "[Epoch 31/200] [Batch 600/938] [D loss: 0.163928] [G loss: 0.496454]\n",
      "[Epoch 31/200] [Batch 900/938] [D loss: 0.182382] [G loss: 0.417145]\n",
      "[Epoch 32/200] [Batch 0/938] [D loss: 0.172102] [G loss: 0.544492]\n",
      "[Epoch 32/200] [Batch 300/938] [D loss: 0.199699] [G loss: 0.334973]\n",
      "[Epoch 32/200] [Batch 600/938] [D loss: 0.192352] [G loss: 0.423734]\n",
      "[Epoch 32/200] [Batch 900/938] [D loss: 0.194896] [G loss: 0.288208]\n",
      "[Epoch 33/200] [Batch 0/938] [D loss: 0.222022] [G loss: 0.795579]\n",
      "[Epoch 33/200] [Batch 300/938] [D loss: 0.214806] [G loss: 0.582232]\n",
      "[Epoch 33/200] [Batch 600/938] [D loss: 0.187459] [G loss: 0.414624]\n",
      "[Epoch 33/200] [Batch 900/938] [D loss: 0.191016] [G loss: 0.364589]\n",
      "[Epoch 34/200] [Batch 0/938] [D loss: 0.191949] [G loss: 0.231904]\n",
      "[Epoch 34/200] [Batch 300/938] [D loss: 0.206981] [G loss: 0.383041]\n",
      "[Epoch 34/200] [Batch 600/938] [D loss: 0.170208] [G loss: 0.446571]\n",
      "[Epoch 34/200] [Batch 900/938] [D loss: 0.182756] [G loss: 0.409121]\n",
      "[Epoch 35/200] [Batch 0/938] [D loss: 0.170598] [G loss: 0.463094]\n",
      "[Epoch 35/200] [Batch 300/938] [D loss: 0.202450] [G loss: 0.527217]\n",
      "[Epoch 35/200] [Batch 600/938] [D loss: 0.193786] [G loss: 0.519644]\n",
      "[Epoch 35/200] [Batch 900/938] [D loss: 0.196512] [G loss: 0.598048]\n",
      "[Epoch 36/200] [Batch 0/938] [D loss: 0.184848] [G loss: 0.380766]\n",
      "[Epoch 36/200] [Batch 300/938] [D loss: 0.183305] [G loss: 0.374943]\n",
      "[Epoch 36/200] [Batch 600/938] [D loss: 0.168188] [G loss: 0.463455]\n",
      "[Epoch 36/200] [Batch 900/938] [D loss: 0.173580] [G loss: 0.418574]\n",
      "[Epoch 37/200] [Batch 0/938] [D loss: 0.188438] [G loss: 0.253699]\n",
      "[Epoch 37/200] [Batch 300/938] [D loss: 0.160088] [G loss: 0.405932]\n",
      "[Epoch 37/200] [Batch 600/938] [D loss: 0.223140] [G loss: 0.233631]\n",
      "[Epoch 37/200] [Batch 900/938] [D loss: 0.241971] [G loss: 0.201547]\n",
      "[Epoch 38/200] [Batch 0/938] [D loss: 0.155286] [G loss: 0.576107]\n",
      "[Epoch 38/200] [Batch 300/938] [D loss: 0.189205] [G loss: 0.386674]\n",
      "[Epoch 38/200] [Batch 600/938] [D loss: 0.190760] [G loss: 0.748098]\n",
      "[Epoch 38/200] [Batch 900/938] [D loss: 0.193284] [G loss: 0.357307]\n",
      "[Epoch 39/200] [Batch 0/938] [D loss: 0.186019] [G loss: 0.834446]\n",
      "[Epoch 39/200] [Batch 300/938] [D loss: 0.152541] [G loss: 0.448223]\n",
      "[Epoch 39/200] [Batch 600/938] [D loss: 0.167538] [G loss: 0.376798]\n",
      "[Epoch 39/200] [Batch 900/938] [D loss: 0.203423] [G loss: 0.569396]\n",
      "[Epoch 40/200] [Batch 0/938] [D loss: 0.184492] [G loss: 0.508240]\n",
      "[Epoch 40/200] [Batch 300/938] [D loss: 0.163448] [G loss: 0.686669]\n",
      "[Epoch 40/200] [Batch 600/938] [D loss: 0.204326] [G loss: 0.421246]\n",
      "[Epoch 40/200] [Batch 900/938] [D loss: 0.185605] [G loss: 0.276537]\n",
      "[Epoch 41/200] [Batch 0/938] [D loss: 0.172068] [G loss: 0.518009]\n",
      "[Epoch 41/200] [Batch 300/938] [D loss: 0.170840] [G loss: 0.376962]\n",
      "[Epoch 41/200] [Batch 600/938] [D loss: 0.168336] [G loss: 0.331924]\n",
      "[Epoch 41/200] [Batch 900/938] [D loss: 0.175087] [G loss: 0.365022]\n",
      "[Epoch 42/200] [Batch 0/938] [D loss: 0.196946] [G loss: 0.499097]\n",
      "[Epoch 42/200] [Batch 300/938] [D loss: 0.150850] [G loss: 0.551747]\n",
      "[Epoch 42/200] [Batch 600/938] [D loss: 0.201798] [G loss: 0.607773]\n",
      "[Epoch 42/200] [Batch 900/938] [D loss: 0.155104] [G loss: 0.620339]\n",
      "[Epoch 43/200] [Batch 0/938] [D loss: 0.198722] [G loss: 0.371937]\n",
      "[Epoch 43/200] [Batch 300/938] [D loss: 0.160099] [G loss: 0.406688]\n",
      "[Epoch 43/200] [Batch 600/938] [D loss: 0.158499] [G loss: 0.656355]\n",
      "[Epoch 43/200] [Batch 900/938] [D loss: 0.173381] [G loss: 0.476893]\n",
      "[Epoch 44/200] [Batch 0/938] [D loss: 0.203708] [G loss: 0.240806]\n",
      "[Epoch 44/200] [Batch 300/938] [D loss: 0.204896] [G loss: 0.377734]\n",
      "[Epoch 44/200] [Batch 600/938] [D loss: 0.172578] [G loss: 0.503591]\n",
      "[Epoch 44/200] [Batch 900/938] [D loss: 0.188277] [G loss: 0.468911]\n",
      "[Epoch 45/200] [Batch 0/938] [D loss: 0.152591] [G loss: 0.686957]\n",
      "[Epoch 45/200] [Batch 300/938] [D loss: 0.175458] [G loss: 0.472173]\n",
      "[Epoch 45/200] [Batch 600/938] [D loss: 0.174831] [G loss: 0.490483]\n",
      "[Epoch 45/200] [Batch 900/938] [D loss: 0.122102] [G loss: 0.587022]\n",
      "[Epoch 46/200] [Batch 0/938] [D loss: 0.198744] [G loss: 0.748245]\n",
      "[Epoch 46/200] [Batch 300/938] [D loss: 0.181068] [G loss: 0.268756]\n",
      "[Epoch 46/200] [Batch 600/938] [D loss: 0.163016] [G loss: 0.480985]\n",
      "[Epoch 46/200] [Batch 900/938] [D loss: 0.178278] [G loss: 0.310264]\n",
      "[Epoch 47/200] [Batch 0/938] [D loss: 0.163634] [G loss: 0.272536]\n",
      "[Epoch 47/200] [Batch 300/938] [D loss: 0.185241] [G loss: 0.756000]\n",
      "[Epoch 47/200] [Batch 600/938] [D loss: 0.145336] [G loss: 0.419786]\n",
      "[Epoch 47/200] [Batch 900/938] [D loss: 0.175942] [G loss: 0.459815]\n",
      "[Epoch 48/200] [Batch 0/938] [D loss: 0.206345] [G loss: 0.231646]\n",
      "[Epoch 48/200] [Batch 300/938] [D loss: 0.175114] [G loss: 0.553945]\n",
      "[Epoch 48/200] [Batch 600/938] [D loss: 0.200450] [G loss: 0.264648]\n",
      "[Epoch 48/200] [Batch 900/938] [D loss: 0.154955] [G loss: 0.688104]\n",
      "[Epoch 49/200] [Batch 0/938] [D loss: 0.158224] [G loss: 0.514326]\n",
      "[Epoch 49/200] [Batch 300/938] [D loss: 0.211722] [G loss: 0.219200]\n",
      "[Epoch 49/200] [Batch 600/938] [D loss: 0.168690] [G loss: 0.495608]\n",
      "[Epoch 49/200] [Batch 900/938] [D loss: 0.159467] [G loss: 0.390439]\n",
      "[Epoch 50/200] [Batch 0/938] [D loss: 0.184628] [G loss: 0.424423]\n",
      "[Epoch 50/200] [Batch 300/938] [D loss: 0.170672] [G loss: 0.344887]\n",
      "[Epoch 50/200] [Batch 600/938] [D loss: 0.169737] [G loss: 0.750062]\n",
      "[Epoch 50/200] [Batch 900/938] [D loss: 0.149947] [G loss: 0.694711]\n",
      "[Epoch 51/200] [Batch 0/938] [D loss: 0.156237] [G loss: 0.580641]\n",
      "[Epoch 51/200] [Batch 300/938] [D loss: 0.156794] [G loss: 0.694411]\n",
      "[Epoch 51/200] [Batch 600/938] [D loss: 0.182799] [G loss: 0.692828]\n",
      "[Epoch 51/200] [Batch 900/938] [D loss: 0.155980] [G loss: 0.612286]\n",
      "[Epoch 52/200] [Batch 0/938] [D loss: 0.157481] [G loss: 0.362445]\n",
      "[Epoch 52/200] [Batch 300/938] [D loss: 0.165925] [G loss: 0.587523]\n",
      "[Epoch 52/200] [Batch 600/938] [D loss: 0.169064] [G loss: 0.486050]\n",
      "[Epoch 52/200] [Batch 900/938] [D loss: 0.130426] [G loss: 0.585196]\n",
      "[Epoch 53/200] [Batch 0/938] [D loss: 0.147763] [G loss: 0.572084]\n",
      "[Epoch 53/200] [Batch 300/938] [D loss: 0.152904] [G loss: 0.477872]\n",
      "[Epoch 53/200] [Batch 600/938] [D loss: 0.135812] [G loss: 0.481002]\n",
      "[Epoch 53/200] [Batch 900/938] [D loss: 0.137605] [G loss: 0.490801]\n",
      "[Epoch 54/200] [Batch 0/938] [D loss: 0.147157] [G loss: 0.493360]\n",
      "[Epoch 54/200] [Batch 300/938] [D loss: 0.156224] [G loss: 0.432247]\n",
      "[Epoch 54/200] [Batch 600/938] [D loss: 0.118677] [G loss: 0.710323]\n",
      "[Epoch 54/200] [Batch 900/938] [D loss: 0.197477] [G loss: 0.715652]\n",
      "[Epoch 55/200] [Batch 0/938] [D loss: 0.159796] [G loss: 0.458851]\n",
      "[Epoch 55/200] [Batch 300/938] [D loss: 0.150267] [G loss: 0.814167]\n",
      "[Epoch 55/200] [Batch 600/938] [D loss: 0.174206] [G loss: 0.421668]\n",
      "[Epoch 55/200] [Batch 900/938] [D loss: 0.144413] [G loss: 0.557931]\n",
      "[Epoch 56/200] [Batch 0/938] [D loss: 0.194929] [G loss: 0.259998]\n",
      "[Epoch 56/200] [Batch 300/938] [D loss: 0.200318] [G loss: 0.259248]\n",
      "[Epoch 56/200] [Batch 600/938] [D loss: 0.155633] [G loss: 0.377422]\n",
      "[Epoch 56/200] [Batch 900/938] [D loss: 0.139792] [G loss: 0.509779]\n",
      "[Epoch 57/200] [Batch 0/938] [D loss: 0.137303] [G loss: 0.486699]\n",
      "[Epoch 57/200] [Batch 300/938] [D loss: 0.152656] [G loss: 0.438240]\n",
      "[Epoch 57/200] [Batch 600/938] [D loss: 0.136692] [G loss: 0.515196]\n",
      "[Epoch 57/200] [Batch 900/938] [D loss: 0.122383] [G loss: 0.542416]\n",
      "[Epoch 58/200] [Batch 0/938] [D loss: 0.273782] [G loss: 0.238558]\n",
      "[Epoch 58/200] [Batch 300/938] [D loss: 0.134025] [G loss: 0.475831]\n",
      "[Epoch 58/200] [Batch 600/938] [D loss: 0.143866] [G loss: 0.494865]\n",
      "[Epoch 58/200] [Batch 900/938] [D loss: 0.159241] [G loss: 0.445542]\n",
      "[Epoch 59/200] [Batch 0/938] [D loss: 0.176396] [G loss: 0.749483]\n",
      "[Epoch 59/200] [Batch 300/938] [D loss: 0.128205] [G loss: 0.572547]\n",
      "[Epoch 59/200] [Batch 600/938] [D loss: 0.134885] [G loss: 0.484987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 59/200] [Batch 900/938] [D loss: 0.192334] [G loss: 0.307849]\n",
      "[Epoch 60/200] [Batch 0/938] [D loss: 0.131668] [G loss: 0.630270]\n",
      "[Epoch 60/200] [Batch 300/938] [D loss: 0.139205] [G loss: 0.658366]\n",
      "[Epoch 60/200] [Batch 600/938] [D loss: 0.128060] [G loss: 0.555829]\n",
      "[Epoch 60/200] [Batch 900/938] [D loss: 0.148948] [G loss: 0.449543]\n",
      "[Epoch 61/200] [Batch 0/938] [D loss: 0.136589] [G loss: 0.476546]\n",
      "[Epoch 61/200] [Batch 300/938] [D loss: 0.151210] [G loss: 0.693677]\n",
      "[Epoch 61/200] [Batch 600/938] [D loss: 0.144048] [G loss: 0.674751]\n",
      "[Epoch 61/200] [Batch 900/938] [D loss: 0.144444] [G loss: 0.422457]\n",
      "[Epoch 62/200] [Batch 0/938] [D loss: 0.143307] [G loss: 0.614684]\n",
      "[Epoch 62/200] [Batch 300/938] [D loss: 0.168336] [G loss: 0.340708]\n",
      "[Epoch 62/200] [Batch 600/938] [D loss: 0.178037] [G loss: 0.785117]\n",
      "[Epoch 62/200] [Batch 900/938] [D loss: 0.154148] [G loss: 0.415526]\n",
      "[Epoch 63/200] [Batch 0/938] [D loss: 0.165001] [G loss: 0.859073]\n",
      "[Epoch 63/200] [Batch 300/938] [D loss: 0.159416] [G loss: 0.505054]\n",
      "[Epoch 63/200] [Batch 600/938] [D loss: 0.132666] [G loss: 0.589471]\n",
      "[Epoch 63/200] [Batch 900/938] [D loss: 0.133012] [G loss: 0.479647]\n",
      "[Epoch 64/200] [Batch 0/938] [D loss: 0.207490] [G loss: 0.210929]\n",
      "[Epoch 64/200] [Batch 300/938] [D loss: 0.225181] [G loss: 1.124692]\n",
      "[Epoch 64/200] [Batch 600/938] [D loss: 0.156919] [G loss: 0.915512]\n",
      "[Epoch 64/200] [Batch 900/938] [D loss: 0.119966] [G loss: 0.656605]\n",
      "[Epoch 65/200] [Batch 0/938] [D loss: 0.158133] [G loss: 0.297182]\n",
      "[Epoch 65/200] [Batch 300/938] [D loss: 0.119518] [G loss: 0.518933]\n",
      "[Epoch 65/200] [Batch 600/938] [D loss: 0.154198] [G loss: 0.418011]\n",
      "[Epoch 65/200] [Batch 900/938] [D loss: 0.123361] [G loss: 0.563586]\n",
      "[Epoch 66/200] [Batch 0/938] [D loss: 0.128056] [G loss: 0.632534]\n",
      "[Epoch 66/200] [Batch 300/938] [D loss: 0.160609] [G loss: 0.295510]\n",
      "[Epoch 66/200] [Batch 600/938] [D loss: 0.173551] [G loss: 0.421555]\n",
      "[Epoch 66/200] [Batch 900/938] [D loss: 0.121486] [G loss: 0.586661]\n",
      "[Epoch 67/200] [Batch 0/938] [D loss: 0.171976] [G loss: 0.994039]\n",
      "[Epoch 67/200] [Batch 300/938] [D loss: 0.147406] [G loss: 0.468413]\n",
      "[Epoch 67/200] [Batch 600/938] [D loss: 0.116117] [G loss: 0.562453]\n",
      "[Epoch 67/200] [Batch 900/938] [D loss: 0.154230] [G loss: 0.427121]\n",
      "[Epoch 68/200] [Batch 0/938] [D loss: 0.108952] [G loss: 0.590967]\n",
      "[Epoch 68/200] [Batch 300/938] [D loss: 0.127752] [G loss: 0.683961]\n",
      "[Epoch 68/200] [Batch 600/938] [D loss: 0.128515] [G loss: 0.721023]\n",
      "[Epoch 68/200] [Batch 900/938] [D loss: 0.175129] [G loss: 0.283717]\n",
      "[Epoch 69/200] [Batch 0/938] [D loss: 0.147312] [G loss: 0.399570]\n",
      "[Epoch 69/200] [Batch 300/938] [D loss: 0.147385] [G loss: 0.681696]\n",
      "[Epoch 69/200] [Batch 600/938] [D loss: 0.165485] [G loss: 0.750266]\n",
      "[Epoch 69/200] [Batch 900/938] [D loss: 0.124280] [G loss: 0.714346]\n",
      "[Epoch 70/200] [Batch 0/938] [D loss: 0.136455] [G loss: 0.438073]\n",
      "[Epoch 70/200] [Batch 300/938] [D loss: 0.229691] [G loss: 0.174586]\n",
      "[Epoch 70/200] [Batch 600/938] [D loss: 0.142670] [G loss: 0.523456]\n",
      "[Epoch 70/200] [Batch 900/938] [D loss: 0.121072] [G loss: 0.624577]\n",
      "[Epoch 71/200] [Batch 0/938] [D loss: 0.142435] [G loss: 0.505747]\n",
      "[Epoch 71/200] [Batch 300/938] [D loss: 0.147709] [G loss: 0.555515]\n",
      "[Epoch 71/200] [Batch 600/938] [D loss: 0.114837] [G loss: 0.646743]\n",
      "[Epoch 71/200] [Batch 900/938] [D loss: 0.126449] [G loss: 0.612801]\n",
      "[Epoch 72/200] [Batch 0/938] [D loss: 0.134954] [G loss: 0.540849]\n",
      "[Epoch 72/200] [Batch 300/938] [D loss: 0.140799] [G loss: 0.513797]\n",
      "[Epoch 72/200] [Batch 600/938] [D loss: 0.194338] [G loss: 0.241318]\n",
      "[Epoch 72/200] [Batch 900/938] [D loss: 0.236882] [G loss: 0.164011]\n",
      "[Epoch 73/200] [Batch 0/938] [D loss: 0.128863] [G loss: 0.803517]\n",
      "[Epoch 73/200] [Batch 300/938] [D loss: 0.129709] [G loss: 0.688459]\n",
      "[Epoch 73/200] [Batch 600/938] [D loss: 0.140333] [G loss: 0.812290]\n",
      "[Epoch 73/200] [Batch 900/938] [D loss: 0.149638] [G loss: 0.627018]\n",
      "[Epoch 74/200] [Batch 0/938] [D loss: 0.133464] [G loss: 0.769259]\n",
      "[Epoch 74/200] [Batch 300/938] [D loss: 0.168436] [G loss: 0.332805]\n",
      "[Epoch 74/200] [Batch 600/938] [D loss: 0.131947] [G loss: 0.610436]\n",
      "[Epoch 74/200] [Batch 900/938] [D loss: 0.149381] [G loss: 0.724855]\n",
      "[Epoch 75/200] [Batch 0/938] [D loss: 0.124104] [G loss: 0.592531]\n",
      "[Epoch 75/200] [Batch 300/938] [D loss: 0.210406] [G loss: 0.923158]\n",
      "[Epoch 75/200] [Batch 600/938] [D loss: 0.136346] [G loss: 0.451278]\n",
      "[Epoch 75/200] [Batch 900/938] [D loss: 0.152780] [G loss: 0.624718]\n",
      "[Epoch 76/200] [Batch 0/938] [D loss: 0.152708] [G loss: 0.778725]\n",
      "[Epoch 76/200] [Batch 300/938] [D loss: 0.132072] [G loss: 0.601820]\n",
      "[Epoch 76/200] [Batch 600/938] [D loss: 0.133540] [G loss: 0.735873]\n",
      "[Epoch 76/200] [Batch 900/938] [D loss: 0.147904] [G loss: 0.384700]\n",
      "[Epoch 77/200] [Batch 0/938] [D loss: 0.109702] [G loss: 0.778151]\n",
      "[Epoch 77/200] [Batch 300/938] [D loss: 0.133085] [G loss: 0.758406]\n",
      "[Epoch 77/200] [Batch 600/938] [D loss: 0.115258] [G loss: 0.910197]\n",
      "[Epoch 77/200] [Batch 900/938] [D loss: 0.130173] [G loss: 0.489338]\n",
      "[Epoch 78/200] [Batch 0/938] [D loss: 0.131497] [G loss: 0.570333]\n",
      "[Epoch 78/200] [Batch 300/938] [D loss: 0.253217] [G loss: 1.117304]\n",
      "[Epoch 78/200] [Batch 600/938] [D loss: 0.133166] [G loss: 0.510595]\n",
      "[Epoch 78/200] [Batch 900/938] [D loss: 0.196631] [G loss: 0.932134]\n",
      "[Epoch 79/200] [Batch 0/938] [D loss: 0.286533] [G loss: 0.102788]\n",
      "[Epoch 79/200] [Batch 300/938] [D loss: 0.138175] [G loss: 0.510610]\n",
      "[Epoch 79/200] [Batch 600/938] [D loss: 0.138784] [G loss: 0.682838]\n",
      "[Epoch 79/200] [Batch 900/938] [D loss: 0.190770] [G loss: 0.302533]\n",
      "[Epoch 80/200] [Batch 0/938] [D loss: 0.132145] [G loss: 0.797793]\n",
      "[Epoch 80/200] [Batch 300/938] [D loss: 0.108329] [G loss: 0.567824]\n",
      "[Epoch 80/200] [Batch 600/938] [D loss: 0.117389] [G loss: 0.677706]\n",
      "[Epoch 80/200] [Batch 900/938] [D loss: 0.174731] [G loss: 0.906482]\n",
      "[Epoch 81/200] [Batch 0/938] [D loss: 0.124948] [G loss: 0.424441]\n",
      "[Epoch 81/200] [Batch 300/938] [D loss: 0.112393] [G loss: 0.605360]\n",
      "[Epoch 81/200] [Batch 600/938] [D loss: 0.159613] [G loss: 0.318116]\n",
      "[Epoch 81/200] [Batch 900/938] [D loss: 0.182550] [G loss: 0.279929]\n",
      "[Epoch 82/200] [Batch 0/938] [D loss: 0.112839] [G loss: 0.651602]\n",
      "[Epoch 82/200] [Batch 300/938] [D loss: 0.151516] [G loss: 0.833866]\n",
      "[Epoch 82/200] [Batch 600/938] [D loss: 0.185328] [G loss: 0.852853]\n",
      "[Epoch 82/200] [Batch 900/938] [D loss: 0.130171] [G loss: 0.436180]\n",
      "[Epoch 83/200] [Batch 0/938] [D loss: 0.168200] [G loss: 0.269831]\n",
      "[Epoch 83/200] [Batch 300/938] [D loss: 0.190214] [G loss: 0.270887]\n",
      "[Epoch 83/200] [Batch 600/938] [D loss: 0.138346] [G loss: 1.016813]\n",
      "[Epoch 83/200] [Batch 900/938] [D loss: 0.094090] [G loss: 0.595029]\n",
      "[Epoch 84/200] [Batch 0/938] [D loss: 0.109847] [G loss: 0.568937]\n",
      "[Epoch 84/200] [Batch 300/938] [D loss: 0.121959] [G loss: 0.583827]\n",
      "[Epoch 84/200] [Batch 600/938] [D loss: 0.144687] [G loss: 0.413777]\n",
      "[Epoch 84/200] [Batch 900/938] [D loss: 0.303670] [G loss: 0.112027]\n",
      "[Epoch 85/200] [Batch 0/938] [D loss: 0.133831] [G loss: 0.550909]\n",
      "[Epoch 85/200] [Batch 300/938] [D loss: 0.119154] [G loss: 0.765318]\n",
      "[Epoch 85/200] [Batch 600/938] [D loss: 0.165105] [G loss: 0.356161]\n",
      "[Epoch 85/200] [Batch 900/938] [D loss: 0.152046] [G loss: 0.801063]\n",
      "[Epoch 86/200] [Batch 0/938] [D loss: 0.138588] [G loss: 0.431266]\n",
      "[Epoch 86/200] [Batch 300/938] [D loss: 0.106267] [G loss: 0.630892]\n",
      "[Epoch 86/200] [Batch 600/938] [D loss: 0.105719] [G loss: 0.575485]\n",
      "[Epoch 86/200] [Batch 900/938] [D loss: 0.113920] [G loss: 0.555299]\n",
      "[Epoch 87/200] [Batch 0/938] [D loss: 0.122685] [G loss: 0.594598]\n",
      "[Epoch 87/200] [Batch 300/938] [D loss: 0.124763] [G loss: 0.751537]\n",
      "[Epoch 87/200] [Batch 600/938] [D loss: 0.107471] [G loss: 0.927356]\n",
      "[Epoch 87/200] [Batch 900/938] [D loss: 0.270337] [G loss: 0.127957]\n",
      "[Epoch 88/200] [Batch 0/938] [D loss: 0.196062] [G loss: 0.215006]\n",
      "[Epoch 88/200] [Batch 300/938] [D loss: 0.136174] [G loss: 0.620013]\n",
      "[Epoch 88/200] [Batch 600/938] [D loss: 0.144969] [G loss: 0.361305]\n",
      "[Epoch 88/200] [Batch 900/938] [D loss: 0.116510] [G loss: 0.753123]\n",
      "[Epoch 89/200] [Batch 0/938] [D loss: 0.131369] [G loss: 0.688614]\n",
      "[Epoch 89/200] [Batch 300/938] [D loss: 0.172164] [G loss: 0.328708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 89/200] [Batch 600/938] [D loss: 0.140505] [G loss: 0.870988]\n",
      "[Epoch 89/200] [Batch 900/938] [D loss: 0.192502] [G loss: 0.438837]\n",
      "[Epoch 90/200] [Batch 0/938] [D loss: 0.113548] [G loss: 0.610395]\n",
      "[Epoch 90/200] [Batch 300/938] [D loss: 0.125120] [G loss: 0.873740]\n",
      "[Epoch 90/200] [Batch 600/938] [D loss: 0.114452] [G loss: 0.737281]\n",
      "[Epoch 90/200] [Batch 900/938] [D loss: 0.148528] [G loss: 0.348770]\n",
      "[Epoch 91/200] [Batch 0/938] [D loss: 0.124586] [G loss: 0.431220]\n",
      "[Epoch 91/200] [Batch 300/938] [D loss: 0.244614] [G loss: 0.226086]\n",
      "[Epoch 91/200] [Batch 600/938] [D loss: 0.094371] [G loss: 0.762733]\n",
      "[Epoch 91/200] [Batch 900/938] [D loss: 0.142735] [G loss: 0.545407]\n",
      "[Epoch 92/200] [Batch 0/938] [D loss: 0.137409] [G loss: 0.439121]\n",
      "[Epoch 92/200] [Batch 300/938] [D loss: 0.173981] [G loss: 0.290557]\n",
      "[Epoch 92/200] [Batch 600/938] [D loss: 0.123945] [G loss: 0.691155]\n",
      "[Epoch 92/200] [Batch 900/938] [D loss: 0.135517] [G loss: 0.977987]\n",
      "[Epoch 93/200] [Batch 0/938] [D loss: 0.111430] [G loss: 0.593968]\n",
      "[Epoch 93/200] [Batch 300/938] [D loss: 0.145657] [G loss: 0.782270]\n",
      "[Epoch 93/200] [Batch 600/938] [D loss: 0.085395] [G loss: 0.675902]\n",
      "[Epoch 93/200] [Batch 900/938] [D loss: 0.102866] [G loss: 0.604340]\n",
      "[Epoch 94/200] [Batch 0/938] [D loss: 0.184128] [G loss: 0.264939]\n",
      "[Epoch 94/200] [Batch 300/938] [D loss: 0.133968] [G loss: 0.661595]\n",
      "[Epoch 94/200] [Batch 600/938] [D loss: 0.122074] [G loss: 0.885688]\n",
      "[Epoch 94/200] [Batch 900/938] [D loss: 0.129936] [G loss: 0.397098]\n",
      "[Epoch 95/200] [Batch 0/938] [D loss: 0.170493] [G loss: 0.390257]\n",
      "[Epoch 95/200] [Batch 300/938] [D loss: 0.085906] [G loss: 0.627903]\n",
      "[Epoch 95/200] [Batch 600/938] [D loss: 0.154317] [G loss: 0.653500]\n",
      "[Epoch 95/200] [Batch 900/938] [D loss: 0.101092] [G loss: 0.692044]\n",
      "[Epoch 96/200] [Batch 0/938] [D loss: 0.126398] [G loss: 0.624098]\n",
      "[Epoch 96/200] [Batch 300/938] [D loss: 0.099510] [G loss: 0.693809]\n",
      "[Epoch 96/200] [Batch 600/938] [D loss: 0.132147] [G loss: 0.372191]\n",
      "[Epoch 96/200] [Batch 900/938] [D loss: 0.113850] [G loss: 0.697015]\n",
      "[Epoch 97/200] [Batch 0/938] [D loss: 0.119313] [G loss: 0.620295]\n",
      "[Epoch 97/200] [Batch 300/938] [D loss: 0.142148] [G loss: 0.386073]\n",
      "[Epoch 97/200] [Batch 600/938] [D loss: 0.090999] [G loss: 0.716528]\n",
      "[Epoch 97/200] [Batch 900/938] [D loss: 0.104230] [G loss: 0.615073]\n",
      "[Epoch 98/200] [Batch 0/938] [D loss: 0.259874] [G loss: 0.161369]\n",
      "[Epoch 98/200] [Batch 300/938] [D loss: 0.137712] [G loss: 0.403845]\n",
      "[Epoch 98/200] [Batch 600/938] [D loss: 0.090565] [G loss: 0.648460]\n",
      "[Epoch 98/200] [Batch 900/938] [D loss: 0.129983] [G loss: 0.772854]\n",
      "[Epoch 99/200] [Batch 0/938] [D loss: 0.130740] [G loss: 0.404518]\n",
      "[Epoch 99/200] [Batch 300/938] [D loss: 0.121750] [G loss: 0.487223]\n",
      "[Epoch 99/200] [Batch 600/938] [D loss: 0.097720] [G loss: 0.611160]\n",
      "[Epoch 99/200] [Batch 900/938] [D loss: 0.096979] [G loss: 0.793597]\n",
      "[Epoch 100/200] [Batch 0/938] [D loss: 0.117777] [G loss: 0.794665]\n",
      "[Epoch 100/200] [Batch 300/938] [D loss: 0.111404] [G loss: 0.677117]\n",
      "[Epoch 100/200] [Batch 600/938] [D loss: 0.159684] [G loss: 1.030800]\n",
      "[Epoch 100/200] [Batch 900/938] [D loss: 0.109128] [G loss: 0.624879]\n",
      "[Epoch 101/200] [Batch 0/938] [D loss: 0.079629] [G loss: 0.735913]\n",
      "[Epoch 101/200] [Batch 300/938] [D loss: 0.125121] [G loss: 0.446132]\n",
      "[Epoch 101/200] [Batch 600/938] [D loss: 0.138724] [G loss: 0.831818]\n",
      "[Epoch 101/200] [Batch 900/938] [D loss: 0.093733] [G loss: 0.591286]\n",
      "[Epoch 102/200] [Batch 0/938] [D loss: 0.180015] [G loss: 0.314701]\n",
      "[Epoch 102/200] [Batch 300/938] [D loss: 0.122808] [G loss: 0.464798]\n",
      "[Epoch 102/200] [Batch 600/938] [D loss: 0.083955] [G loss: 0.740185]\n",
      "[Epoch 102/200] [Batch 900/938] [D loss: 0.110942] [G loss: 0.807486]\n",
      "[Epoch 103/200] [Batch 0/938] [D loss: 0.251462] [G loss: 0.233444]\n",
      "[Epoch 103/200] [Batch 300/938] [D loss: 0.189427] [G loss: 0.251061]\n",
      "[Epoch 103/200] [Batch 600/938] [D loss: 0.170968] [G loss: 0.303331]\n",
      "[Epoch 103/200] [Batch 900/938] [D loss: 0.141955] [G loss: 0.403032]\n",
      "[Epoch 104/200] [Batch 0/938] [D loss: 0.201775] [G loss: 0.177552]\n",
      "[Epoch 104/200] [Batch 300/938] [D loss: 0.108099] [G loss: 0.546234]\n",
      "[Epoch 104/200] [Batch 600/938] [D loss: 0.144242] [G loss: 0.429149]\n",
      "[Epoch 104/200] [Batch 900/938] [D loss: 0.137601] [G loss: 0.956816]\n",
      "[Epoch 105/200] [Batch 0/938] [D loss: 0.090156] [G loss: 0.619416]\n",
      "[Epoch 105/200] [Batch 300/938] [D loss: 0.085947] [G loss: 0.736006]\n",
      "[Epoch 105/200] [Batch 600/938] [D loss: 0.137376] [G loss: 0.965499]\n",
      "[Epoch 105/200] [Batch 900/938] [D loss: 0.122350] [G loss: 0.680533]\n",
      "[Epoch 106/200] [Batch 0/938] [D loss: 0.209784] [G loss: 0.234920]\n",
      "[Epoch 106/200] [Batch 300/938] [D loss: 0.074544] [G loss: 0.737430]\n",
      "[Epoch 106/200] [Batch 600/938] [D loss: 0.124596] [G loss: 0.925913]\n",
      "[Epoch 106/200] [Batch 900/938] [D loss: 0.121867] [G loss: 0.858264]\n",
      "[Epoch 107/200] [Batch 0/938] [D loss: 0.112672] [G loss: 0.770842]\n",
      "[Epoch 107/200] [Batch 300/938] [D loss: 0.138550] [G loss: 0.434897]\n",
      "[Epoch 107/200] [Batch 600/938] [D loss: 0.113567] [G loss: 0.729218]\n",
      "[Epoch 107/200] [Batch 900/938] [D loss: 0.086147] [G loss: 0.710291]\n",
      "[Epoch 108/200] [Batch 0/938] [D loss: 0.122809] [G loss: 0.412533]\n",
      "[Epoch 108/200] [Batch 300/938] [D loss: 0.138771] [G loss: 0.775033]\n",
      "[Epoch 108/200] [Batch 600/938] [D loss: 0.158524] [G loss: 0.418378]\n",
      "[Epoch 108/200] [Batch 900/938] [D loss: 0.169648] [G loss: 0.885216]\n",
      "[Epoch 109/200] [Batch 0/938] [D loss: 0.090844] [G loss: 0.726609]\n",
      "[Epoch 109/200] [Batch 300/938] [D loss: 0.152284] [G loss: 0.350949]\n",
      "[Epoch 109/200] [Batch 600/938] [D loss: 0.138454] [G loss: 0.773033]\n",
      "[Epoch 109/200] [Batch 900/938] [D loss: 0.105092] [G loss: 0.849781]\n",
      "[Epoch 110/200] [Batch 0/938] [D loss: 0.107173] [G loss: 0.759814]\n",
      "[Epoch 110/200] [Batch 300/938] [D loss: 0.094743] [G loss: 0.598905]\n",
      "[Epoch 110/200] [Batch 600/938] [D loss: 0.093538] [G loss: 0.663673]\n",
      "[Epoch 110/200] [Batch 900/938] [D loss: 0.103282] [G loss: 0.998968]\n",
      "[Epoch 111/200] [Batch 0/938] [D loss: 0.330519] [G loss: 0.103299]\n",
      "[Epoch 111/200] [Batch 300/938] [D loss: 0.091085] [G loss: 0.769212]\n",
      "[Epoch 111/200] [Batch 600/938] [D loss: 0.202640] [G loss: 0.924461]\n",
      "[Epoch 111/200] [Batch 900/938] [D loss: 0.119675] [G loss: 0.637859]\n",
      "[Epoch 112/200] [Batch 0/938] [D loss: 0.132470] [G loss: 0.419052]\n",
      "[Epoch 112/200] [Batch 300/938] [D loss: 0.128567] [G loss: 0.413422]\n",
      "[Epoch 112/200] [Batch 600/938] [D loss: 0.088644] [G loss: 0.688932]\n",
      "[Epoch 112/200] [Batch 900/938] [D loss: 0.117855] [G loss: 0.657598]\n",
      "[Epoch 113/200] [Batch 0/938] [D loss: 0.112747] [G loss: 0.445638]\n",
      "[Epoch 113/200] [Batch 300/938] [D loss: 0.141692] [G loss: 0.363070]\n",
      "[Epoch 113/200] [Batch 600/938] [D loss: 0.115855] [G loss: 0.816822]\n",
      "[Epoch 113/200] [Batch 900/938] [D loss: 0.074259] [G loss: 0.718877]\n",
      "[Epoch 114/200] [Batch 0/938] [D loss: 0.079388] [G loss: 0.595953]\n",
      "[Epoch 114/200] [Batch 300/938] [D loss: 0.120875] [G loss: 0.825949]\n",
      "[Epoch 114/200] [Batch 600/938] [D loss: 0.169337] [G loss: 0.949859]\n",
      "[Epoch 114/200] [Batch 900/938] [D loss: 0.108209] [G loss: 0.570995]\n",
      "[Epoch 115/200] [Batch 0/938] [D loss: 0.073250] [G loss: 0.769735]\n",
      "[Epoch 115/200] [Batch 300/938] [D loss: 0.103295] [G loss: 0.484607]\n",
      "[Epoch 115/200] [Batch 600/938] [D loss: 0.117927] [G loss: 0.465285]\n",
      "[Epoch 115/200] [Batch 900/938] [D loss: 0.078645] [G loss: 0.653009]\n",
      "[Epoch 116/200] [Batch 0/938] [D loss: 0.105563] [G loss: 0.410920]\n",
      "[Epoch 116/200] [Batch 300/938] [D loss: 0.112249] [G loss: 0.740016]\n",
      "[Epoch 116/200] [Batch 600/938] [D loss: 0.198943] [G loss: 0.229342]\n",
      "[Epoch 116/200] [Batch 900/938] [D loss: 0.246658] [G loss: 0.143318]\n",
      "[Epoch 117/200] [Batch 0/938] [D loss: 0.095943] [G loss: 0.658642]\n",
      "[Epoch 117/200] [Batch 300/938] [D loss: 0.085109] [G loss: 0.638941]\n",
      "[Epoch 117/200] [Batch 600/938] [D loss: 0.136955] [G loss: 0.414001]\n",
      "[Epoch 117/200] [Batch 900/938] [D loss: 0.127561] [G loss: 0.384963]\n",
      "[Epoch 118/200] [Batch 0/938] [D loss: 0.150655] [G loss: 1.075987]\n",
      "[Epoch 118/200] [Batch 300/938] [D loss: 0.074813] [G loss: 0.805146]\n",
      "[Epoch 118/200] [Batch 600/938] [D loss: 0.091290] [G loss: 0.842763]\n",
      "[Epoch 118/200] [Batch 900/938] [D loss: 0.126107] [G loss: 0.402206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 119/200] [Batch 0/938] [D loss: 0.076513] [G loss: 0.685787]\n",
      "[Epoch 119/200] [Batch 300/938] [D loss: 0.098736] [G loss: 0.767387]\n",
      "[Epoch 119/200] [Batch 600/938] [D loss: 0.093218] [G loss: 0.622034]\n",
      "[Epoch 119/200] [Batch 900/938] [D loss: 0.140459] [G loss: 0.399323]\n",
      "[Epoch 120/200] [Batch 0/938] [D loss: 0.114701] [G loss: 0.437412]\n",
      "[Epoch 120/200] [Batch 300/938] [D loss: 0.135472] [G loss: 0.409887]\n",
      "[Epoch 120/200] [Batch 600/938] [D loss: 0.288013] [G loss: 1.047294]\n",
      "[Epoch 120/200] [Batch 900/938] [D loss: 0.168577] [G loss: 0.893852]\n",
      "[Epoch 121/200] [Batch 0/938] [D loss: 0.192391] [G loss: 0.251037]\n",
      "[Epoch 121/200] [Batch 300/938] [D loss: 0.076166] [G loss: 0.698154]\n",
      "[Epoch 121/200] [Batch 600/938] [D loss: 0.127043] [G loss: 0.396835]\n",
      "[Epoch 121/200] [Batch 900/938] [D loss: 0.097042] [G loss: 0.566063]\n",
      "[Epoch 122/200] [Batch 0/938] [D loss: 0.296715] [G loss: 0.136358]\n",
      "[Epoch 122/200] [Batch 300/938] [D loss: 0.083972] [G loss: 0.702947]\n",
      "[Epoch 122/200] [Batch 600/938] [D loss: 0.088392] [G loss: 0.669981]\n",
      "[Epoch 122/200] [Batch 900/938] [D loss: 0.194503] [G loss: 0.250786]\n",
      "[Epoch 123/200] [Batch 0/938] [D loss: 0.073241] [G loss: 0.718258]\n",
      "[Epoch 123/200] [Batch 300/938] [D loss: 0.103812] [G loss: 0.735521]\n",
      "[Epoch 123/200] [Batch 600/938] [D loss: 0.107647] [G loss: 0.924302]\n",
      "[Epoch 123/200] [Batch 900/938] [D loss: 0.177820] [G loss: 0.265492]\n",
      "[Epoch 124/200] [Batch 0/938] [D loss: 0.082313] [G loss: 0.678881]\n",
      "[Epoch 124/200] [Batch 300/938] [D loss: 0.060916] [G loss: 0.726346]\n",
      "[Epoch 124/200] [Batch 600/938] [D loss: 0.069204] [G loss: 0.799574]\n",
      "[Epoch 124/200] [Batch 900/938] [D loss: 0.065798] [G loss: 0.817056]\n",
      "[Epoch 125/200] [Batch 0/938] [D loss: 0.084313] [G loss: 0.775706]\n",
      "[Epoch 125/200] [Batch 300/938] [D loss: 0.146182] [G loss: 1.036792]\n",
      "[Epoch 125/200] [Batch 600/938] [D loss: 0.077391] [G loss: 0.703343]\n",
      "[Epoch 125/200] [Batch 900/938] [D loss: 0.267185] [G loss: 0.214457]\n",
      "[Epoch 126/200] [Batch 0/938] [D loss: 0.089744] [G loss: 0.662852]\n",
      "[Epoch 126/200] [Batch 300/938] [D loss: 0.096158] [G loss: 0.630591]\n",
      "[Epoch 126/200] [Batch 600/938] [D loss: 0.147560] [G loss: 1.019557]\n",
      "[Epoch 126/200] [Batch 900/938] [D loss: 0.258808] [G loss: 0.147930]\n",
      "[Epoch 127/200] [Batch 0/938] [D loss: 0.086431] [G loss: 0.932066]\n",
      "[Epoch 127/200] [Batch 300/938] [D loss: 0.088571] [G loss: 0.677448]\n",
      "[Epoch 127/200] [Batch 600/938] [D loss: 0.098808] [G loss: 0.580873]\n",
      "[Epoch 127/200] [Batch 900/938] [D loss: 0.103136] [G loss: 0.935714]\n",
      "[Epoch 128/200] [Batch 0/938] [D loss: 0.153948] [G loss: 0.332307]\n",
      "[Epoch 128/200] [Batch 300/938] [D loss: 0.072860] [G loss: 0.859148]\n",
      "[Epoch 128/200] [Batch 600/938] [D loss: 0.081429] [G loss: 0.808425]\n",
      "[Epoch 128/200] [Batch 900/938] [D loss: 0.086776] [G loss: 0.985821]\n",
      "[Epoch 129/200] [Batch 0/938] [D loss: 0.187898] [G loss: 0.256132]\n",
      "[Epoch 129/200] [Batch 300/938] [D loss: 0.074262] [G loss: 0.930254]\n",
      "[Epoch 129/200] [Batch 600/938] [D loss: 0.082338] [G loss: 0.788255]\n",
      "[Epoch 129/200] [Batch 900/938] [D loss: 0.072642] [G loss: 0.636438]\n",
      "[Epoch 130/200] [Batch 0/938] [D loss: 0.090771] [G loss: 0.725055]\n",
      "[Epoch 130/200] [Batch 300/938] [D loss: 0.069841] [G loss: 0.910484]\n",
      "[Epoch 130/200] [Batch 600/938] [D loss: 0.128202] [G loss: 0.818525]\n",
      "[Epoch 130/200] [Batch 900/938] [D loss: 0.120178] [G loss: 0.743194]\n",
      "[Epoch 131/200] [Batch 0/938] [D loss: 0.101898] [G loss: 0.715266]\n",
      "[Epoch 131/200] [Batch 300/938] [D loss: 0.221194] [G loss: 0.270921]\n",
      "[Epoch 131/200] [Batch 600/938] [D loss: 0.050837] [G loss: 0.859507]\n",
      "[Epoch 131/200] [Batch 900/938] [D loss: 0.070128] [G loss: 0.834632]\n",
      "[Epoch 132/200] [Batch 0/938] [D loss: 0.078846] [G loss: 0.699300]\n",
      "[Epoch 132/200] [Batch 300/938] [D loss: 0.134682] [G loss: 0.718179]\n",
      "[Epoch 132/200] [Batch 600/938] [D loss: 0.165435] [G loss: 0.773057]\n",
      "[Epoch 132/200] [Batch 900/938] [D loss: 0.090041] [G loss: 0.551066]\n",
      "[Epoch 133/200] [Batch 0/938] [D loss: 0.244343] [G loss: 0.167285]\n",
      "[Epoch 133/200] [Batch 300/938] [D loss: 0.094500] [G loss: 1.020086]\n",
      "[Epoch 133/200] [Batch 600/938] [D loss: 0.070484] [G loss: 0.638884]\n",
      "[Epoch 133/200] [Batch 900/938] [D loss: 0.099369] [G loss: 1.004760]\n",
      "[Epoch 134/200] [Batch 0/938] [D loss: 0.070710] [G loss: 0.622836]\n",
      "[Epoch 134/200] [Batch 300/938] [D loss: 0.056139] [G loss: 0.764012]\n",
      "[Epoch 134/200] [Batch 600/938] [D loss: 0.079261] [G loss: 0.624850]\n",
      "[Epoch 134/200] [Batch 900/938] [D loss: 0.114436] [G loss: 0.446386]\n",
      "[Epoch 135/200] [Batch 0/938] [D loss: 0.076935] [G loss: 0.703580]\n",
      "[Epoch 135/200] [Batch 300/938] [D loss: 0.241806] [G loss: 1.000883]\n",
      "[Epoch 135/200] [Batch 600/938] [D loss: 0.110591] [G loss: 0.544168]\n",
      "[Epoch 135/200] [Batch 900/938] [D loss: 0.070594] [G loss: 0.780604]\n",
      "[Epoch 136/200] [Batch 0/938] [D loss: 0.103007] [G loss: 0.540952]\n",
      "[Epoch 136/200] [Batch 300/938] [D loss: 0.095551] [G loss: 0.716121]\n",
      "[Epoch 136/200] [Batch 600/938] [D loss: 0.066885] [G loss: 0.766120]\n",
      "[Epoch 136/200] [Batch 900/938] [D loss: 0.087758] [G loss: 0.586452]\n",
      "[Epoch 137/200] [Batch 0/938] [D loss: 0.078597] [G loss: 0.932415]\n",
      "[Epoch 137/200] [Batch 300/938] [D loss: 0.168149] [G loss: 0.319411]\n",
      "[Epoch 137/200] [Batch 600/938] [D loss: 0.107377] [G loss: 1.078006]\n",
      "[Epoch 137/200] [Batch 900/938] [D loss: 0.088431] [G loss: 0.641939]\n",
      "[Epoch 138/200] [Batch 0/938] [D loss: 0.138071] [G loss: 0.386844]\n",
      "[Epoch 138/200] [Batch 300/938] [D loss: 0.139201] [G loss: 0.349395]\n",
      "[Epoch 138/200] [Batch 600/938] [D loss: 0.108563] [G loss: 0.461279]\n",
      "[Epoch 138/200] [Batch 900/938] [D loss: 0.080639] [G loss: 0.805917]\n",
      "[Epoch 139/200] [Batch 0/938] [D loss: 0.091205] [G loss: 0.623485]\n",
      "[Epoch 139/200] [Batch 300/938] [D loss: 0.114873] [G loss: 0.867132]\n",
      "[Epoch 139/200] [Batch 600/938] [D loss: 0.063000] [G loss: 0.680342]\n",
      "[Epoch 139/200] [Batch 900/938] [D loss: 0.149275] [G loss: 0.330074]\n",
      "[Epoch 140/200] [Batch 0/938] [D loss: 0.085211] [G loss: 0.759582]\n",
      "[Epoch 140/200] [Batch 300/938] [D loss: 0.080091] [G loss: 0.824098]\n",
      "[Epoch 140/200] [Batch 600/938] [D loss: 0.081351] [G loss: 0.508740]\n",
      "[Epoch 140/200] [Batch 900/938] [D loss: 0.116121] [G loss: 0.683576]\n",
      "[Epoch 141/200] [Batch 0/938] [D loss: 0.162500] [G loss: 0.343982]\n",
      "[Epoch 141/200] [Batch 300/938] [D loss: 0.075469] [G loss: 0.917208]\n",
      "[Epoch 141/200] [Batch 600/938] [D loss: 0.260380] [G loss: 0.130523]\n",
      "[Epoch 141/200] [Batch 900/938] [D loss: 0.076873] [G loss: 0.840999]\n",
      "[Epoch 142/200] [Batch 0/938] [D loss: 0.197798] [G loss: 0.227068]\n",
      "[Epoch 142/200] [Batch 300/938] [D loss: 0.082987] [G loss: 0.608112]\n",
      "[Epoch 142/200] [Batch 600/938] [D loss: 0.045374] [G loss: 0.813916]\n",
      "[Epoch 142/200] [Batch 900/938] [D loss: 0.071067] [G loss: 0.678917]\n",
      "[Epoch 143/200] [Batch 0/938] [D loss: 0.234103] [G loss: 0.217546]\n",
      "[Epoch 143/200] [Batch 300/938] [D loss: 0.126005] [G loss: 0.474596]\n",
      "[Epoch 143/200] [Batch 600/938] [D loss: 0.075888] [G loss: 0.716575]\n",
      "[Epoch 143/200] [Batch 900/938] [D loss: 0.074716] [G loss: 0.917082]\n",
      "[Epoch 144/200] [Batch 0/938] [D loss: 0.065650] [G loss: 0.918220]\n",
      "[Epoch 144/200] [Batch 300/938] [D loss: 0.089018] [G loss: 0.856139]\n",
      "[Epoch 144/200] [Batch 600/938] [D loss: 0.236082] [G loss: 0.899385]\n",
      "[Epoch 144/200] [Batch 900/938] [D loss: 0.088085] [G loss: 0.946258]\n",
      "[Epoch 145/200] [Batch 0/938] [D loss: 0.154947] [G loss: 0.366194]\n",
      "[Epoch 145/200] [Batch 300/938] [D loss: 0.153472] [G loss: 0.815549]\n",
      "[Epoch 145/200] [Batch 600/938] [D loss: 0.102176] [G loss: 0.997711]\n",
      "[Epoch 145/200] [Batch 900/938] [D loss: 0.072270] [G loss: 0.828075]\n",
      "[Epoch 146/200] [Batch 0/938] [D loss: 0.153701] [G loss: 0.357932]\n",
      "[Epoch 146/200] [Batch 300/938] [D loss: 0.091129] [G loss: 0.558607]\n",
      "[Epoch 146/200] [Batch 600/938] [D loss: 0.179985] [G loss: 0.280311]\n",
      "[Epoch 146/200] [Batch 900/938] [D loss: 0.090725] [G loss: 0.543699]\n",
      "[Epoch 147/200] [Batch 0/938] [D loss: 0.089639] [G loss: 0.689772]\n",
      "[Epoch 147/200] [Batch 300/938] [D loss: 0.089157] [G loss: 0.620563]\n",
      "[Epoch 147/200] [Batch 600/938] [D loss: 0.098691] [G loss: 0.523719]\n",
      "[Epoch 147/200] [Batch 900/938] [D loss: 0.114882] [G loss: 0.487589]\n",
      "[Epoch 148/200] [Batch 0/938] [D loss: 0.085721] [G loss: 0.603560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 148/200] [Batch 300/938] [D loss: 0.149274] [G loss: 0.416853]\n",
      "[Epoch 148/200] [Batch 600/938] [D loss: 0.296557] [G loss: 1.053385]\n",
      "[Epoch 148/200] [Batch 900/938] [D loss: 0.084732] [G loss: 0.664017]\n",
      "[Epoch 149/200] [Batch 0/938] [D loss: 0.042029] [G loss: 0.905223]\n",
      "[Epoch 149/200] [Batch 300/938] [D loss: 0.076429] [G loss: 0.634982]\n",
      "[Epoch 149/200] [Batch 600/938] [D loss: 0.077851] [G loss: 0.610547]\n",
      "[Epoch 149/200] [Batch 900/938] [D loss: 0.153270] [G loss: 0.361222]\n",
      "[Epoch 150/200] [Batch 0/938] [D loss: 0.084640] [G loss: 0.625465]\n",
      "[Epoch 150/200] [Batch 300/938] [D loss: 0.096516] [G loss: 0.950441]\n",
      "[Epoch 150/200] [Batch 600/938] [D loss: 0.082571] [G loss: 0.843550]\n",
      "[Epoch 150/200] [Batch 900/938] [D loss: 0.074898] [G loss: 0.651730]\n",
      "[Epoch 151/200] [Batch 0/938] [D loss: 0.068270] [G loss: 0.744203]\n",
      "[Epoch 151/200] [Batch 300/938] [D loss: 0.090057] [G loss: 0.770030]\n",
      "[Epoch 151/200] [Batch 600/938] [D loss: 0.155975] [G loss: 0.649675]\n",
      "[Epoch 151/200] [Batch 900/938] [D loss: 0.170044] [G loss: 0.315582]\n",
      "[Epoch 152/200] [Batch 0/938] [D loss: 0.091275] [G loss: 0.577065]\n",
      "[Epoch 152/200] [Batch 300/938] [D loss: 0.084632] [G loss: 0.912928]\n",
      "[Epoch 152/200] [Batch 600/938] [D loss: 0.092309] [G loss: 0.674762]\n",
      "[Epoch 152/200] [Batch 900/938] [D loss: 0.101586] [G loss: 1.015187]\n",
      "[Epoch 153/200] [Batch 0/938] [D loss: 0.114134] [G loss: 0.490837]\n",
      "[Epoch 153/200] [Batch 300/938] [D loss: 0.089809] [G loss: 0.790441]\n",
      "[Epoch 153/200] [Batch 600/938] [D loss: 0.070242] [G loss: 0.877796]\n",
      "[Epoch 153/200] [Batch 900/938] [D loss: 0.064532] [G loss: 0.894631]\n",
      "[Epoch 154/200] [Batch 0/938] [D loss: 0.129083] [G loss: 0.381535]\n",
      "[Epoch 154/200] [Batch 300/938] [D loss: 0.067901] [G loss: 0.983644]\n",
      "[Epoch 154/200] [Batch 600/938] [D loss: 0.088073] [G loss: 0.662790]\n",
      "[Epoch 154/200] [Batch 900/938] [D loss: 0.139398] [G loss: 0.381634]\n",
      "[Epoch 155/200] [Batch 0/938] [D loss: 0.222036] [G loss: 0.273660]\n",
      "[Epoch 155/200] [Batch 300/938] [D loss: 0.075550] [G loss: 0.705274]\n",
      "[Epoch 155/200] [Batch 600/938] [D loss: 0.165311] [G loss: 0.355689]\n",
      "[Epoch 155/200] [Batch 900/938] [D loss: 0.139477] [G loss: 0.401467]\n",
      "[Epoch 156/200] [Batch 0/938] [D loss: 0.084569] [G loss: 0.797142]\n",
      "[Epoch 156/200] [Batch 300/938] [D loss: 0.049819] [G loss: 0.889504]\n",
      "[Epoch 156/200] [Batch 600/938] [D loss: 0.073611] [G loss: 0.916581]\n",
      "[Epoch 156/200] [Batch 900/938] [D loss: 0.086001] [G loss: 0.661956]\n",
      "[Epoch 157/200] [Batch 0/938] [D loss: 0.168241] [G loss: 1.115122]\n",
      "[Epoch 157/200] [Batch 300/938] [D loss: 0.053871] [G loss: 0.706620]\n",
      "[Epoch 157/200] [Batch 600/938] [D loss: 0.086252] [G loss: 0.789004]\n",
      "[Epoch 157/200] [Batch 900/938] [D loss: 0.060465] [G loss: 0.830434]\n",
      "[Epoch 158/200] [Batch 0/938] [D loss: 0.203387] [G loss: 0.895120]\n",
      "[Epoch 158/200] [Batch 300/938] [D loss: 0.078943] [G loss: 0.668262]\n",
      "[Epoch 158/200] [Batch 600/938] [D loss: 0.082542] [G loss: 0.902497]\n",
      "[Epoch 158/200] [Batch 900/938] [D loss: 0.085477] [G loss: 0.795213]\n",
      "[Epoch 159/200] [Batch 0/938] [D loss: 0.071730] [G loss: 0.804196]\n",
      "[Epoch 159/200] [Batch 300/938] [D loss: 0.118243] [G loss: 0.897935]\n",
      "[Epoch 159/200] [Batch 600/938] [D loss: 0.086028] [G loss: 0.876954]\n",
      "[Epoch 159/200] [Batch 900/938] [D loss: 0.068374] [G loss: 0.671622]\n",
      "[Epoch 160/200] [Batch 0/938] [D loss: 0.079057] [G loss: 0.821063]\n",
      "[Epoch 160/200] [Batch 300/938] [D loss: 0.061507] [G loss: 0.940057]\n",
      "[Epoch 160/200] [Batch 600/938] [D loss: 0.103649] [G loss: 0.519198]\n",
      "[Epoch 160/200] [Batch 900/938] [D loss: 0.119777] [G loss: 0.418104]\n",
      "[Epoch 161/200] [Batch 0/938] [D loss: 0.070233] [G loss: 0.639852]\n",
      "[Epoch 161/200] [Batch 300/938] [D loss: 0.318678] [G loss: 0.133303]\n",
      "[Epoch 161/200] [Batch 600/938] [D loss: 0.084290] [G loss: 0.580217]\n",
      "[Epoch 161/200] [Batch 900/938] [D loss: 0.117098] [G loss: 0.505489]\n",
      "[Epoch 162/200] [Batch 0/938] [D loss: 0.060025] [G loss: 0.709589]\n",
      "[Epoch 162/200] [Batch 300/938] [D loss: 0.078356] [G loss: 0.538988]\n",
      "[Epoch 162/200] [Batch 600/938] [D loss: 0.165156] [G loss: 0.960467]\n",
      "[Epoch 162/200] [Batch 900/938] [D loss: 0.094207] [G loss: 0.506416]\n",
      "[Epoch 163/200] [Batch 0/938] [D loss: 0.123811] [G loss: 0.452318]\n",
      "[Epoch 163/200] [Batch 300/938] [D loss: 0.091773] [G loss: 0.968663]\n",
      "[Epoch 163/200] [Batch 600/938] [D loss: 0.164240] [G loss: 0.886318]\n",
      "[Epoch 163/200] [Batch 900/938] [D loss: 0.112475] [G loss: 0.456202]\n",
      "[Epoch 164/200] [Batch 0/938] [D loss: 0.067486] [G loss: 0.796193]\n",
      "[Epoch 164/200] [Batch 300/938] [D loss: 0.137528] [G loss: 0.831877]\n",
      "[Epoch 164/200] [Batch 600/938] [D loss: 0.121344] [G loss: 1.010772]\n",
      "[Epoch 164/200] [Batch 900/938] [D loss: 0.081532] [G loss: 0.619251]\n",
      "[Epoch 165/200] [Batch 0/938] [D loss: 0.076053] [G loss: 0.810125]\n",
      "[Epoch 165/200] [Batch 300/938] [D loss: 0.097812] [G loss: 1.025070]\n",
      "[Epoch 165/200] [Batch 600/938] [D loss: 0.065311] [G loss: 0.777022]\n",
      "[Epoch 165/200] [Batch 900/938] [D loss: 0.062253] [G loss: 0.744498]\n",
      "[Epoch 166/200] [Batch 0/938] [D loss: 0.109284] [G loss: 0.504648]\n",
      "[Epoch 166/200] [Batch 300/938] [D loss: 0.130011] [G loss: 0.990962]\n",
      "[Epoch 166/200] [Batch 600/938] [D loss: 0.067564] [G loss: 0.868700]\n",
      "[Epoch 166/200] [Batch 900/938] [D loss: 0.064814] [G loss: 0.732281]\n",
      "[Epoch 167/200] [Batch 0/938] [D loss: 0.124966] [G loss: 0.536050]\n",
      "[Epoch 167/200] [Batch 300/938] [D loss: 0.135429] [G loss: 0.461491]\n",
      "[Epoch 167/200] [Batch 600/938] [D loss: 0.062417] [G loss: 0.666581]\n",
      "[Epoch 167/200] [Batch 900/938] [D loss: 0.070568] [G loss: 0.571209]\n",
      "[Epoch 168/200] [Batch 0/938] [D loss: 0.121996] [G loss: 0.717539]\n",
      "[Epoch 168/200] [Batch 300/938] [D loss: 0.067075] [G loss: 0.829389]\n",
      "[Epoch 168/200] [Batch 600/938] [D loss: 0.086697] [G loss: 0.670360]\n",
      "[Epoch 168/200] [Batch 900/938] [D loss: 0.245132] [G loss: 0.143865]\n",
      "[Epoch 169/200] [Batch 0/938] [D loss: 0.051540] [G loss: 0.818469]\n",
      "[Epoch 169/200] [Batch 300/938] [D loss: 0.058809] [G loss: 0.841016]\n",
      "[Epoch 169/200] [Batch 600/938] [D loss: 0.091894] [G loss: 0.747102]\n",
      "[Epoch 169/200] [Batch 900/938] [D loss: 0.066932] [G loss: 0.871268]\n",
      "[Epoch 170/200] [Batch 0/938] [D loss: 0.108192] [G loss: 0.563734]\n",
      "[Epoch 170/200] [Batch 300/938] [D loss: 0.195238] [G loss: 0.786236]\n",
      "[Epoch 170/200] [Batch 600/938] [D loss: 0.073554] [G loss: 0.789963]\n",
      "[Epoch 170/200] [Batch 900/938] [D loss: 0.132965] [G loss: 0.473278]\n",
      "[Epoch 171/200] [Batch 0/938] [D loss: 0.127777] [G loss: 0.436057]\n",
      "[Epoch 171/200] [Batch 300/938] [D loss: 0.059316] [G loss: 0.812060]\n",
      "[Epoch 171/200] [Batch 600/938] [D loss: 0.084330] [G loss: 0.597392]\n",
      "[Epoch 171/200] [Batch 900/938] [D loss: 0.098475] [G loss: 0.510855]\n",
      "[Epoch 172/200] [Batch 0/938] [D loss: 0.337442] [G loss: 0.121247]\n",
      "[Epoch 172/200] [Batch 300/938] [D loss: 0.074987] [G loss: 0.656538]\n",
      "[Epoch 172/200] [Batch 600/938] [D loss: 0.061349] [G loss: 0.676279]\n",
      "[Epoch 172/200] [Batch 900/938] [D loss: 0.090625] [G loss: 0.760885]\n",
      "[Epoch 173/200] [Batch 0/938] [D loss: 0.056954] [G loss: 0.721248]\n",
      "[Epoch 173/200] [Batch 300/938] [D loss: 0.045654] [G loss: 0.884774]\n",
      "[Epoch 173/200] [Batch 600/938] [D loss: 0.345299] [G loss: 0.087299]\n",
      "[Epoch 173/200] [Batch 900/938] [D loss: 0.061010] [G loss: 0.784212]\n",
      "[Epoch 174/200] [Batch 0/938] [D loss: 0.089045] [G loss: 0.523663]\n",
      "[Epoch 174/200] [Batch 300/938] [D loss: 0.113653] [G loss: 0.525901]\n",
      "[Epoch 174/200] [Batch 600/938] [D loss: 0.060495] [G loss: 0.954991]\n",
      "[Epoch 174/200] [Batch 900/938] [D loss: 0.100583] [G loss: 1.075764]\n",
      "[Epoch 175/200] [Batch 0/938] [D loss: 0.064091] [G loss: 0.647353]\n",
      "[Epoch 175/200] [Batch 300/938] [D loss: 0.065706] [G loss: 0.696047]\n",
      "[Epoch 175/200] [Batch 600/938] [D loss: 0.068682] [G loss: 0.717685]\n",
      "[Epoch 175/200] [Batch 900/938] [D loss: 0.149344] [G loss: 0.395655]\n",
      "[Epoch 176/200] [Batch 0/938] [D loss: 0.073583] [G loss: 0.651028]\n",
      "[Epoch 176/200] [Batch 300/938] [D loss: 0.067166] [G loss: 0.851168]\n",
      "[Epoch 176/200] [Batch 600/938] [D loss: 0.065612] [G loss: 0.915360]\n",
      "[Epoch 176/200] [Batch 900/938] [D loss: 0.043309] [G loss: 0.799883]\n",
      "[Epoch 177/200] [Batch 0/938] [D loss: 0.068635] [G loss: 1.018625]\n",
      "[Epoch 177/200] [Batch 300/938] [D loss: 0.093115] [G loss: 0.650670]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 177/200] [Batch 600/938] [D loss: 0.094875] [G loss: 0.540559]\n",
      "[Epoch 177/200] [Batch 900/938] [D loss: 0.048919] [G loss: 0.825742]\n",
      "[Epoch 178/200] [Batch 0/938] [D loss: 0.138401] [G loss: 0.389016]\n",
      "[Epoch 178/200] [Batch 300/938] [D loss: 0.067839] [G loss: 0.795975]\n",
      "[Epoch 178/200] [Batch 600/938] [D loss: 0.049651] [G loss: 0.957341]\n",
      "[Epoch 178/200] [Batch 900/938] [D loss: 0.069743] [G loss: 0.646859]\n",
      "[Epoch 179/200] [Batch 0/938] [D loss: 0.165743] [G loss: 0.613261]\n",
      "[Epoch 179/200] [Batch 300/938] [D loss: 0.082292] [G loss: 0.645154]\n",
      "[Epoch 179/200] [Batch 600/938] [D loss: 0.068295] [G loss: 0.780937]\n",
      "[Epoch 179/200] [Batch 900/938] [D loss: 0.057608] [G loss: 0.857681]\n",
      "[Epoch 180/200] [Batch 0/938] [D loss: 0.067419] [G loss: 0.806958]\n",
      "[Epoch 180/200] [Batch 300/938] [D loss: 0.095359] [G loss: 0.535465]\n",
      "[Epoch 180/200] [Batch 600/938] [D loss: 0.057337] [G loss: 0.829018]\n",
      "[Epoch 180/200] [Batch 900/938] [D loss: 0.234255] [G loss: 0.256645]\n",
      "[Epoch 181/200] [Batch 0/938] [D loss: 0.061874] [G loss: 0.811114]\n",
      "[Epoch 181/200] [Batch 300/938] [D loss: 0.070948] [G loss: 0.658927]\n",
      "[Epoch 181/200] [Batch 600/938] [D loss: 0.058149] [G loss: 0.697666]\n",
      "[Epoch 181/200] [Batch 900/938] [D loss: 0.057264] [G loss: 0.669107]\n",
      "[Epoch 182/200] [Batch 0/938] [D loss: 0.068394] [G loss: 0.800261]\n",
      "[Epoch 182/200] [Batch 300/938] [D loss: 0.162970] [G loss: 0.354998]\n",
      "[Epoch 182/200] [Batch 600/938] [D loss: 0.052213] [G loss: 0.867651]\n",
      "[Epoch 182/200] [Batch 900/938] [D loss: 0.111670] [G loss: 0.432117]\n",
      "[Epoch 183/200] [Batch 0/938] [D loss: 0.066386] [G loss: 0.740807]\n",
      "[Epoch 183/200] [Batch 300/938] [D loss: 0.063372] [G loss: 0.653037]\n",
      "[Epoch 183/200] [Batch 600/938] [D loss: 0.074015] [G loss: 0.806204]\n",
      "[Epoch 183/200] [Batch 900/938] [D loss: 0.098634] [G loss: 0.512715]\n",
      "[Epoch 184/200] [Batch 0/938] [D loss: 0.066697] [G loss: 0.816632]\n",
      "[Epoch 184/200] [Batch 300/938] [D loss: 0.066586] [G loss: 0.872454]\n",
      "[Epoch 184/200] [Batch 600/938] [D loss: 0.139847] [G loss: 0.899946]\n",
      "[Epoch 184/200] [Batch 900/938] [D loss: 0.059158] [G loss: 0.769904]\n",
      "[Epoch 185/200] [Batch 0/938] [D loss: 0.079348] [G loss: 0.643482]\n",
      "[Epoch 185/200] [Batch 300/938] [D loss: 0.051278] [G loss: 0.760590]\n",
      "[Epoch 185/200] [Batch 600/938] [D loss: 0.065369] [G loss: 0.800404]\n",
      "[Epoch 185/200] [Batch 900/938] [D loss: 0.049455] [G loss: 0.895176]\n",
      "[Epoch 186/200] [Batch 0/938] [D loss: 0.067975] [G loss: 0.969949]\n",
      "[Epoch 186/200] [Batch 300/938] [D loss: 0.146455] [G loss: 1.007227]\n",
      "[Epoch 186/200] [Batch 600/938] [D loss: 0.063761] [G loss: 0.694206]\n",
      "[Epoch 186/200] [Batch 900/938] [D loss: 0.126575] [G loss: 0.508854]\n",
      "[Epoch 187/200] [Batch 0/938] [D loss: 0.070744] [G loss: 0.816091]\n",
      "[Epoch 187/200] [Batch 300/938] [D loss: 0.084446] [G loss: 0.837366]\n",
      "[Epoch 187/200] [Batch 600/938] [D loss: 0.092189] [G loss: 0.545093]\n",
      "[Epoch 187/200] [Batch 900/938] [D loss: 0.111761] [G loss: 0.535993]\n",
      "[Epoch 188/200] [Batch 0/938] [D loss: 0.067631] [G loss: 0.832131]\n",
      "[Epoch 188/200] [Batch 300/938] [D loss: 0.075486] [G loss: 0.936518]\n",
      "[Epoch 188/200] [Batch 600/938] [D loss: 0.105837] [G loss: 0.927681]\n",
      "[Epoch 188/200] [Batch 900/938] [D loss: 0.058461] [G loss: 0.696551]\n",
      "[Epoch 189/200] [Batch 0/938] [D loss: 0.197381] [G loss: 0.263285]\n",
      "[Epoch 189/200] [Batch 300/938] [D loss: 0.080957] [G loss: 0.892883]\n",
      "[Epoch 189/200] [Batch 600/938] [D loss: 0.080678] [G loss: 0.874892]\n",
      "[Epoch 189/200] [Batch 900/938] [D loss: 0.127152] [G loss: 0.913394]\n",
      "[Epoch 190/200] [Batch 0/938] [D loss: 0.076212] [G loss: 0.586431]\n",
      "[Epoch 190/200] [Batch 300/938] [D loss: 0.240470] [G loss: 0.216163]\n",
      "[Epoch 190/200] [Batch 600/938] [D loss: 0.088775] [G loss: 0.806197]\n",
      "[Epoch 190/200] [Batch 900/938] [D loss: 0.176684] [G loss: 0.280842]\n",
      "[Epoch 191/200] [Batch 0/938] [D loss: 0.061438] [G loss: 0.915083]\n",
      "[Epoch 191/200] [Batch 300/938] [D loss: 0.058178] [G loss: 0.738727]\n",
      "[Epoch 191/200] [Batch 600/938] [D loss: 0.099966] [G loss: 0.839013]\n",
      "[Epoch 191/200] [Batch 900/938] [D loss: 0.088291] [G loss: 0.783372]\n",
      "[Epoch 192/200] [Batch 0/938] [D loss: 0.089282] [G loss: 0.826474]\n",
      "[Epoch 192/200] [Batch 300/938] [D loss: 0.077078] [G loss: 0.630263]\n",
      "[Epoch 192/200] [Batch 600/938] [D loss: 0.066835] [G loss: 1.032874]\n",
      "[Epoch 192/200] [Batch 900/938] [D loss: 0.083133] [G loss: 0.931860]\n",
      "[Epoch 193/200] [Batch 0/938] [D loss: 0.112730] [G loss: 0.768084]\n",
      "[Epoch 193/200] [Batch 300/938] [D loss: 0.052909] [G loss: 0.848522]\n",
      "[Epoch 193/200] [Batch 600/938] [D loss: 0.049682] [G loss: 0.962412]\n",
      "[Epoch 193/200] [Batch 900/938] [D loss: 0.060054] [G loss: 0.766308]\n",
      "[Epoch 194/200] [Batch 0/938] [D loss: 0.135310] [G loss: 0.428469]\n",
      "[Epoch 194/200] [Batch 300/938] [D loss: 0.063025] [G loss: 0.720791]\n",
      "[Epoch 194/200] [Batch 600/938] [D loss: 0.085224] [G loss: 0.668453]\n",
      "[Epoch 194/200] [Batch 900/938] [D loss: 0.079629] [G loss: 0.769877]\n",
      "[Epoch 195/200] [Batch 0/938] [D loss: 0.051669] [G loss: 0.916134]\n",
      "[Epoch 195/200] [Batch 300/938] [D loss: 0.124017] [G loss: 0.541705]\n",
      "[Epoch 195/200] [Batch 600/938] [D loss: 0.132559] [G loss: 0.449985]\n",
      "[Epoch 195/200] [Batch 900/938] [D loss: 0.073588] [G loss: 0.733410]\n",
      "[Epoch 196/200] [Batch 0/938] [D loss: 0.065437] [G loss: 0.697083]\n",
      "[Epoch 196/200] [Batch 300/938] [D loss: 0.086447] [G loss: 0.747756]\n",
      "[Epoch 196/200] [Batch 600/938] [D loss: 0.066554] [G loss: 1.010350]\n",
      "[Epoch 196/200] [Batch 900/938] [D loss: 0.168733] [G loss: 0.331274]\n",
      "[Epoch 197/200] [Batch 0/938] [D loss: 0.049603] [G loss: 0.965458]\n",
      "[Epoch 197/200] [Batch 300/938] [D loss: 0.072303] [G loss: 0.630963]\n",
      "[Epoch 197/200] [Batch 600/938] [D loss: 0.078125] [G loss: 0.571436]\n",
      "[Epoch 197/200] [Batch 900/938] [D loss: 0.105830] [G loss: 0.511607]\n",
      "[Epoch 198/200] [Batch 0/938] [D loss: 0.086384] [G loss: 0.802756]\n",
      "[Epoch 198/200] [Batch 300/938] [D loss: 0.092289] [G loss: 0.565820]\n",
      "[Epoch 198/200] [Batch 600/938] [D loss: 0.061832] [G loss: 0.740530]\n",
      "[Epoch 198/200] [Batch 900/938] [D loss: 0.053570] [G loss: 0.795037]\n",
      "[Epoch 199/200] [Batch 0/938] [D loss: 0.067612] [G loss: 0.723447]\n",
      "[Epoch 199/200] [Batch 300/938] [D loss: 0.041765] [G loss: 0.753502]\n",
      "[Epoch 199/200] [Batch 600/938] [D loss: 0.302484] [G loss: 0.851445]\n",
      "[Epoch 199/200] [Batch 900/938] [D loss: 0.123045] [G loss: 0.462745]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "# parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "# parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "# parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "# parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "# parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "# parser.add_argument(\"--n_classes\", type=int, default=10, help=\"number of classes for dataset\")\n",
    "# parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of each image dimension\")\n",
    "# parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "# parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval between image sampling\")\n",
    "# opt = parser.parse_args()\n",
    "# print(opt)\n",
    "\n",
    "class Arguments():\n",
    "  def __init__(self):\n",
    "    self.n_epochs = 200\n",
    "    self.batch_size = 64\n",
    "    self.lr = 0.0002\n",
    "    self.b1 = 0.5\n",
    "    self.b2 = 0.999\n",
    "    self.n_cpu = 8\n",
    "    self.latent_dim = 100\n",
    "    self.n_classes = 10\n",
    "    self.img_size = 32\n",
    "    self.channels = 1\n",
    "    self.sample_interval = 400\n",
    "\n",
    "opt = Arguments()\n",
    "\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.n_classes, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n",
    "\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Configure data loader\n",
    "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
    "    labels = Variable(LongTensor(labels))\n",
    "    gen_imgs = generator(z, labels)\n",
    "    save_image(gen_imgs.data, \"images/original/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        validity_real = discriminator(real_imgs, labels)\n",
    "        d_real_loss = adversarial_loss(validity_real, valid)\n",
    "\n",
    "        # Loss for fake images\n",
    "        validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        d_fake_loss = adversarial_loss(validity_fake, fake)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        if i%300==0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            sample_image(n_row=10, batches_done=batches_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

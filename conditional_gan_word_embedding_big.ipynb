{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(300000, 28, 56)\n",
      "(300000,)\n"
     ]
    }
   ],
   "source": [
    "x = np.load(\"data/xtrain32.npy\")\n",
    "y = np.load(\"data/ytrain.npy\")\n",
    "print(x[0])\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(x)\n",
    "y = torch.Tensor(y).to(torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5, dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers [1, 22, 23, 26, 29, 33, 35, 37, 42, 48]\n",
      "[Epoch 0/200] [Batch 0/4688] [D loss: 0.457435] [G loss: 0.962952]\n",
      "[Epoch 0/200] [Batch 1000/4688] [D loss: 0.246377] [G loss: 0.249058]\n",
      "[Epoch 0/200] [Batch 2000/4688] [D loss: 0.248893] [G loss: 0.244712]\n",
      "[Epoch 0/200] [Batch 3000/4688] [D loss: 0.244984] [G loss: 0.273848]\n",
      "[Epoch 0/200] [Batch 4000/4688] [D loss: 0.244644] [G loss: 0.264036]\n",
      "[Epoch 1/200] [Batch 0/4688] [D loss: 0.252201] [G loss: 0.253050]\n",
      "[Epoch 1/200] [Batch 1000/4688] [D loss: 0.251820] [G loss: 0.255851]\n",
      "[Epoch 1/200] [Batch 2000/4688] [D loss: 0.248889] [G loss: 0.279270]\n",
      "[Epoch 1/200] [Batch 3000/4688] [D loss: 0.246908] [G loss: 0.215853]\n",
      "[Epoch 1/200] [Batch 4000/4688] [D loss: 0.251058] [G loss: 0.255259]\n",
      "[Epoch 2/200] [Batch 0/4688] [D loss: 0.249478] [G loss: 0.250518]\n",
      "[Epoch 2/200] [Batch 1000/4688] [D loss: 0.266143] [G loss: 0.245598]\n",
      "[Epoch 2/200] [Batch 2000/4688] [D loss: 0.242533] [G loss: 0.258739]\n",
      "[Epoch 2/200] [Batch 3000/4688] [D loss: 0.243123] [G loss: 0.258771]\n",
      "[Epoch 2/200] [Batch 4000/4688] [D loss: 0.247065] [G loss: 0.249883]\n",
      "[Epoch 3/200] [Batch 0/4688] [D loss: 0.249327] [G loss: 0.251885]\n",
      "[Epoch 3/200] [Batch 1000/4688] [D loss: 0.250929] [G loss: 0.253215]\n",
      "[Epoch 3/200] [Batch 2000/4688] [D loss: 0.243720] [G loss: 0.252007]\n",
      "[Epoch 3/200] [Batch 3000/4688] [D loss: 0.252266] [G loss: 0.256721]\n",
      "[Epoch 3/200] [Batch 4000/4688] [D loss: 0.257196] [G loss: 0.236430]\n",
      "[Epoch 4/200] [Batch 0/4688] [D loss: 0.245353] [G loss: 0.259179]\n",
      "[Epoch 4/200] [Batch 1000/4688] [D loss: 0.243930] [G loss: 0.253896]\n",
      "[Epoch 4/200] [Batch 2000/4688] [D loss: 0.250491] [G loss: 0.259577]\n",
      "[Epoch 4/200] [Batch 3000/4688] [D loss: 0.252175] [G loss: 0.247073]\n",
      "[Epoch 4/200] [Batch 4000/4688] [D loss: 0.248371] [G loss: 0.256045]\n",
      "[Epoch 5/200] [Batch 0/4688] [D loss: 0.257598] [G loss: 0.229362]\n",
      "[Epoch 5/200] [Batch 1000/4688] [D loss: 0.251309] [G loss: 0.259374]\n",
      "[Epoch 5/200] [Batch 2000/4688] [D loss: 0.249568] [G loss: 0.250327]\n",
      "[Epoch 5/200] [Batch 3000/4688] [D loss: 0.249735] [G loss: 0.256740]\n",
      "[Epoch 5/200] [Batch 4000/4688] [D loss: 0.245766] [G loss: 0.269677]\n",
      "[Epoch 6/200] [Batch 0/4688] [D loss: 0.258357] [G loss: 0.269025]\n",
      "[Epoch 6/200] [Batch 1000/4688] [D loss: 0.251693] [G loss: 0.256647]\n",
      "[Epoch 6/200] [Batch 2000/4688] [D loss: 0.250213] [G loss: 0.244900]\n",
      "[Epoch 6/200] [Batch 3000/4688] [D loss: 0.243937] [G loss: 0.263390]\n",
      "[Epoch 6/200] [Batch 4000/4688] [D loss: 0.252703] [G loss: 0.255147]\n",
      "[Epoch 7/200] [Batch 0/4688] [D loss: 0.247315] [G loss: 0.251036]\n",
      "[Epoch 7/200] [Batch 1000/4688] [D loss: 0.244695] [G loss: 0.252111]\n",
      "[Epoch 7/200] [Batch 2000/4688] [D loss: 0.245712] [G loss: 0.250828]\n",
      "[Epoch 7/200] [Batch 3000/4688] [D loss: 0.245760] [G loss: 0.269121]\n",
      "[Epoch 7/200] [Batch 4000/4688] [D loss: 0.247214] [G loss: 0.248415]\n",
      "[Epoch 8/200] [Batch 0/4688] [D loss: 0.262898] [G loss: 0.293782]\n",
      "[Epoch 8/200] [Batch 1000/4688] [D loss: 0.255044] [G loss: 0.257111]\n",
      "[Epoch 8/200] [Batch 2000/4688] [D loss: 0.254182] [G loss: 0.272139]\n",
      "[Epoch 8/200] [Batch 3000/4688] [D loss: 0.228288] [G loss: 0.284537]\n",
      "[Epoch 8/200] [Batch 4000/4688] [D loss: 0.240402] [G loss: 0.285296]\n",
      "[Epoch 9/200] [Batch 0/4688] [D loss: 0.249851] [G loss: 0.243422]\n",
      "[Epoch 9/200] [Batch 1000/4688] [D loss: 0.253125] [G loss: 0.265597]\n",
      "[Epoch 9/200] [Batch 2000/4688] [D loss: 0.250364] [G loss: 0.255508]\n",
      "[Epoch 9/200] [Batch 3000/4688] [D loss: 0.258886] [G loss: 0.246484]\n",
      "[Epoch 9/200] [Batch 4000/4688] [D loss: 0.245487] [G loss: 0.257681]\n",
      "[Epoch 10/200] [Batch 0/4688] [D loss: 0.236501] [G loss: 0.252721]\n",
      "[Epoch 10/200] [Batch 1000/4688] [D loss: 0.238293] [G loss: 0.279288]\n",
      "[Epoch 10/200] [Batch 2000/4688] [D loss: 0.252321] [G loss: 0.190682]\n",
      "[Epoch 10/200] [Batch 3000/4688] [D loss: 0.263338] [G loss: 0.220604]\n",
      "[Epoch 10/200] [Batch 4000/4688] [D loss: 0.250697] [G loss: 0.241383]\n",
      "[Epoch 11/200] [Batch 0/4688] [D loss: 0.248510] [G loss: 0.254263]\n",
      "[Epoch 11/200] [Batch 1000/4688] [D loss: 0.260194] [G loss: 0.224952]\n",
      "[Epoch 11/200] [Batch 2000/4688] [D loss: 0.245996] [G loss: 0.241273]\n",
      "[Epoch 11/200] [Batch 3000/4688] [D loss: 0.243256] [G loss: 0.278077]\n",
      "[Epoch 11/200] [Batch 4000/4688] [D loss: 0.248021] [G loss: 0.247368]\n",
      "[Epoch 12/200] [Batch 0/4688] [D loss: 0.241316] [G loss: 0.279499]\n",
      "[Epoch 12/200] [Batch 1000/4688] [D loss: 0.258845] [G loss: 0.239832]\n",
      "[Epoch 12/200] [Batch 2000/4688] [D loss: 0.252892] [G loss: 0.269399]\n",
      "[Epoch 12/200] [Batch 3000/4688] [D loss: 0.230813] [G loss: 0.257312]\n",
      "[Epoch 12/200] [Batch 4000/4688] [D loss: 0.261966] [G loss: 0.241082]\n",
      "[Epoch 13/200] [Batch 0/4688] [D loss: 0.284906] [G loss: 0.318403]\n",
      "[Epoch 13/200] [Batch 1000/4688] [D loss: 0.249877] [G loss: 0.246653]\n",
      "[Epoch 13/200] [Batch 2000/4688] [D loss: 0.246371] [G loss: 0.250358]\n",
      "[Epoch 13/200] [Batch 3000/4688] [D loss: 0.242229] [G loss: 0.272602]\n",
      "[Epoch 13/200] [Batch 4000/4688] [D loss: 0.246835] [G loss: 0.244670]\n",
      "[Epoch 14/200] [Batch 0/4688] [D loss: 0.246926] [G loss: 0.255025]\n",
      "[Epoch 14/200] [Batch 1000/4688] [D loss: 0.247813] [G loss: 0.257552]\n",
      "[Epoch 14/200] [Batch 2000/4688] [D loss: 0.251613] [G loss: 0.247526]\n",
      "[Epoch 14/200] [Batch 3000/4688] [D loss: 0.257537] [G loss: 0.207246]\n",
      "[Epoch 14/200] [Batch 4000/4688] [D loss: 0.256855] [G loss: 0.246398]\n",
      "[Epoch 15/200] [Batch 0/4688] [D loss: 0.249273] [G loss: 0.257135]\n",
      "[Epoch 15/200] [Batch 1000/4688] [D loss: 0.249217] [G loss: 0.261801]\n",
      "[Epoch 15/200] [Batch 2000/4688] [D loss: 0.237919] [G loss: 0.283526]\n",
      "[Epoch 15/200] [Batch 3000/4688] [D loss: 0.262145] [G loss: 0.242371]\n",
      "[Epoch 15/200] [Batch 4000/4688] [D loss: 0.252462] [G loss: 0.256719]\n",
      "[Epoch 16/200] [Batch 0/4688] [D loss: 0.249989] [G loss: 0.256484]\n",
      "[Epoch 16/200] [Batch 1000/4688] [D loss: 0.248346] [G loss: 0.248822]\n",
      "[Epoch 16/200] [Batch 2000/4688] [D loss: 0.254193] [G loss: 0.264674]\n",
      "[Epoch 16/200] [Batch 3000/4688] [D loss: 0.254381] [G loss: 0.213131]\n",
      "[Epoch 16/200] [Batch 4000/4688] [D loss: 0.243105] [G loss: 0.283629]\n",
      "[Epoch 17/200] [Batch 0/4688] [D loss: 0.256924] [G loss: 0.234698]\n",
      "[Epoch 17/200] [Batch 1000/4688] [D loss: 0.257992] [G loss: 0.231382]\n",
      "[Epoch 17/200] [Batch 2000/4688] [D loss: 0.249503] [G loss: 0.249398]\n",
      "[Epoch 17/200] [Batch 3000/4688] [D loss: 0.224571] [G loss: 0.282547]\n",
      "[Epoch 17/200] [Batch 4000/4688] [D loss: 0.244042] [G loss: 0.246082]\n",
      "[Epoch 18/200] [Batch 0/4688] [D loss: 0.247945] [G loss: 0.242542]\n",
      "[Epoch 18/200] [Batch 1000/4688] [D loss: 0.253693] [G loss: 0.265062]\n",
      "[Epoch 18/200] [Batch 2000/4688] [D loss: 0.486055] [G loss: 0.922654]\n",
      "[Epoch 18/200] [Batch 3000/4688] [D loss: 0.246681] [G loss: 0.292174]\n",
      "[Epoch 18/200] [Batch 4000/4688] [D loss: 0.248284] [G loss: 0.253272]\n",
      "[Epoch 19/200] [Batch 0/4688] [D loss: 0.244202] [G loss: 0.263771]\n",
      "[Epoch 19/200] [Batch 1000/4688] [D loss: 0.263890] [G loss: 0.212762]\n",
      "[Epoch 19/200] [Batch 2000/4688] [D loss: 0.243558] [G loss: 0.295215]\n",
      "[Epoch 19/200] [Batch 3000/4688] [D loss: 0.247171] [G loss: 0.263043]\n",
      "[Epoch 19/200] [Batch 4000/4688] [D loss: 0.261964] [G loss: 0.164208]\n",
      "[Epoch 20/200] [Batch 0/4688] [D loss: 0.233133] [G loss: 0.287226]\n",
      "[Epoch 20/200] [Batch 1000/4688] [D loss: 0.245982] [G loss: 0.240484]\n",
      "[Epoch 20/200] [Batch 2000/4688] [D loss: 0.247215] [G loss: 0.282903]\n",
      "[Epoch 20/200] [Batch 3000/4688] [D loss: 0.250899] [G loss: 0.259765]\n",
      "[Epoch 20/200] [Batch 4000/4688] [D loss: 0.243486] [G loss: 0.214537]\n",
      "[Epoch 21/200] [Batch 0/4688] [D loss: 0.253480] [G loss: 0.251532]\n",
      "[Epoch 21/200] [Batch 1000/4688] [D loss: 0.240984] [G loss: 0.249059]\n",
      "[Epoch 21/200] [Batch 2000/4688] [D loss: 0.218010] [G loss: 0.357641]\n",
      "[Epoch 21/200] [Batch 3000/4688] [D loss: 0.241887] [G loss: 0.264921]\n",
      "[Epoch 21/200] [Batch 4000/4688] [D loss: 0.245767] [G loss: 0.299753]\n",
      "[Epoch 22/200] [Batch 0/4688] [D loss: 0.226877] [G loss: 0.429576]\n",
      "[Epoch 22/200] [Batch 1000/4688] [D loss: 0.265003] [G loss: 0.184009]\n",
      "[Epoch 22/200] [Batch 2000/4688] [D loss: 0.224390] [G loss: 0.270857]\n",
      "[Epoch 22/200] [Batch 3000/4688] [D loss: 0.255627] [G loss: 0.209348]\n",
      "[Epoch 22/200] [Batch 4000/4688] [D loss: 0.252823] [G loss: 0.138180]\n",
      "[Epoch 23/200] [Batch 0/4688] [D loss: 0.221507] [G loss: 0.299234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/200] [Batch 1000/4688] [D loss: 0.215652] [G loss: 0.391505]\n",
      "[Epoch 23/200] [Batch 2000/4688] [D loss: 0.222133] [G loss: 0.301776]\n",
      "[Epoch 23/200] [Batch 3000/4688] [D loss: 0.214047] [G loss: 0.259669]\n",
      "[Epoch 23/200] [Batch 4000/4688] [D loss: 0.240520] [G loss: 0.148434]\n",
      "[Epoch 24/200] [Batch 0/4688] [D loss: 0.236007] [G loss: 0.296397]\n",
      "[Epoch 24/200] [Batch 1000/4688] [D loss: 0.211243] [G loss: 0.350189]\n",
      "[Epoch 24/200] [Batch 2000/4688] [D loss: 0.219551] [G loss: 0.258571]\n",
      "[Epoch 24/200] [Batch 3000/4688] [D loss: 0.206185] [G loss: 0.375058]\n",
      "[Epoch 24/200] [Batch 4000/4688] [D loss: 0.293883] [G loss: 0.503525]\n",
      "[Epoch 25/200] [Batch 0/4688] [D loss: 0.216717] [G loss: 0.335201]\n",
      "[Epoch 25/200] [Batch 1000/4688] [D loss: 0.201251] [G loss: 0.375304]\n",
      "[Epoch 25/200] [Batch 2000/4688] [D loss: 0.195525] [G loss: 0.372774]\n",
      "[Epoch 25/200] [Batch 3000/4688] [D loss: 0.204135] [G loss: 0.566992]\n",
      "[Epoch 25/200] [Batch 4000/4688] [D loss: 0.239711] [G loss: 0.261751]\n",
      "[Epoch 26/200] [Batch 0/4688] [D loss: 0.214232] [G loss: 0.328258]\n",
      "[Epoch 26/200] [Batch 1000/4688] [D loss: 0.211337] [G loss: 0.191681]\n",
      "[Epoch 26/200] [Batch 2000/4688] [D loss: 0.209098] [G loss: 0.578328]\n",
      "[Epoch 26/200] [Batch 3000/4688] [D loss: 0.240046] [G loss: 0.254310]\n",
      "[Epoch 26/200] [Batch 4000/4688] [D loss: 0.195478] [G loss: 0.314685]\n",
      "[Epoch 27/200] [Batch 0/4688] [D loss: 0.220565] [G loss: 0.310300]\n",
      "[Epoch 27/200] [Batch 1000/4688] [D loss: 0.306724] [G loss: 0.076529]\n",
      "[Epoch 27/200] [Batch 2000/4688] [D loss: 0.222166] [G loss: 0.306741]\n",
      "[Epoch 27/200] [Batch 3000/4688] [D loss: 0.225858] [G loss: 0.257878]\n",
      "[Epoch 27/200] [Batch 4000/4688] [D loss: 0.195290] [G loss: 0.453242]\n",
      "[Epoch 28/200] [Batch 0/4688] [D loss: 0.178669] [G loss: 0.485489]\n",
      "[Epoch 28/200] [Batch 1000/4688] [D loss: 0.255273] [G loss: 0.147191]\n",
      "[Epoch 28/200] [Batch 2000/4688] [D loss: 0.228060] [G loss: 0.196580]\n",
      "[Epoch 28/200] [Batch 3000/4688] [D loss: 0.329849] [G loss: 0.068358]\n",
      "[Epoch 28/200] [Batch 4000/4688] [D loss: 0.251052] [G loss: 0.153101]\n",
      "[Epoch 29/200] [Batch 0/4688] [D loss: 0.213429] [G loss: 0.516901]\n",
      "[Epoch 29/200] [Batch 1000/4688] [D loss: 0.201927] [G loss: 0.528770]\n",
      "[Epoch 29/200] [Batch 2000/4688] [D loss: 0.221674] [G loss: 0.322263]\n",
      "[Epoch 29/200] [Batch 3000/4688] [D loss: 0.212351] [G loss: 0.322069]\n",
      "[Epoch 29/200] [Batch 4000/4688] [D loss: 0.229038] [G loss: 0.261877]\n",
      "[Epoch 30/200] [Batch 0/4688] [D loss: 0.183309] [G loss: 0.398943]\n",
      "[Epoch 30/200] [Batch 1000/4688] [D loss: 0.232784] [G loss: 0.211012]\n",
      "[Epoch 30/200] [Batch 2000/4688] [D loss: 0.236528] [G loss: 0.723314]\n",
      "[Epoch 30/200] [Batch 3000/4688] [D loss: 0.183205] [G loss: 0.422847]\n",
      "[Epoch 30/200] [Batch 4000/4688] [D loss: 0.220820] [G loss: 0.360178]\n",
      "[Epoch 31/200] [Batch 0/4688] [D loss: 0.214443] [G loss: 0.413936]\n",
      "[Epoch 31/200] [Batch 1000/4688] [D loss: 0.216044] [G loss: 0.248673]\n",
      "[Epoch 31/200] [Batch 2000/4688] [D loss: 0.163931] [G loss: 0.596522]\n",
      "[Epoch 31/200] [Batch 3000/4688] [D loss: 0.202372] [G loss: 0.393619]\n",
      "[Epoch 31/200] [Batch 4000/4688] [D loss: 0.181372] [G loss: 0.505107]\n",
      "[Epoch 32/200] [Batch 0/4688] [D loss: 0.226754] [G loss: 0.331684]\n",
      "[Epoch 32/200] [Batch 1000/4688] [D loss: 0.181687] [G loss: 0.431118]\n",
      "[Epoch 32/200] [Batch 2000/4688] [D loss: 0.193049] [G loss: 0.327717]\n",
      "[Epoch 32/200] [Batch 3000/4688] [D loss: 0.199543] [G loss: 0.249709]\n",
      "[Epoch 32/200] [Batch 4000/4688] [D loss: 0.169170] [G loss: 0.484560]\n",
      "[Epoch 33/200] [Batch 0/4688] [D loss: 0.197437] [G loss: 0.370882]\n",
      "[Epoch 33/200] [Batch 1000/4688] [D loss: 0.198911] [G loss: 0.366261]\n",
      "[Epoch 33/200] [Batch 2000/4688] [D loss: 0.184750] [G loss: 0.364749]\n",
      "[Epoch 33/200] [Batch 3000/4688] [D loss: 0.186296] [G loss: 0.654200]\n",
      "[Epoch 33/200] [Batch 4000/4688] [D loss: 0.240071] [G loss: 0.275785]\n",
      "[Epoch 34/200] [Batch 0/4688] [D loss: 0.213495] [G loss: 0.410139]\n",
      "[Epoch 34/200] [Batch 1000/4688] [D loss: 0.155398] [G loss: 0.468192]\n",
      "[Epoch 34/200] [Batch 2000/4688] [D loss: 0.166678] [G loss: 0.451949]\n",
      "[Epoch 34/200] [Batch 3000/4688] [D loss: 0.227496] [G loss: 0.402899]\n",
      "[Epoch 34/200] [Batch 4000/4688] [D loss: 0.187005] [G loss: 0.450606]\n",
      "[Epoch 35/200] [Batch 0/4688] [D loss: 0.266121] [G loss: 0.337224]\n",
      "[Epoch 35/200] [Batch 1000/4688] [D loss: 0.189521] [G loss: 0.322672]\n",
      "[Epoch 35/200] [Batch 2000/4688] [D loss: 0.281406] [G loss: 0.107173]\n",
      "[Epoch 35/200] [Batch 3000/4688] [D loss: 0.201424] [G loss: 0.284782]\n",
      "[Epoch 35/200] [Batch 4000/4688] [D loss: 0.174235] [G loss: 0.372598]\n",
      "[Epoch 36/200] [Batch 0/4688] [D loss: 0.196868] [G loss: 0.518074]\n",
      "[Epoch 36/200] [Batch 1000/4688] [D loss: 0.210613] [G loss: 0.368906]\n",
      "[Epoch 36/200] [Batch 2000/4688] [D loss: 0.199945] [G loss: 0.497733]\n",
      "[Epoch 36/200] [Batch 3000/4688] [D loss: 0.200744] [G loss: 0.324488]\n",
      "[Epoch 36/200] [Batch 4000/4688] [D loss: 0.166125] [G loss: 0.532504]\n",
      "[Epoch 37/200] [Batch 0/4688] [D loss: 0.213200] [G loss: 0.328956]\n",
      "[Epoch 37/200] [Batch 1000/4688] [D loss: 0.206932] [G loss: 0.261290]\n",
      "[Epoch 37/200] [Batch 2000/4688] [D loss: 0.204461] [G loss: 0.339701]\n",
      "[Epoch 37/200] [Batch 3000/4688] [D loss: 0.230277] [G loss: 0.399138]\n",
      "[Epoch 37/200] [Batch 4000/4688] [D loss: 0.211539] [G loss: 0.453363]\n",
      "[Epoch 38/200] [Batch 0/4688] [D loss: 0.198691] [G loss: 0.311566]\n",
      "[Epoch 38/200] [Batch 1000/4688] [D loss: 0.164013] [G loss: 0.417373]\n",
      "[Epoch 38/200] [Batch 2000/4688] [D loss: 0.160848] [G loss: 0.356759]\n",
      "[Epoch 38/200] [Batch 3000/4688] [D loss: 0.206918] [G loss: 0.554924]\n",
      "[Epoch 38/200] [Batch 4000/4688] [D loss: 0.158387] [G loss: 0.628464]\n",
      "[Epoch 39/200] [Batch 0/4688] [D loss: 0.186466] [G loss: 0.369124]\n",
      "[Epoch 39/200] [Batch 1000/4688] [D loss: 0.230685] [G loss: 0.264069]\n",
      "[Epoch 39/200] [Batch 2000/4688] [D loss: 0.212966] [G loss: 0.289346]\n",
      "[Epoch 39/200] [Batch 3000/4688] [D loss: 0.170675] [G loss: 0.664837]\n",
      "[Epoch 39/200] [Batch 4000/4688] [D loss: 0.168953] [G loss: 0.655313]\n",
      "[Epoch 40/200] [Batch 0/4688] [D loss: 0.176454] [G loss: 0.447211]\n",
      "[Epoch 40/200] [Batch 1000/4688] [D loss: 0.180372] [G loss: 0.509083]\n",
      "[Epoch 40/200] [Batch 2000/4688] [D loss: 0.130591] [G loss: 0.703633]\n",
      "[Epoch 40/200] [Batch 3000/4688] [D loss: 0.183024] [G loss: 0.655249]\n",
      "[Epoch 40/200] [Batch 4000/4688] [D loss: 0.191898] [G loss: 0.440184]\n",
      "[Epoch 41/200] [Batch 0/4688] [D loss: 0.162376] [G loss: 0.492655]\n",
      "[Epoch 41/200] [Batch 1000/4688] [D loss: 0.230089] [G loss: 0.323338]\n",
      "[Epoch 41/200] [Batch 2000/4688] [D loss: 0.167565] [G loss: 0.522310]\n",
      "[Epoch 41/200] [Batch 3000/4688] [D loss: 0.160969] [G loss: 0.545949]\n",
      "[Epoch 41/200] [Batch 4000/4688] [D loss: 0.198134] [G loss: 0.299310]\n",
      "[Epoch 42/200] [Batch 0/4688] [D loss: 0.186711] [G loss: 0.383254]\n",
      "[Epoch 42/200] [Batch 1000/4688] [D loss: 0.186630] [G loss: 0.317987]\n",
      "[Epoch 42/200] [Batch 2000/4688] [D loss: 0.148742] [G loss: 0.662459]\n",
      "[Epoch 42/200] [Batch 3000/4688] [D loss: 0.191576] [G loss: 0.395348]\n",
      "[Epoch 42/200] [Batch 4000/4688] [D loss: 0.210799] [G loss: 0.343402]\n",
      "[Epoch 43/200] [Batch 0/4688] [D loss: 0.183755] [G loss: 0.335235]\n",
      "[Epoch 43/200] [Batch 1000/4688] [D loss: 0.184582] [G loss: 0.410066]\n",
      "[Epoch 43/200] [Batch 2000/4688] [D loss: 0.171458] [G loss: 0.575671]\n",
      "[Epoch 43/200] [Batch 3000/4688] [D loss: 0.152091] [G loss: 0.744629]\n",
      "[Epoch 43/200] [Batch 4000/4688] [D loss: 0.207981] [G loss: 0.323333]\n",
      "[Epoch 44/200] [Batch 0/4688] [D loss: 0.194875] [G loss: 0.270899]\n",
      "[Epoch 44/200] [Batch 1000/4688] [D loss: 0.156606] [G loss: 0.736351]\n",
      "[Epoch 44/200] [Batch 2000/4688] [D loss: 0.164128] [G loss: 0.500826]\n",
      "[Epoch 44/200] [Batch 3000/4688] [D loss: 0.205818] [G loss: 0.429208]\n",
      "[Epoch 44/200] [Batch 4000/4688] [D loss: 0.157987] [G loss: 0.548444]\n",
      "[Epoch 45/200] [Batch 0/4688] [D loss: 0.252334] [G loss: 0.154608]\n",
      "[Epoch 45/200] [Batch 1000/4688] [D loss: 0.189689] [G loss: 0.362529]\n",
      "[Epoch 45/200] [Batch 2000/4688] [D loss: 0.206039] [G loss: 0.339417]\n",
      "[Epoch 45/200] [Batch 3000/4688] [D loss: 0.159964] [G loss: 0.662123]\n",
      "[Epoch 45/200] [Batch 4000/4688] [D loss: 0.141780] [G loss: 0.533796]\n",
      "[Epoch 46/200] [Batch 0/4688] [D loss: 0.170259] [G loss: 0.425564]\n",
      "[Epoch 46/200] [Batch 1000/4688] [D loss: 0.197310] [G loss: 0.325835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 46/200] [Batch 2000/4688] [D loss: 0.176067] [G loss: 0.473062]\n",
      "[Epoch 46/200] [Batch 3000/4688] [D loss: 0.179059] [G loss: 0.438571]\n",
      "[Epoch 46/200] [Batch 4000/4688] [D loss: 0.195046] [G loss: 0.369775]\n",
      "[Epoch 47/200] [Batch 0/4688] [D loss: 0.154892] [G loss: 0.542294]\n",
      "[Epoch 47/200] [Batch 1000/4688] [D loss: 0.150064] [G loss: 0.598671]\n",
      "[Epoch 47/200] [Batch 2000/4688] [D loss: 0.167604] [G loss: 0.382281]\n",
      "[Epoch 47/200] [Batch 3000/4688] [D loss: 0.162782] [G loss: 0.425661]\n",
      "[Epoch 47/200] [Batch 4000/4688] [D loss: 0.172079] [G loss: 0.420341]\n",
      "[Epoch 48/200] [Batch 0/4688] [D loss: 0.168966] [G loss: 0.499750]\n",
      "[Epoch 48/200] [Batch 1000/4688] [D loss: 0.166946] [G loss: 0.601422]\n",
      "[Epoch 48/200] [Batch 2000/4688] [D loss: 0.164195] [G loss: 0.506110]\n",
      "[Epoch 48/200] [Batch 3000/4688] [D loss: 0.140451] [G loss: 0.527338]\n",
      "[Epoch 48/200] [Batch 4000/4688] [D loss: 0.171695] [G loss: 0.391536]\n",
      "[Epoch 49/200] [Batch 0/4688] [D loss: 0.183259] [G loss: 0.560354]\n",
      "[Epoch 49/200] [Batch 1000/4688] [D loss: 0.182176] [G loss: 0.355419]\n",
      "[Epoch 49/200] [Batch 2000/4688] [D loss: 0.168324] [G loss: 0.433967]\n",
      "[Epoch 49/200] [Batch 3000/4688] [D loss: 0.166649] [G loss: 0.377984]\n",
      "[Epoch 49/200] [Batch 4000/4688] [D loss: 0.222812] [G loss: 0.374125]\n",
      "[Epoch 50/200] [Batch 0/4688] [D loss: 0.164886] [G loss: 0.591910]\n",
      "[Epoch 50/200] [Batch 1000/4688] [D loss: 0.184400] [G loss: 0.368540]\n",
      "[Epoch 50/200] [Batch 2000/4688] [D loss: 0.163862] [G loss: 0.545411]\n",
      "[Epoch 50/200] [Batch 3000/4688] [D loss: 0.139889] [G loss: 0.638999]\n",
      "[Epoch 50/200] [Batch 4000/4688] [D loss: 0.190561] [G loss: 0.511231]\n",
      "[Epoch 51/200] [Batch 0/4688] [D loss: 0.189317] [G loss: 0.319361]\n",
      "[Epoch 51/200] [Batch 1000/4688] [D loss: 0.197028] [G loss: 0.499480]\n",
      "[Epoch 51/200] [Batch 2000/4688] [D loss: 0.136479] [G loss: 0.769525]\n",
      "[Epoch 51/200] [Batch 3000/4688] [D loss: 0.168794] [G loss: 0.355611]\n",
      "[Epoch 51/200] [Batch 4000/4688] [D loss: 0.148037] [G loss: 0.584102]\n",
      "[Epoch 52/200] [Batch 0/4688] [D loss: 0.158690] [G loss: 0.541298]\n",
      "[Epoch 52/200] [Batch 1000/4688] [D loss: 0.185555] [G loss: 0.340423]\n",
      "[Epoch 52/200] [Batch 2000/4688] [D loss: 0.176816] [G loss: 0.424439]\n",
      "[Epoch 52/200] [Batch 3000/4688] [D loss: 0.166560] [G loss: 0.517163]\n",
      "[Epoch 52/200] [Batch 4000/4688] [D loss: 0.218083] [G loss: 0.297469]\n",
      "[Epoch 53/200] [Batch 0/4688] [D loss: 0.147218] [G loss: 0.433727]\n",
      "[Epoch 53/200] [Batch 1000/4688] [D loss: 0.198912] [G loss: 0.438196]\n",
      "[Epoch 53/200] [Batch 2000/4688] [D loss: 0.151987] [G loss: 0.513760]\n",
      "[Epoch 53/200] [Batch 3000/4688] [D loss: 0.223553] [G loss: 0.214277]\n",
      "[Epoch 53/200] [Batch 4000/4688] [D loss: 0.156507] [G loss: 0.354580]\n",
      "[Epoch 54/200] [Batch 0/4688] [D loss: 0.229731] [G loss: 0.264396]\n",
      "[Epoch 54/200] [Batch 1000/4688] [D loss: 0.268494] [G loss: 0.190168]\n",
      "[Epoch 54/200] [Batch 2000/4688] [D loss: 0.162849] [G loss: 0.456007]\n",
      "[Epoch 54/200] [Batch 3000/4688] [D loss: 0.133708] [G loss: 0.567238]\n",
      "[Epoch 54/200] [Batch 4000/4688] [D loss: 0.150716] [G loss: 0.484462]\n",
      "[Epoch 55/200] [Batch 0/4688] [D loss: 0.176656] [G loss: 0.332122]\n",
      "[Epoch 55/200] [Batch 1000/4688] [D loss: 0.197483] [G loss: 0.474425]\n",
      "[Epoch 55/200] [Batch 2000/4688] [D loss: 0.193885] [G loss: 0.347233]\n",
      "[Epoch 55/200] [Batch 3000/4688] [D loss: 0.208444] [G loss: 0.271763]\n",
      "[Epoch 55/200] [Batch 4000/4688] [D loss: 0.172405] [G loss: 0.476203]\n",
      "[Epoch 56/200] [Batch 0/4688] [D loss: 0.185780] [G loss: 0.397823]\n",
      "[Epoch 56/200] [Batch 1000/4688] [D loss: 0.170096] [G loss: 0.521006]\n",
      "[Epoch 56/200] [Batch 2000/4688] [D loss: 0.148219] [G loss: 0.719456]\n",
      "[Epoch 56/200] [Batch 3000/4688] [D loss: 0.134334] [G loss: 0.609436]\n",
      "[Epoch 56/200] [Batch 4000/4688] [D loss: 0.152403] [G loss: 0.466048]\n",
      "[Epoch 57/200] [Batch 0/4688] [D loss: 0.167228] [G loss: 0.558028]\n",
      "[Epoch 57/200] [Batch 1000/4688] [D loss: 0.174151] [G loss: 0.366399]\n",
      "[Epoch 57/200] [Batch 2000/4688] [D loss: 0.221421] [G loss: 0.400878]\n",
      "[Epoch 57/200] [Batch 3000/4688] [D loss: 0.197639] [G loss: 0.476626]\n",
      "[Epoch 57/200] [Batch 4000/4688] [D loss: 0.169929] [G loss: 0.323476]\n",
      "[Epoch 58/200] [Batch 0/4688] [D loss: 0.209950] [G loss: 0.405864]\n",
      "[Epoch 58/200] [Batch 1000/4688] [D loss: 0.222948] [G loss: 0.226464]\n",
      "[Epoch 58/200] [Batch 2000/4688] [D loss: 0.191338] [G loss: 0.256560]\n",
      "[Epoch 58/200] [Batch 3000/4688] [D loss: 0.138807] [G loss: 0.483981]\n",
      "[Epoch 58/200] [Batch 4000/4688] [D loss: 0.134997] [G loss: 0.776928]\n",
      "[Epoch 59/200] [Batch 0/4688] [D loss: 0.198295] [G loss: 0.372837]\n",
      "[Epoch 59/200] [Batch 1000/4688] [D loss: 0.151567] [G loss: 0.631535]\n",
      "[Epoch 59/200] [Batch 2000/4688] [D loss: 0.226497] [G loss: 0.459943]\n",
      "[Epoch 59/200] [Batch 3000/4688] [D loss: 0.153804] [G loss: 0.573109]\n",
      "[Epoch 59/200] [Batch 4000/4688] [D loss: 0.196603] [G loss: 0.504895]\n",
      "[Epoch 60/200] [Batch 0/4688] [D loss: 0.189991] [G loss: 0.359777]\n",
      "[Epoch 60/200] [Batch 1000/4688] [D loss: 0.187247] [G loss: 0.413862]\n",
      "[Epoch 60/200] [Batch 2000/4688] [D loss: 0.230424] [G loss: 0.371053]\n",
      "[Epoch 60/200] [Batch 3000/4688] [D loss: 0.174284] [G loss: 0.402731]\n",
      "[Epoch 60/200] [Batch 4000/4688] [D loss: 0.197666] [G loss: 0.567468]\n",
      "[Epoch 61/200] [Batch 0/4688] [D loss: 0.148225] [G loss: 0.454409]\n",
      "[Epoch 61/200] [Batch 1000/4688] [D loss: 0.126632] [G loss: 0.526560]\n",
      "[Epoch 61/200] [Batch 2000/4688] [D loss: 0.172658] [G loss: 0.478675]\n",
      "[Epoch 61/200] [Batch 3000/4688] [D loss: 0.148723] [G loss: 0.675476]\n",
      "[Epoch 61/200] [Batch 4000/4688] [D loss: 0.201497] [G loss: 0.337171]\n",
      "[Epoch 62/200] [Batch 0/4688] [D loss: 0.208783] [G loss: 0.273338]\n",
      "[Epoch 62/200] [Batch 1000/4688] [D loss: 0.166845] [G loss: 0.508689]\n",
      "[Epoch 62/200] [Batch 2000/4688] [D loss: 0.184558] [G loss: 0.434014]\n",
      "[Epoch 62/200] [Batch 3000/4688] [D loss: 0.174725] [G loss: 0.414989]\n",
      "[Epoch 62/200] [Batch 4000/4688] [D loss: 0.153771] [G loss: 0.498905]\n",
      "[Epoch 63/200] [Batch 0/4688] [D loss: 0.184817] [G loss: 0.334072]\n",
      "[Epoch 63/200] [Batch 1000/4688] [D loss: 0.137891] [G loss: 0.417648]\n",
      "[Epoch 63/200] [Batch 2000/4688] [D loss: 0.164862] [G loss: 0.626002]\n",
      "[Epoch 63/200] [Batch 3000/4688] [D loss: 0.201163] [G loss: 0.438307]\n",
      "[Epoch 63/200] [Batch 4000/4688] [D loss: 0.138423] [G loss: 0.593133]\n",
      "[Epoch 64/200] [Batch 0/4688] [D loss: 0.169000] [G loss: 0.640449]\n",
      "[Epoch 64/200] [Batch 1000/4688] [D loss: 0.135167] [G loss: 0.534585]\n",
      "[Epoch 64/200] [Batch 2000/4688] [D loss: 0.172142] [G loss: 0.457356]\n",
      "[Epoch 64/200] [Batch 3000/4688] [D loss: 0.200578] [G loss: 0.403217]\n",
      "[Epoch 64/200] [Batch 4000/4688] [D loss: 0.169727] [G loss: 0.614773]\n",
      "[Epoch 65/200] [Batch 0/4688] [D loss: 0.166257] [G loss: 0.388196]\n",
      "[Epoch 65/200] [Batch 1000/4688] [D loss: 0.179531] [G loss: 0.340473]\n",
      "[Epoch 65/200] [Batch 2000/4688] [D loss: 0.123690] [G loss: 0.658656]\n",
      "[Epoch 65/200] [Batch 3000/4688] [D loss: 0.155175] [G loss: 0.522163]\n",
      "[Epoch 65/200] [Batch 4000/4688] [D loss: 0.166805] [G loss: 0.576110]\n",
      "[Epoch 66/200] [Batch 0/4688] [D loss: 0.155212] [G loss: 0.534663]\n",
      "[Epoch 66/200] [Batch 1000/4688] [D loss: 0.148499] [G loss: 0.548371]\n",
      "[Epoch 66/200] [Batch 2000/4688] [D loss: 0.167881] [G loss: 0.320579]\n",
      "[Epoch 66/200] [Batch 3000/4688] [D loss: 0.135089] [G loss: 0.435591]\n",
      "[Epoch 66/200] [Batch 4000/4688] [D loss: 0.177387] [G loss: 0.505985]\n",
      "[Epoch 67/200] [Batch 0/4688] [D loss: 0.169046] [G loss: 0.556595]\n",
      "[Epoch 67/200] [Batch 1000/4688] [D loss: 0.167784] [G loss: 0.412864]\n",
      "[Epoch 67/200] [Batch 2000/4688] [D loss: 0.170262] [G loss: 0.481199]\n",
      "[Epoch 67/200] [Batch 3000/4688] [D loss: 0.126094] [G loss: 0.647064]\n",
      "[Epoch 67/200] [Batch 4000/4688] [D loss: 0.184314] [G loss: 0.450730]\n",
      "[Epoch 68/200] [Batch 0/4688] [D loss: 0.143795] [G loss: 0.561756]\n",
      "[Epoch 68/200] [Batch 1000/4688] [D loss: 0.147335] [G loss: 0.574177]\n",
      "[Epoch 68/200] [Batch 2000/4688] [D loss: 0.116648] [G loss: 0.628271]\n",
      "[Epoch 68/200] [Batch 3000/4688] [D loss: 0.149197] [G loss: 0.405798]\n",
      "[Epoch 68/200] [Batch 4000/4688] [D loss: 0.148493] [G loss: 0.725497]\n",
      "[Epoch 69/200] [Batch 0/4688] [D loss: 0.170565] [G loss: 0.576191]\n",
      "[Epoch 69/200] [Batch 1000/4688] [D loss: 0.219190] [G loss: 0.374632]\n",
      "[Epoch 69/200] [Batch 2000/4688] [D loss: 0.150868] [G loss: 0.496147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 69/200] [Batch 3000/4688] [D loss: 0.134705] [G loss: 0.474614]\n",
      "[Epoch 69/200] [Batch 4000/4688] [D loss: 0.192033] [G loss: 0.571949]\n",
      "[Epoch 70/200] [Batch 0/4688] [D loss: 0.165886] [G loss: 0.490886]\n",
      "[Epoch 70/200] [Batch 1000/4688] [D loss: 0.155628] [G loss: 0.394113]\n",
      "[Epoch 70/200] [Batch 2000/4688] [D loss: 0.176813] [G loss: 0.589015]\n",
      "[Epoch 70/200] [Batch 3000/4688] [D loss: 0.122763] [G loss: 0.542323]\n",
      "[Epoch 70/200] [Batch 4000/4688] [D loss: 0.167341] [G loss: 0.402179]\n",
      "[Epoch 71/200] [Batch 0/4688] [D loss: 0.157340] [G loss: 0.524566]\n",
      "[Epoch 71/200] [Batch 1000/4688] [D loss: 0.156193] [G loss: 0.646850]\n",
      "[Epoch 71/200] [Batch 2000/4688] [D loss: 0.196558] [G loss: 0.329974]\n",
      "[Epoch 71/200] [Batch 3000/4688] [D loss: 0.177245] [G loss: 0.523431]\n",
      "[Epoch 71/200] [Batch 4000/4688] [D loss: 0.137829] [G loss: 0.585688]\n",
      "[Epoch 72/200] [Batch 0/4688] [D loss: 0.181047] [G loss: 0.363899]\n",
      "[Epoch 72/200] [Batch 1000/4688] [D loss: 0.161000] [G loss: 0.565419]\n",
      "[Epoch 72/200] [Batch 2000/4688] [D loss: 0.137823] [G loss: 0.539425]\n",
      "[Epoch 72/200] [Batch 3000/4688] [D loss: 0.192931] [G loss: 0.305481]\n",
      "[Epoch 72/200] [Batch 4000/4688] [D loss: 0.172118] [G loss: 0.532472]\n",
      "[Epoch 73/200] [Batch 0/4688] [D loss: 0.175815] [G loss: 0.371792]\n",
      "[Epoch 73/200] [Batch 1000/4688] [D loss: 0.157496] [G loss: 0.520255]\n",
      "[Epoch 73/200] [Batch 2000/4688] [D loss: 0.137908] [G loss: 0.521055]\n",
      "[Epoch 73/200] [Batch 3000/4688] [D loss: 0.181414] [G loss: 0.424476]\n",
      "[Epoch 73/200] [Batch 4000/4688] [D loss: 0.173328] [G loss: 0.408053]\n",
      "[Epoch 74/200] [Batch 0/4688] [D loss: 0.147539] [G loss: 0.362812]\n",
      "[Epoch 74/200] [Batch 1000/4688] [D loss: 0.169178] [G loss: 0.539958]\n",
      "[Epoch 74/200] [Batch 2000/4688] [D loss: 0.210784] [G loss: 0.633174]\n",
      "[Epoch 74/200] [Batch 3000/4688] [D loss: 0.181163] [G loss: 0.397059]\n",
      "[Epoch 74/200] [Batch 4000/4688] [D loss: 0.165743] [G loss: 0.428564]\n",
      "[Epoch 75/200] [Batch 0/4688] [D loss: 0.124953] [G loss: 0.592181]\n",
      "[Epoch 75/200] [Batch 1000/4688] [D loss: 0.150697] [G loss: 0.507385]\n",
      "[Epoch 75/200] [Batch 2000/4688] [D loss: 0.149176] [G loss: 0.504986]\n",
      "[Epoch 75/200] [Batch 3000/4688] [D loss: 0.159751] [G loss: 0.529333]\n",
      "[Epoch 75/200] [Batch 4000/4688] [D loss: 0.166332] [G loss: 0.503332]\n",
      "[Epoch 76/200] [Batch 0/4688] [D loss: 0.130786] [G loss: 0.647411]\n",
      "[Epoch 76/200] [Batch 1000/4688] [D loss: 0.153554] [G loss: 0.495621]\n",
      "[Epoch 76/200] [Batch 2000/4688] [D loss: 0.198130] [G loss: 0.436725]\n",
      "[Epoch 76/200] [Batch 3000/4688] [D loss: 0.171352] [G loss: 0.403209]\n",
      "[Epoch 76/200] [Batch 4000/4688] [D loss: 0.181479] [G loss: 0.303744]\n",
      "[Epoch 77/200] [Batch 0/4688] [D loss: 0.143929] [G loss: 0.408901]\n",
      "[Epoch 77/200] [Batch 1000/4688] [D loss: 0.140064] [G loss: 0.625144]\n",
      "[Epoch 77/200] [Batch 2000/4688] [D loss: 0.187557] [G loss: 0.349159]\n",
      "[Epoch 77/200] [Batch 3000/4688] [D loss: 0.132817] [G loss: 0.428921]\n",
      "[Epoch 77/200] [Batch 4000/4688] [D loss: 0.157331] [G loss: 0.545972]\n",
      "[Epoch 78/200] [Batch 0/4688] [D loss: 0.154397] [G loss: 0.349735]\n",
      "[Epoch 78/200] [Batch 1000/4688] [D loss: 0.151634] [G loss: 0.410466]\n",
      "[Epoch 78/200] [Batch 2000/4688] [D loss: 0.185240] [G loss: 0.544079]\n",
      "[Epoch 78/200] [Batch 3000/4688] [D loss: 0.155138] [G loss: 0.504618]\n",
      "[Epoch 78/200] [Batch 4000/4688] [D loss: 0.136504] [G loss: 0.828948]\n",
      "[Epoch 79/200] [Batch 0/4688] [D loss: 0.187677] [G loss: 0.407199]\n",
      "[Epoch 79/200] [Batch 1000/4688] [D loss: 0.191607] [G loss: 0.677068]\n",
      "[Epoch 79/200] [Batch 2000/4688] [D loss: 0.161287] [G loss: 0.563284]\n",
      "[Epoch 79/200] [Batch 3000/4688] [D loss: 0.177592] [G loss: 0.438039]\n",
      "[Epoch 79/200] [Batch 4000/4688] [D loss: 0.123385] [G loss: 0.624753]\n",
      "[Epoch 80/200] [Batch 0/4688] [D loss: 0.145549] [G loss: 0.614234]\n",
      "[Epoch 80/200] [Batch 1000/4688] [D loss: 0.157946] [G loss: 0.573667]\n",
      "[Epoch 80/200] [Batch 2000/4688] [D loss: 0.171276] [G loss: 0.574849]\n",
      "[Epoch 80/200] [Batch 3000/4688] [D loss: 0.168645] [G loss: 0.489355]\n",
      "[Epoch 80/200] [Batch 4000/4688] [D loss: 0.177457] [G loss: 0.329306]\n",
      "[Epoch 81/200] [Batch 0/4688] [D loss: 0.178727] [G loss: 0.508085]\n",
      "[Epoch 81/200] [Batch 1000/4688] [D loss: 0.129791] [G loss: 0.538291]\n",
      "[Epoch 81/200] [Batch 2000/4688] [D loss: 0.138295] [G loss: 0.744905]\n",
      "[Epoch 81/200] [Batch 3000/4688] [D loss: 0.153370] [G loss: 0.566801]\n",
      "[Epoch 81/200] [Batch 4000/4688] [D loss: 0.162230] [G loss: 0.472863]\n",
      "[Epoch 82/200] [Batch 0/4688] [D loss: 0.166465] [G loss: 0.380286]\n",
      "[Epoch 82/200] [Batch 1000/4688] [D loss: 0.116833] [G loss: 0.647065]\n",
      "[Epoch 82/200] [Batch 2000/4688] [D loss: 0.164239] [G loss: 0.541545]\n",
      "[Epoch 82/200] [Batch 3000/4688] [D loss: 0.175018] [G loss: 0.549230]\n",
      "[Epoch 82/200] [Batch 4000/4688] [D loss: 0.172081] [G loss: 0.577046]\n",
      "[Epoch 83/200] [Batch 0/4688] [D loss: 0.186602] [G loss: 0.352424]\n",
      "[Epoch 83/200] [Batch 1000/4688] [D loss: 0.181088] [G loss: 0.451956]\n",
      "[Epoch 83/200] [Batch 2000/4688] [D loss: 0.158394] [G loss: 0.470543]\n",
      "[Epoch 83/200] [Batch 3000/4688] [D loss: 0.171172] [G loss: 0.554337]\n",
      "[Epoch 83/200] [Batch 4000/4688] [D loss: 0.147746] [G loss: 0.404919]\n",
      "[Epoch 84/200] [Batch 0/4688] [D loss: 0.147093] [G loss: 0.545150]\n",
      "[Epoch 84/200] [Batch 1000/4688] [D loss: 0.161266] [G loss: 0.416294]\n",
      "[Epoch 84/200] [Batch 2000/4688] [D loss: 0.148728] [G loss: 0.570088]\n",
      "[Epoch 84/200] [Batch 3000/4688] [D loss: 0.130320] [G loss: 0.541091]\n",
      "[Epoch 84/200] [Batch 4000/4688] [D loss: 0.147070] [G loss: 0.544851]\n",
      "[Epoch 85/200] [Batch 0/4688] [D loss: 0.135598] [G loss: 0.632452]\n",
      "[Epoch 85/200] [Batch 1000/4688] [D loss: 0.180641] [G loss: 0.560483]\n",
      "[Epoch 85/200] [Batch 2000/4688] [D loss: 0.146558] [G loss: 0.494283]\n",
      "[Epoch 85/200] [Batch 3000/4688] [D loss: 0.135784] [G loss: 0.485145]\n",
      "[Epoch 85/200] [Batch 4000/4688] [D loss: 0.136375] [G loss: 0.542992]\n",
      "[Epoch 86/200] [Batch 0/4688] [D loss: 0.187861] [G loss: 0.530269]\n",
      "[Epoch 86/200] [Batch 1000/4688] [D loss: 0.143989] [G loss: 0.569617]\n",
      "[Epoch 86/200] [Batch 2000/4688] [D loss: 0.159790] [G loss: 0.693247]\n",
      "[Epoch 86/200] [Batch 3000/4688] [D loss: 0.145743] [G loss: 0.530699]\n",
      "[Epoch 86/200] [Batch 4000/4688] [D loss: 0.124302] [G loss: 0.584158]\n",
      "[Epoch 87/200] [Batch 0/4688] [D loss: 0.158879] [G loss: 0.392344]\n",
      "[Epoch 87/200] [Batch 1000/4688] [D loss: 0.124747] [G loss: 0.685021]\n",
      "[Epoch 87/200] [Batch 2000/4688] [D loss: 0.126479] [G loss: 0.544347]\n",
      "[Epoch 87/200] [Batch 3000/4688] [D loss: 0.165950] [G loss: 0.407814]\n",
      "[Epoch 87/200] [Batch 4000/4688] [D loss: 0.128222] [G loss: 0.455800]\n",
      "[Epoch 88/200] [Batch 0/4688] [D loss: 0.143637] [G loss: 0.466058]\n",
      "[Epoch 88/200] [Batch 1000/4688] [D loss: 0.182566] [G loss: 0.358897]\n",
      "[Epoch 88/200] [Batch 2000/4688] [D loss: 0.172608] [G loss: 0.389385]\n",
      "[Epoch 88/200] [Batch 3000/4688] [D loss: 0.165943] [G loss: 0.431186]\n",
      "[Epoch 88/200] [Batch 4000/4688] [D loss: 0.165717] [G loss: 0.538465]\n",
      "[Epoch 89/200] [Batch 0/4688] [D loss: 0.122214] [G loss: 0.690090]\n",
      "[Epoch 89/200] [Batch 1000/4688] [D loss: 0.134011] [G loss: 0.605710]\n",
      "[Epoch 89/200] [Batch 2000/4688] [D loss: 0.141235] [G loss: 0.568792]\n",
      "[Epoch 89/200] [Batch 3000/4688] [D loss: 0.140994] [G loss: 0.612530]\n",
      "[Epoch 89/200] [Batch 4000/4688] [D loss: 0.141056] [G loss: 0.451124]\n",
      "[Epoch 90/200] [Batch 0/4688] [D loss: 0.147057] [G loss: 0.439258]\n",
      "[Epoch 90/200] [Batch 1000/4688] [D loss: 0.124994] [G loss: 0.580368]\n",
      "[Epoch 90/200] [Batch 2000/4688] [D loss: 0.126938] [G loss: 0.587462]\n",
      "[Epoch 90/200] [Batch 3000/4688] [D loss: 0.150681] [G loss: 0.378480]\n",
      "[Epoch 90/200] [Batch 4000/4688] [D loss: 0.148821] [G loss: 0.502654]\n",
      "[Epoch 91/200] [Batch 0/4688] [D loss: 0.170728] [G loss: 0.360746]\n",
      "[Epoch 91/200] [Batch 1000/4688] [D loss: 0.166234] [G loss: 0.765222]\n",
      "[Epoch 91/200] [Batch 2000/4688] [D loss: 0.187931] [G loss: 0.359465]\n",
      "[Epoch 91/200] [Batch 3000/4688] [D loss: 0.079938] [G loss: 0.657973]\n",
      "[Epoch 91/200] [Batch 4000/4688] [D loss: 0.126821] [G loss: 0.548210]\n",
      "[Epoch 92/200] [Batch 0/4688] [D loss: 0.152784] [G loss: 0.472567]\n",
      "[Epoch 92/200] [Batch 1000/4688] [D loss: 0.171745] [G loss: 0.505502]\n",
      "[Epoch 92/200] [Batch 2000/4688] [D loss: 0.142945] [G loss: 0.632212]\n",
      "[Epoch 92/200] [Batch 3000/4688] [D loss: 0.171142] [G loss: 0.604580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 92/200] [Batch 4000/4688] [D loss: 0.178802] [G loss: 0.623074]\n",
      "[Epoch 93/200] [Batch 0/4688] [D loss: 0.150539] [G loss: 0.594306]\n",
      "[Epoch 93/200] [Batch 1000/4688] [D loss: 0.114374] [G loss: 0.687733]\n",
      "[Epoch 93/200] [Batch 2000/4688] [D loss: 0.148666] [G loss: 0.635372]\n",
      "[Epoch 93/200] [Batch 3000/4688] [D loss: 0.130079] [G loss: 0.507533]\n",
      "[Epoch 93/200] [Batch 4000/4688] [D loss: 0.156543] [G loss: 0.388943]\n",
      "[Epoch 94/200] [Batch 0/4688] [D loss: 0.136982] [G loss: 0.582811]\n",
      "[Epoch 94/200] [Batch 1000/4688] [D loss: 0.138918] [G loss: 0.625504]\n",
      "[Epoch 94/200] [Batch 2000/4688] [D loss: 0.110242] [G loss: 0.806286]\n",
      "[Epoch 94/200] [Batch 3000/4688] [D loss: 0.146742] [G loss: 0.548056]\n",
      "[Epoch 94/200] [Batch 4000/4688] [D loss: 0.160412] [G loss: 0.402836]\n",
      "[Epoch 95/200] [Batch 0/4688] [D loss: 0.152200] [G loss: 0.528581]\n",
      "[Epoch 95/200] [Batch 1000/4688] [D loss: 0.153916] [G loss: 0.523834]\n",
      "[Epoch 95/200] [Batch 2000/4688] [D loss: 0.112046] [G loss: 0.669241]\n",
      "[Epoch 95/200] [Batch 3000/4688] [D loss: 0.124389] [G loss: 0.540024]\n",
      "[Epoch 95/200] [Batch 4000/4688] [D loss: 0.108969] [G loss: 0.613624]\n",
      "[Epoch 96/200] [Batch 0/4688] [D loss: 0.109153] [G loss: 0.573696]\n",
      "[Epoch 96/200] [Batch 1000/4688] [D loss: 0.160374] [G loss: 0.410851]\n",
      "[Epoch 96/200] [Batch 2000/4688] [D loss: 0.152241] [G loss: 0.423352]\n",
      "[Epoch 96/200] [Batch 3000/4688] [D loss: 0.110284] [G loss: 0.710114]\n",
      "[Epoch 96/200] [Batch 4000/4688] [D loss: 0.157118] [G loss: 0.447110]\n",
      "[Epoch 97/200] [Batch 0/4688] [D loss: 0.100616] [G loss: 0.740674]\n",
      "[Epoch 97/200] [Batch 1000/4688] [D loss: 0.113962] [G loss: 0.464950]\n",
      "[Epoch 97/200] [Batch 2000/4688] [D loss: 0.164199] [G loss: 0.474045]\n",
      "[Epoch 97/200] [Batch 3000/4688] [D loss: 0.147717] [G loss: 0.761497]\n",
      "[Epoch 97/200] [Batch 4000/4688] [D loss: 0.181466] [G loss: 0.720071]\n",
      "[Epoch 98/200] [Batch 0/4688] [D loss: 0.154559] [G loss: 0.609604]\n",
      "[Epoch 98/200] [Batch 1000/4688] [D loss: 0.100904] [G loss: 0.723248]\n",
      "[Epoch 98/200] [Batch 2000/4688] [D loss: 0.114471] [G loss: 0.643135]\n",
      "[Epoch 98/200] [Batch 3000/4688] [D loss: 0.148536] [G loss: 0.438836]\n",
      "[Epoch 98/200] [Batch 4000/4688] [D loss: 0.136579] [G loss: 0.478775]\n",
      "[Epoch 99/200] [Batch 0/4688] [D loss: 0.101630] [G loss: 0.660691]\n",
      "[Epoch 99/200] [Batch 1000/4688] [D loss: 0.168509] [G loss: 0.475312]\n",
      "[Epoch 99/200] [Batch 2000/4688] [D loss: 0.133751] [G loss: 0.440671]\n",
      "[Epoch 99/200] [Batch 3000/4688] [D loss: 0.146509] [G loss: 0.635484]\n",
      "[Epoch 99/200] [Batch 4000/4688] [D loss: 0.145457] [G loss: 0.472577]\n",
      "[Epoch 100/200] [Batch 0/4688] [D loss: 0.155091] [G loss: 0.401961]\n",
      "[Epoch 100/200] [Batch 1000/4688] [D loss: 0.105098] [G loss: 0.766318]\n",
      "[Epoch 100/200] [Batch 2000/4688] [D loss: 0.092088] [G loss: 0.849225]\n",
      "[Epoch 100/200] [Batch 3000/4688] [D loss: 0.108259] [G loss: 0.714928]\n",
      "[Epoch 100/200] [Batch 4000/4688] [D loss: 0.112619] [G loss: 0.582423]\n",
      "[Epoch 101/200] [Batch 0/4688] [D loss: 0.110579] [G loss: 0.554096]\n",
      "[Epoch 101/200] [Batch 1000/4688] [D loss: 0.139590] [G loss: 0.699098]\n",
      "[Epoch 101/200] [Batch 2000/4688] [D loss: 0.136406] [G loss: 0.434782]\n",
      "[Epoch 101/200] [Batch 3000/4688] [D loss: 0.129492] [G loss: 0.511333]\n",
      "[Epoch 101/200] [Batch 4000/4688] [D loss: 0.175006] [G loss: 0.385050]\n",
      "[Epoch 102/200] [Batch 0/4688] [D loss: 0.129437] [G loss: 0.659917]\n",
      "[Epoch 102/200] [Batch 1000/4688] [D loss: 0.118248] [G loss: 0.639757]\n",
      "[Epoch 102/200] [Batch 2000/4688] [D loss: 0.111655] [G loss: 0.630494]\n",
      "[Epoch 102/200] [Batch 3000/4688] [D loss: 0.154618] [G loss: 0.485381]\n",
      "[Epoch 102/200] [Batch 4000/4688] [D loss: 0.126360] [G loss: 0.700047]\n",
      "[Epoch 103/200] [Batch 0/4688] [D loss: 0.125348] [G loss: 0.592867]\n",
      "[Epoch 103/200] [Batch 1000/4688] [D loss: 0.171467] [G loss: 0.432180]\n",
      "[Epoch 103/200] [Batch 2000/4688] [D loss: 0.148547] [G loss: 0.836611]\n",
      "[Epoch 103/200] [Batch 3000/4688] [D loss: 0.123989] [G loss: 0.588376]\n",
      "[Epoch 103/200] [Batch 4000/4688] [D loss: 0.130636] [G loss: 0.582404]\n",
      "[Epoch 104/200] [Batch 0/4688] [D loss: 0.127253] [G loss: 0.583426]\n",
      "[Epoch 104/200] [Batch 1000/4688] [D loss: 0.134156] [G loss: 0.580926]\n",
      "[Epoch 104/200] [Batch 2000/4688] [D loss: 0.148997] [G loss: 0.557045]\n",
      "[Epoch 104/200] [Batch 3000/4688] [D loss: 0.126045] [G loss: 0.677086]\n",
      "[Epoch 104/200] [Batch 4000/4688] [D loss: 0.128307] [G loss: 0.641882]\n",
      "[Epoch 105/200] [Batch 0/4688] [D loss: 0.132884] [G loss: 0.488837]\n",
      "[Epoch 105/200] [Batch 1000/4688] [D loss: 0.152803] [G loss: 0.460402]\n",
      "[Epoch 105/200] [Batch 2000/4688] [D loss: 0.139400] [G loss: 0.606515]\n",
      "[Epoch 105/200] [Batch 3000/4688] [D loss: 0.127949] [G loss: 0.538671]\n",
      "[Epoch 105/200] [Batch 4000/4688] [D loss: 0.133742] [G loss: 0.441055]\n",
      "[Epoch 106/200] [Batch 0/4688] [D loss: 0.111807] [G loss: 0.494637]\n",
      "[Epoch 106/200] [Batch 1000/4688] [D loss: 0.169889] [G loss: 0.584779]\n",
      "[Epoch 106/200] [Batch 2000/4688] [D loss: 0.139699] [G loss: 0.398060]\n",
      "[Epoch 106/200] [Batch 3000/4688] [D loss: 0.132554] [G loss: 0.679510]\n",
      "[Epoch 106/200] [Batch 4000/4688] [D loss: 0.149247] [G loss: 0.521111]\n",
      "[Epoch 107/200] [Batch 0/4688] [D loss: 0.151826] [G loss: 0.367692]\n",
      "[Epoch 107/200] [Batch 1000/4688] [D loss: 0.128253] [G loss: 0.648378]\n",
      "[Epoch 107/200] [Batch 2000/4688] [D loss: 0.128582] [G loss: 0.706598]\n",
      "[Epoch 107/200] [Batch 3000/4688] [D loss: 0.126818] [G loss: 0.566963]\n",
      "[Epoch 107/200] [Batch 4000/4688] [D loss: 0.085561] [G loss: 0.694104]\n",
      "[Epoch 108/200] [Batch 0/4688] [D loss: 0.119425] [G loss: 0.784811]\n",
      "[Epoch 108/200] [Batch 1000/4688] [D loss: 0.149009] [G loss: 0.738464]\n",
      "[Epoch 108/200] [Batch 2000/4688] [D loss: 0.121253] [G loss: 0.729915]\n",
      "[Epoch 108/200] [Batch 3000/4688] [D loss: 0.135062] [G loss: 0.675673]\n",
      "[Epoch 108/200] [Batch 4000/4688] [D loss: 0.200467] [G loss: 0.339351]\n",
      "[Epoch 109/200] [Batch 0/4688] [D loss: 0.115618] [G loss: 0.553263]\n",
      "[Epoch 109/200] [Batch 1000/4688] [D loss: 0.109210] [G loss: 0.702554]\n",
      "[Epoch 109/200] [Batch 2000/4688] [D loss: 0.128601] [G loss: 0.506864]\n",
      "[Epoch 109/200] [Batch 3000/4688] [D loss: 0.098701] [G loss: 0.712747]\n",
      "[Epoch 109/200] [Batch 4000/4688] [D loss: 0.119499] [G loss: 0.605254]\n",
      "[Epoch 110/200] [Batch 0/4688] [D loss: 0.119788] [G loss: 0.774645]\n",
      "[Epoch 110/200] [Batch 1000/4688] [D loss: 0.123416] [G loss: 0.728464]\n",
      "[Epoch 110/200] [Batch 2000/4688] [D loss: 0.132441] [G loss: 0.559072]\n",
      "[Epoch 110/200] [Batch 3000/4688] [D loss: 0.121856] [G loss: 0.717722]\n",
      "[Epoch 110/200] [Batch 4000/4688] [D loss: 0.087774] [G loss: 0.791647]\n",
      "[Epoch 111/200] [Batch 0/4688] [D loss: 0.136502] [G loss: 0.521897]\n",
      "[Epoch 111/200] [Batch 1000/4688] [D loss: 0.125918] [G loss: 0.574332]\n",
      "[Epoch 111/200] [Batch 2000/4688] [D loss: 0.141118] [G loss: 0.535118]\n",
      "[Epoch 111/200] [Batch 3000/4688] [D loss: 0.124042] [G loss: 0.513767]\n",
      "[Epoch 111/200] [Batch 4000/4688] [D loss: 0.150433] [G loss: 0.445892]\n",
      "[Epoch 112/200] [Batch 0/4688] [D loss: 0.137796] [G loss: 0.574491]\n",
      "[Epoch 112/200] [Batch 1000/4688] [D loss: 0.128127] [G loss: 0.709534]\n",
      "[Epoch 112/200] [Batch 2000/4688] [D loss: 0.105692] [G loss: 0.635062]\n",
      "[Epoch 112/200] [Batch 3000/4688] [D loss: 0.118507] [G loss: 0.629692]\n",
      "[Epoch 112/200] [Batch 4000/4688] [D loss: 0.139117] [G loss: 0.466106]\n",
      "[Epoch 113/200] [Batch 0/4688] [D loss: 0.112094] [G loss: 0.623327]\n",
      "[Epoch 113/200] [Batch 1000/4688] [D loss: 0.130581] [G loss: 0.577152]\n",
      "[Epoch 113/200] [Batch 2000/4688] [D loss: 0.110624] [G loss: 0.656935]\n",
      "[Epoch 113/200] [Batch 3000/4688] [D loss: 0.157556] [G loss: 0.431404]\n",
      "[Epoch 113/200] [Batch 4000/4688] [D loss: 0.127776] [G loss: 0.621169]\n",
      "[Epoch 114/200] [Batch 0/4688] [D loss: 0.122808] [G loss: 0.606781]\n",
      "[Epoch 114/200] [Batch 1000/4688] [D loss: 0.135534] [G loss: 0.426946]\n",
      "[Epoch 114/200] [Batch 2000/4688] [D loss: 0.108462] [G loss: 0.707337]\n",
      "[Epoch 114/200] [Batch 3000/4688] [D loss: 0.139580] [G loss: 0.537789]\n",
      "[Epoch 114/200] [Batch 4000/4688] [D loss: 0.115407] [G loss: 0.833273]\n",
      "[Epoch 115/200] [Batch 0/4688] [D loss: 0.163073] [G loss: 0.542616]\n",
      "[Epoch 115/200] [Batch 1000/4688] [D loss: 0.138190] [G loss: 0.732817]\n",
      "[Epoch 115/200] [Batch 2000/4688] [D loss: 0.103499] [G loss: 0.652460]\n",
      "[Epoch 115/200] [Batch 3000/4688] [D loss: 0.115606] [G loss: 0.694023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 115/200] [Batch 4000/4688] [D loss: 0.157094] [G loss: 0.662823]\n",
      "[Epoch 116/200] [Batch 0/4688] [D loss: 0.106427] [G loss: 0.749799]\n",
      "[Epoch 116/200] [Batch 1000/4688] [D loss: 0.129981] [G loss: 0.565241]\n",
      "[Epoch 116/200] [Batch 2000/4688] [D loss: 0.101730] [G loss: 0.801313]\n",
      "[Epoch 116/200] [Batch 3000/4688] [D loss: 0.113048] [G loss: 0.668790]\n",
      "[Epoch 116/200] [Batch 4000/4688] [D loss: 0.119651] [G loss: 0.694148]\n",
      "[Epoch 117/200] [Batch 0/4688] [D loss: 0.149573] [G loss: 0.586764]\n",
      "[Epoch 117/200] [Batch 1000/4688] [D loss: 0.121795] [G loss: 0.592493]\n",
      "[Epoch 117/200] [Batch 2000/4688] [D loss: 0.112732] [G loss: 0.681178]\n",
      "[Epoch 117/200] [Batch 3000/4688] [D loss: 0.111261] [G loss: 0.492649]\n",
      "[Epoch 117/200] [Batch 4000/4688] [D loss: 0.121244] [G loss: 0.642495]\n",
      "[Epoch 118/200] [Batch 0/4688] [D loss: 0.112934] [G loss: 0.515984]\n",
      "[Epoch 118/200] [Batch 1000/4688] [D loss: 0.115310] [G loss: 0.636364]\n",
      "[Epoch 118/200] [Batch 2000/4688] [D loss: 0.113848] [G loss: 0.724360]\n",
      "[Epoch 118/200] [Batch 3000/4688] [D loss: 0.152009] [G loss: 0.693999]\n",
      "[Epoch 118/200] [Batch 4000/4688] [D loss: 0.160731] [G loss: 0.656062]\n",
      "[Epoch 119/200] [Batch 0/4688] [D loss: 0.124832] [G loss: 0.807091]\n",
      "[Epoch 119/200] [Batch 1000/4688] [D loss: 0.122763] [G loss: 0.536544]\n",
      "[Epoch 119/200] [Batch 2000/4688] [D loss: 0.152698] [G loss: 0.694488]\n",
      "[Epoch 119/200] [Batch 3000/4688] [D loss: 0.135658] [G loss: 0.411724]\n",
      "[Epoch 119/200] [Batch 4000/4688] [D loss: 0.114663] [G loss: 0.696205]\n",
      "[Epoch 120/200] [Batch 0/4688] [D loss: 0.123909] [G loss: 0.754524]\n",
      "[Epoch 120/200] [Batch 1000/4688] [D loss: 0.128617] [G loss: 0.622890]\n",
      "[Epoch 120/200] [Batch 2000/4688] [D loss: 0.127723] [G loss: 0.522393]\n",
      "[Epoch 120/200] [Batch 3000/4688] [D loss: 0.091793] [G loss: 0.617426]\n",
      "[Epoch 120/200] [Batch 4000/4688] [D loss: 0.092992] [G loss: 0.754136]\n",
      "[Epoch 121/200] [Batch 0/4688] [D loss: 0.125410] [G loss: 0.459622]\n",
      "[Epoch 121/200] [Batch 1000/4688] [D loss: 0.120729] [G loss: 0.674496]\n",
      "[Epoch 121/200] [Batch 2000/4688] [D loss: 0.093942] [G loss: 0.644094]\n",
      "[Epoch 121/200] [Batch 3000/4688] [D loss: 0.158285] [G loss: 0.501248]\n",
      "[Epoch 121/200] [Batch 4000/4688] [D loss: 0.131847] [G loss: 0.541186]\n",
      "[Epoch 122/200] [Batch 0/4688] [D loss: 0.142079] [G loss: 0.383959]\n",
      "[Epoch 122/200] [Batch 1000/4688] [D loss: 0.098535] [G loss: 0.730563]\n",
      "[Epoch 122/200] [Batch 2000/4688] [D loss: 0.115531] [G loss: 0.557287]\n",
      "[Epoch 122/200] [Batch 3000/4688] [D loss: 0.121122] [G loss: 0.716833]\n",
      "[Epoch 122/200] [Batch 4000/4688] [D loss: 0.141160] [G loss: 0.500680]\n",
      "[Epoch 123/200] [Batch 0/4688] [D loss: 0.091948] [G loss: 0.681032]\n",
      "[Epoch 123/200] [Batch 1000/4688] [D loss: 0.127532] [G loss: 0.404311]\n",
      "[Epoch 123/200] [Batch 2000/4688] [D loss: 0.115690] [G loss: 0.668954]\n",
      "[Epoch 123/200] [Batch 3000/4688] [D loss: 0.129352] [G loss: 0.484062]\n",
      "[Epoch 123/200] [Batch 4000/4688] [D loss: 0.101396] [G loss: 0.637548]\n",
      "[Epoch 124/200] [Batch 0/4688] [D loss: 0.202140] [G loss: 0.260841]\n",
      "[Epoch 124/200] [Batch 1000/4688] [D loss: 0.135635] [G loss: 0.488550]\n",
      "[Epoch 124/200] [Batch 2000/4688] [D loss: 0.099157] [G loss: 0.509654]\n",
      "[Epoch 124/200] [Batch 3000/4688] [D loss: 0.127561] [G loss: 0.610136]\n",
      "[Epoch 124/200] [Batch 4000/4688] [D loss: 0.100863] [G loss: 0.708577]\n",
      "[Epoch 125/200] [Batch 0/4688] [D loss: 0.150851] [G loss: 0.451196]\n",
      "[Epoch 125/200] [Batch 1000/4688] [D loss: 0.133532] [G loss: 0.531626]\n",
      "[Epoch 125/200] [Batch 2000/4688] [D loss: 0.117894] [G loss: 0.500906]\n",
      "[Epoch 125/200] [Batch 3000/4688] [D loss: 0.116155] [G loss: 0.784131]\n",
      "[Epoch 125/200] [Batch 4000/4688] [D loss: 0.115281] [G loss: 0.501151]\n",
      "[Epoch 126/200] [Batch 0/4688] [D loss: 0.099877] [G loss: 0.701721]\n",
      "[Epoch 126/200] [Batch 1000/4688] [D loss: 0.137850] [G loss: 0.603482]\n",
      "[Epoch 126/200] [Batch 2000/4688] [D loss: 0.105453] [G loss: 0.664584]\n",
      "[Epoch 126/200] [Batch 3000/4688] [D loss: 0.118801] [G loss: 0.568376]\n",
      "[Epoch 126/200] [Batch 4000/4688] [D loss: 0.117467] [G loss: 0.587360]\n",
      "[Epoch 127/200] [Batch 0/4688] [D loss: 0.105680] [G loss: 0.623301]\n",
      "[Epoch 127/200] [Batch 1000/4688] [D loss: 0.134564] [G loss: 0.405400]\n",
      "[Epoch 127/200] [Batch 2000/4688] [D loss: 0.131367] [G loss: 0.633508]\n",
      "[Epoch 127/200] [Batch 3000/4688] [D loss: 0.115304] [G loss: 0.602845]\n",
      "[Epoch 127/200] [Batch 4000/4688] [D loss: 0.091086] [G loss: 0.603119]\n",
      "[Epoch 128/200] [Batch 0/4688] [D loss: 0.118930] [G loss: 0.616912]\n",
      "[Epoch 128/200] [Batch 1000/4688] [D loss: 0.107879] [G loss: 0.914734]\n",
      "[Epoch 128/200] [Batch 2000/4688] [D loss: 0.128046] [G loss: 0.527188]\n",
      "[Epoch 128/200] [Batch 3000/4688] [D loss: 0.133165] [G loss: 0.902710]\n",
      "[Epoch 128/200] [Batch 4000/4688] [D loss: 0.087912] [G loss: 0.763254]\n",
      "[Epoch 129/200] [Batch 0/4688] [D loss: 0.090805] [G loss: 0.679055]\n",
      "[Epoch 129/200] [Batch 1000/4688] [D loss: 0.083979] [G loss: 0.625430]\n",
      "[Epoch 129/200] [Batch 2000/4688] [D loss: 0.092687] [G loss: 0.852618]\n",
      "[Epoch 129/200] [Batch 3000/4688] [D loss: 0.113260] [G loss: 0.644036]\n",
      "[Epoch 129/200] [Batch 4000/4688] [D loss: 0.108957] [G loss: 0.713519]\n",
      "[Epoch 130/200] [Batch 0/4688] [D loss: 0.107283] [G loss: 0.886513]\n",
      "[Epoch 130/200] [Batch 1000/4688] [D loss: 0.105872] [G loss: 0.712434]\n",
      "[Epoch 130/200] [Batch 2000/4688] [D loss: 0.110778] [G loss: 0.670498]\n",
      "[Epoch 130/200] [Batch 3000/4688] [D loss: 0.093103] [G loss: 0.700714]\n",
      "[Epoch 130/200] [Batch 4000/4688] [D loss: 0.118387] [G loss: 0.595366]\n",
      "[Epoch 131/200] [Batch 0/4688] [D loss: 0.084576] [G loss: 0.808597]\n",
      "[Epoch 131/200] [Batch 1000/4688] [D loss: 0.098433] [G loss: 0.649548]\n",
      "[Epoch 131/200] [Batch 2000/4688] [D loss: 0.091429] [G loss: 0.708442]\n",
      "[Epoch 131/200] [Batch 3000/4688] [D loss: 0.087560] [G loss: 0.610352]\n",
      "[Epoch 131/200] [Batch 4000/4688] [D loss: 0.135726] [G loss: 0.637512]\n",
      "[Epoch 132/200] [Batch 0/4688] [D loss: 0.117860] [G loss: 0.467292]\n",
      "[Epoch 132/200] [Batch 1000/4688] [D loss: 0.104171] [G loss: 0.828524]\n",
      "[Epoch 132/200] [Batch 2000/4688] [D loss: 0.147187] [G loss: 0.480533]\n",
      "[Epoch 132/200] [Batch 3000/4688] [D loss: 0.137692] [G loss: 0.421166]\n",
      "[Epoch 132/200] [Batch 4000/4688] [D loss: 0.111947] [G loss: 0.630727]\n",
      "[Epoch 133/200] [Batch 0/4688] [D loss: 0.105807] [G loss: 0.703363]\n",
      "[Epoch 133/200] [Batch 1000/4688] [D loss: 0.125678] [G loss: 0.453244]\n",
      "[Epoch 133/200] [Batch 2000/4688] [D loss: 0.104368] [G loss: 0.829237]\n",
      "[Epoch 133/200] [Batch 3000/4688] [D loss: 0.114423] [G loss: 0.694159]\n",
      "[Epoch 133/200] [Batch 4000/4688] [D loss: 0.086761] [G loss: 0.722126]\n",
      "[Epoch 134/200] [Batch 0/4688] [D loss: 0.154181] [G loss: 0.480984]\n",
      "[Epoch 134/200] [Batch 1000/4688] [D loss: 0.099461] [G loss: 0.778955]\n",
      "[Epoch 134/200] [Batch 2000/4688] [D loss: 0.100665] [G loss: 0.740119]\n",
      "[Epoch 134/200] [Batch 3000/4688] [D loss: 0.073458] [G loss: 0.676168]\n",
      "[Epoch 134/200] [Batch 4000/4688] [D loss: 0.088674] [G loss: 0.889585]\n",
      "[Epoch 135/200] [Batch 0/4688] [D loss: 0.073731] [G loss: 0.828141]\n",
      "[Epoch 135/200] [Batch 1000/4688] [D loss: 0.093995] [G loss: 0.656516]\n",
      "[Epoch 135/200] [Batch 2000/4688] [D loss: 0.159292] [G loss: 0.442208]\n",
      "[Epoch 135/200] [Batch 3000/4688] [D loss: 0.098381] [G loss: 0.959080]\n",
      "[Epoch 135/200] [Batch 4000/4688] [D loss: 0.089181] [G loss: 0.690834]\n",
      "[Epoch 136/200] [Batch 0/4688] [D loss: 0.081786] [G loss: 0.693730]\n",
      "[Epoch 136/200] [Batch 1000/4688] [D loss: 0.113596] [G loss: 0.637849]\n",
      "[Epoch 136/200] [Batch 2000/4688] [D loss: 0.134581] [G loss: 0.631773]\n",
      "[Epoch 136/200] [Batch 3000/4688] [D loss: 0.098182] [G loss: 0.616490]\n",
      "[Epoch 136/200] [Batch 4000/4688] [D loss: 0.117008] [G loss: 0.642210]\n",
      "[Epoch 137/200] [Batch 0/4688] [D loss: 0.111821] [G loss: 0.662938]\n",
      "[Epoch 137/200] [Batch 1000/4688] [D loss: 0.126705] [G loss: 0.469429]\n",
      "[Epoch 137/200] [Batch 2000/4688] [D loss: 0.099235] [G loss: 0.716813]\n",
      "[Epoch 137/200] [Batch 3000/4688] [D loss: 0.074775] [G loss: 0.667988]\n",
      "[Epoch 137/200] [Batch 4000/4688] [D loss: 0.172444] [G loss: 0.331228]\n",
      "[Epoch 138/200] [Batch 0/4688] [D loss: 0.114829] [G loss: 0.615541]\n",
      "[Epoch 138/200] [Batch 1000/4688] [D loss: 0.124004] [G loss: 0.917767]\n",
      "[Epoch 138/200] [Batch 2000/4688] [D loss: 0.079921] [G loss: 0.774963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 138/200] [Batch 3000/4688] [D loss: 0.102471] [G loss: 0.630278]\n",
      "[Epoch 138/200] [Batch 4000/4688] [D loss: 0.104251] [G loss: 0.580227]\n",
      "[Epoch 139/200] [Batch 0/4688] [D loss: 0.169003] [G loss: 0.381224]\n",
      "[Epoch 139/200] [Batch 1000/4688] [D loss: 0.098583] [G loss: 0.706650]\n",
      "[Epoch 139/200] [Batch 2000/4688] [D loss: 0.102139] [G loss: 0.702117]\n",
      "[Epoch 139/200] [Batch 3000/4688] [D loss: 0.123758] [G loss: 0.664964]\n",
      "[Epoch 139/200] [Batch 4000/4688] [D loss: 0.079095] [G loss: 0.810292]\n",
      "[Epoch 140/200] [Batch 0/4688] [D loss: 0.082351] [G loss: 0.717705]\n",
      "[Epoch 140/200] [Batch 1000/4688] [D loss: 0.097082] [G loss: 0.568859]\n",
      "[Epoch 140/200] [Batch 2000/4688] [D loss: 0.094635] [G loss: 0.629803]\n",
      "[Epoch 140/200] [Batch 3000/4688] [D loss: 0.094916] [G loss: 0.740366]\n",
      "[Epoch 140/200] [Batch 4000/4688] [D loss: 0.092689] [G loss: 0.632232]\n",
      "[Epoch 141/200] [Batch 0/4688] [D loss: 0.107047] [G loss: 0.555655]\n",
      "[Epoch 141/200] [Batch 1000/4688] [D loss: 0.084984] [G loss: 0.708300]\n",
      "[Epoch 141/200] [Batch 2000/4688] [D loss: 0.098721] [G loss: 0.785752]\n",
      "[Epoch 141/200] [Batch 3000/4688] [D loss: 0.115653] [G loss: 0.499459]\n",
      "[Epoch 141/200] [Batch 4000/4688] [D loss: 0.114427] [G loss: 0.635030]\n",
      "[Epoch 142/200] [Batch 0/4688] [D loss: 0.070765] [G loss: 0.609973]\n",
      "[Epoch 142/200] [Batch 1000/4688] [D loss: 0.114675] [G loss: 0.545962]\n",
      "[Epoch 142/200] [Batch 2000/4688] [D loss: 0.142830] [G loss: 0.536562]\n",
      "[Epoch 142/200] [Batch 3000/4688] [D loss: 0.131120] [G loss: 0.626102]\n",
      "[Epoch 142/200] [Batch 4000/4688] [D loss: 0.117306] [G loss: 0.559136]\n",
      "[Epoch 143/200] [Batch 0/4688] [D loss: 0.100540] [G loss: 0.673847]\n",
      "[Epoch 143/200] [Batch 1000/4688] [D loss: 0.108380] [G loss: 0.690651]\n",
      "[Epoch 143/200] [Batch 2000/4688] [D loss: 0.087266] [G loss: 0.873405]\n",
      "[Epoch 143/200] [Batch 3000/4688] [D loss: 0.095289] [G loss: 0.582881]\n",
      "[Epoch 143/200] [Batch 4000/4688] [D loss: 0.091648] [G loss: 0.593493]\n",
      "[Epoch 144/200] [Batch 0/4688] [D loss: 0.095761] [G loss: 0.810134]\n",
      "[Epoch 144/200] [Batch 1000/4688] [D loss: 0.131979] [G loss: 0.573803]\n",
      "[Epoch 144/200] [Batch 2000/4688] [D loss: 0.084518] [G loss: 0.757931]\n",
      "[Epoch 144/200] [Batch 3000/4688] [D loss: 0.089677] [G loss: 0.681609]\n",
      "[Epoch 144/200] [Batch 4000/4688] [D loss: 0.083233] [G loss: 0.706627]\n",
      "[Epoch 145/200] [Batch 0/4688] [D loss: 0.087804] [G loss: 0.698975]\n",
      "[Epoch 145/200] [Batch 1000/4688] [D loss: 0.084010] [G loss: 0.583344]\n",
      "[Epoch 145/200] [Batch 2000/4688] [D loss: 0.104521] [G loss: 0.585131]\n",
      "[Epoch 145/200] [Batch 3000/4688] [D loss: 0.085494] [G loss: 0.638337]\n",
      "[Epoch 145/200] [Batch 4000/4688] [D loss: 0.118349] [G loss: 0.703513]\n",
      "[Epoch 146/200] [Batch 0/4688] [D loss: 0.134607] [G loss: 0.546546]\n",
      "[Epoch 146/200] [Batch 1000/4688] [D loss: 0.110016] [G loss: 0.648643]\n",
      "[Epoch 146/200] [Batch 2000/4688] [D loss: 0.083758] [G loss: 0.674577]\n",
      "[Epoch 146/200] [Batch 3000/4688] [D loss: 0.086948] [G loss: 0.667609]\n",
      "[Epoch 146/200] [Batch 4000/4688] [D loss: 0.115108] [G loss: 0.865824]\n",
      "[Epoch 147/200] [Batch 0/4688] [D loss: 0.098735] [G loss: 0.856545]\n",
      "[Epoch 147/200] [Batch 1000/4688] [D loss: 0.106179] [G loss: 0.771177]\n",
      "[Epoch 147/200] [Batch 2000/4688] [D loss: 0.116510] [G loss: 0.494641]\n",
      "[Epoch 147/200] [Batch 3000/4688] [D loss: 0.086870] [G loss: 0.716080]\n",
      "[Epoch 147/200] [Batch 4000/4688] [D loss: 0.073582] [G loss: 0.810619]\n",
      "[Epoch 148/200] [Batch 0/4688] [D loss: 0.131997] [G loss: 0.506223]\n",
      "[Epoch 148/200] [Batch 1000/4688] [D loss: 0.080783] [G loss: 0.888894]\n",
      "[Epoch 148/200] [Batch 2000/4688] [D loss: 0.075751] [G loss: 0.604411]\n",
      "[Epoch 148/200] [Batch 3000/4688] [D loss: 0.104707] [G loss: 0.791580]\n",
      "[Epoch 148/200] [Batch 4000/4688] [D loss: 0.089124] [G loss: 0.819774]\n",
      "[Epoch 149/200] [Batch 0/4688] [D loss: 0.114359] [G loss: 0.542696]\n",
      "[Epoch 149/200] [Batch 1000/4688] [D loss: 0.098823] [G loss: 0.702200]\n",
      "[Epoch 149/200] [Batch 2000/4688] [D loss: 0.075284] [G loss: 0.805582]\n",
      "[Epoch 149/200] [Batch 3000/4688] [D loss: 0.097403] [G loss: 0.778432]\n",
      "[Epoch 149/200] [Batch 4000/4688] [D loss: 0.073138] [G loss: 0.863306]\n",
      "[Epoch 150/200] [Batch 0/4688] [D loss: 0.112707] [G loss: 0.621700]\n",
      "[Epoch 150/200] [Batch 1000/4688] [D loss: 0.102979] [G loss: 1.024717]\n",
      "[Epoch 150/200] [Batch 2000/4688] [D loss: 0.097711] [G loss: 0.928364]\n",
      "[Epoch 150/200] [Batch 3000/4688] [D loss: 0.114211] [G loss: 0.776608]\n",
      "[Epoch 150/200] [Batch 4000/4688] [D loss: 0.090470] [G loss: 0.685961]\n",
      "[Epoch 151/200] [Batch 0/4688] [D loss: 0.106969] [G loss: 0.832525]\n",
      "[Epoch 151/200] [Batch 1000/4688] [D loss: 0.083896] [G loss: 0.692182]\n",
      "[Epoch 151/200] [Batch 2000/4688] [D loss: 0.081684] [G loss: 0.881874]\n",
      "[Epoch 151/200] [Batch 3000/4688] [D loss: 0.088589] [G loss: 0.746577]\n",
      "[Epoch 151/200] [Batch 4000/4688] [D loss: 0.104244] [G loss: 0.642475]\n",
      "[Epoch 152/200] [Batch 0/4688] [D loss: 0.084260] [G loss: 0.740476]\n",
      "[Epoch 152/200] [Batch 1000/4688] [D loss: 0.083185] [G loss: 0.758462]\n",
      "[Epoch 152/200] [Batch 2000/4688] [D loss: 0.075286] [G loss: 0.863468]\n",
      "[Epoch 152/200] [Batch 3000/4688] [D loss: 0.114919] [G loss: 0.769916]\n",
      "[Epoch 152/200] [Batch 4000/4688] [D loss: 0.091341] [G loss: 0.761811]\n",
      "[Epoch 153/200] [Batch 0/4688] [D loss: 0.096972] [G loss: 0.770726]\n",
      "[Epoch 153/200] [Batch 1000/4688] [D loss: 0.091913] [G loss: 0.578099]\n",
      "[Epoch 153/200] [Batch 2000/4688] [D loss: 0.106194] [G loss: 0.827700]\n",
      "[Epoch 153/200] [Batch 3000/4688] [D loss: 0.085941] [G loss: 0.828766]\n",
      "[Epoch 153/200] [Batch 4000/4688] [D loss: 0.146892] [G loss: 0.454095]\n",
      "[Epoch 154/200] [Batch 0/4688] [D loss: 0.072619] [G loss: 0.910229]\n",
      "[Epoch 154/200] [Batch 1000/4688] [D loss: 0.094746] [G loss: 0.529148]\n",
      "[Epoch 154/200] [Batch 2000/4688] [D loss: 0.099761] [G loss: 0.426247]\n",
      "[Epoch 154/200] [Batch 3000/4688] [D loss: 0.087640] [G loss: 0.777432]\n",
      "[Epoch 154/200] [Batch 4000/4688] [D loss: 0.106765] [G loss: 0.845272]\n",
      "[Epoch 155/200] [Batch 0/4688] [D loss: 0.103095] [G loss: 0.851515]\n",
      "[Epoch 155/200] [Batch 1000/4688] [D loss: 0.102946] [G loss: 0.564074]\n",
      "[Epoch 155/200] [Batch 2000/4688] [D loss: 0.089885] [G loss: 0.504947]\n",
      "[Epoch 155/200] [Batch 3000/4688] [D loss: 0.074974] [G loss: 0.783000]\n",
      "[Epoch 155/200] [Batch 4000/4688] [D loss: 0.067548] [G loss: 0.858120]\n",
      "[Epoch 156/200] [Batch 0/4688] [D loss: 0.115572] [G loss: 0.531359]\n",
      "[Epoch 156/200] [Batch 1000/4688] [D loss: 0.097347] [G loss: 0.640830]\n",
      "[Epoch 156/200] [Batch 2000/4688] [D loss: 0.074297] [G loss: 0.726296]\n",
      "[Epoch 156/200] [Batch 3000/4688] [D loss: 0.097507] [G loss: 0.798566]\n",
      "[Epoch 156/200] [Batch 4000/4688] [D loss: 0.078237] [G loss: 0.654345]\n",
      "[Epoch 157/200] [Batch 0/4688] [D loss: 0.097362] [G loss: 0.647071]\n",
      "[Epoch 157/200] [Batch 1000/4688] [D loss: 0.067212] [G loss: 0.872728]\n",
      "[Epoch 157/200] [Batch 2000/4688] [D loss: 0.149550] [G loss: 0.657743]\n",
      "[Epoch 157/200] [Batch 3000/4688] [D loss: 0.114367] [G loss: 0.704464]\n",
      "[Epoch 157/200] [Batch 4000/4688] [D loss: 0.090780] [G loss: 0.901209]\n",
      "[Epoch 158/200] [Batch 0/4688] [D loss: 0.100053] [G loss: 0.638869]\n",
      "[Epoch 158/200] [Batch 1000/4688] [D loss: 0.105977] [G loss: 0.535233]\n",
      "[Epoch 158/200] [Batch 2000/4688] [D loss: 0.080157] [G loss: 0.743571]\n",
      "[Epoch 158/200] [Batch 3000/4688] [D loss: 0.086681] [G loss: 0.678263]\n",
      "[Epoch 158/200] [Batch 4000/4688] [D loss: 0.113609] [G loss: 0.644320]\n",
      "[Epoch 159/200] [Batch 0/4688] [D loss: 0.131799] [G loss: 0.583161]\n",
      "[Epoch 159/200] [Batch 1000/4688] [D loss: 0.088215] [G loss: 0.756292]\n",
      "[Epoch 159/200] [Batch 2000/4688] [D loss: 0.088737] [G loss: 0.797775]\n",
      "[Epoch 159/200] [Batch 3000/4688] [D loss: 0.069570] [G loss: 0.872929]\n",
      "[Epoch 159/200] [Batch 4000/4688] [D loss: 0.075110] [G loss: 0.707963]\n",
      "[Epoch 160/200] [Batch 0/4688] [D loss: 0.090667] [G loss: 0.832463]\n",
      "[Epoch 160/200] [Batch 1000/4688] [D loss: 0.096229] [G loss: 0.616755]\n",
      "[Epoch 160/200] [Batch 2000/4688] [D loss: 0.083895] [G loss: 0.662028]\n",
      "[Epoch 160/200] [Batch 3000/4688] [D loss: 0.101784] [G loss: 0.433029]\n",
      "[Epoch 160/200] [Batch 4000/4688] [D loss: 0.112972] [G loss: 0.899114]\n",
      "[Epoch 161/200] [Batch 0/4688] [D loss: 0.078426] [G loss: 0.600854]\n",
      "[Epoch 161/200] [Batch 1000/4688] [D loss: 0.088173] [G loss: 0.617618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 161/200] [Batch 2000/4688] [D loss: 0.085809] [G loss: 0.644749]\n",
      "[Epoch 161/200] [Batch 3000/4688] [D loss: 0.074438] [G loss: 0.708595]\n",
      "[Epoch 161/200] [Batch 4000/4688] [D loss: 0.095023] [G loss: 0.880102]\n",
      "[Epoch 162/200] [Batch 0/4688] [D loss: 0.071847] [G loss: 0.894380]\n",
      "[Epoch 162/200] [Batch 1000/4688] [D loss: 0.093451] [G loss: 0.878483]\n",
      "[Epoch 162/200] [Batch 2000/4688] [D loss: 0.081106] [G loss: 0.645998]\n",
      "[Epoch 162/200] [Batch 3000/4688] [D loss: 0.084824] [G loss: 0.849919]\n",
      "[Epoch 162/200] [Batch 4000/4688] [D loss: 0.081470] [G loss: 0.783135]\n",
      "[Epoch 163/200] [Batch 0/4688] [D loss: 0.086329] [G loss: 0.843010]\n",
      "[Epoch 163/200] [Batch 1000/4688] [D loss: 0.086074] [G loss: 0.692192]\n",
      "[Epoch 163/200] [Batch 2000/4688] [D loss: 0.082936] [G loss: 0.747461]\n",
      "[Epoch 163/200] [Batch 3000/4688] [D loss: 0.123575] [G loss: 0.950116]\n",
      "[Epoch 163/200] [Batch 4000/4688] [D loss: 0.094227] [G loss: 0.715355]\n",
      "[Epoch 164/200] [Batch 0/4688] [D loss: 0.085429] [G loss: 0.711391]\n",
      "[Epoch 164/200] [Batch 1000/4688] [D loss: 0.086664] [G loss: 0.766121]\n",
      "[Epoch 164/200] [Batch 2000/4688] [D loss: 0.078731] [G loss: 0.704011]\n",
      "[Epoch 164/200] [Batch 3000/4688] [D loss: 0.109828] [G loss: 0.750863]\n",
      "[Epoch 164/200] [Batch 4000/4688] [D loss: 0.094925] [G loss: 0.702659]\n",
      "[Epoch 165/200] [Batch 0/4688] [D loss: 0.095323] [G loss: 0.554533]\n",
      "[Epoch 165/200] [Batch 1000/4688] [D loss: 0.112119] [G loss: 0.446513]\n",
      "[Epoch 165/200] [Batch 2000/4688] [D loss: 0.087375] [G loss: 0.691806]\n",
      "[Epoch 165/200] [Batch 3000/4688] [D loss: 0.097695] [G loss: 0.816111]\n",
      "[Epoch 165/200] [Batch 4000/4688] [D loss: 0.127896] [G loss: 0.929901]\n",
      "[Epoch 166/200] [Batch 0/4688] [D loss: 0.093448] [G loss: 0.888750]\n",
      "[Epoch 166/200] [Batch 1000/4688] [D loss: 0.078439] [G loss: 0.811974]\n",
      "[Epoch 166/200] [Batch 2000/4688] [D loss: 0.085252] [G loss: 0.940626]\n",
      "[Epoch 166/200] [Batch 3000/4688] [D loss: 0.104142] [G loss: 0.781236]\n",
      "[Epoch 166/200] [Batch 4000/4688] [D loss: 0.069816] [G loss: 0.830416]\n",
      "[Epoch 167/200] [Batch 0/4688] [D loss: 0.121065] [G loss: 0.815879]\n",
      "[Epoch 167/200] [Batch 1000/4688] [D loss: 0.090035] [G loss: 0.723479]\n",
      "[Epoch 167/200] [Batch 2000/4688] [D loss: 0.102041] [G loss: 0.845070]\n",
      "[Epoch 167/200] [Batch 3000/4688] [D loss: 0.056258] [G loss: 0.830193]\n",
      "[Epoch 167/200] [Batch 4000/4688] [D loss: 0.082647] [G loss: 0.737563]\n",
      "[Epoch 168/200] [Batch 0/4688] [D loss: 0.092590] [G loss: 0.874237]\n",
      "[Epoch 168/200] [Batch 1000/4688] [D loss: 0.100893] [G loss: 0.635922]\n",
      "[Epoch 168/200] [Batch 2000/4688] [D loss: 0.086975] [G loss: 0.562423]\n",
      "[Epoch 168/200] [Batch 3000/4688] [D loss: 0.080263] [G loss: 0.556714]\n",
      "[Epoch 168/200] [Batch 4000/4688] [D loss: 0.092322] [G loss: 0.698500]\n",
      "[Epoch 169/200] [Batch 0/4688] [D loss: 0.090867] [G loss: 0.575006]\n",
      "[Epoch 169/200] [Batch 1000/4688] [D loss: 0.093067] [G loss: 0.678312]\n",
      "[Epoch 169/200] [Batch 2000/4688] [D loss: 0.101047] [G loss: 0.698838]\n",
      "[Epoch 169/200] [Batch 3000/4688] [D loss: 0.058393] [G loss: 0.714042]\n",
      "[Epoch 169/200] [Batch 4000/4688] [D loss: 0.070879] [G loss: 0.722908]\n",
      "[Epoch 170/200] [Batch 0/4688] [D loss: 0.071822] [G loss: 0.789344]\n",
      "[Epoch 170/200] [Batch 1000/4688] [D loss: 0.070526] [G loss: 0.710352]\n",
      "[Epoch 170/200] [Batch 2000/4688] [D loss: 0.065860] [G loss: 0.790738]\n",
      "[Epoch 170/200] [Batch 3000/4688] [D loss: 0.059446] [G loss: 0.715460]\n",
      "[Epoch 170/200] [Batch 4000/4688] [D loss: 0.077747] [G loss: 0.657854]\n",
      "[Epoch 171/200] [Batch 0/4688] [D loss: 0.087682] [G loss: 1.061433]\n",
      "[Epoch 171/200] [Batch 1000/4688] [D loss: 0.082929] [G loss: 0.790727]\n",
      "[Epoch 171/200] [Batch 2000/4688] [D loss: 0.092796] [G loss: 0.912395]\n",
      "[Epoch 171/200] [Batch 3000/4688] [D loss: 0.102117] [G loss: 0.670366]\n",
      "[Epoch 171/200] [Batch 4000/4688] [D loss: 0.092222] [G loss: 0.568235]\n",
      "[Epoch 172/200] [Batch 0/4688] [D loss: 0.083933] [G loss: 0.655566]\n",
      "[Epoch 172/200] [Batch 1000/4688] [D loss: 0.120369] [G loss: 0.949258]\n",
      "[Epoch 172/200] [Batch 2000/4688] [D loss: 0.082581] [G loss: 0.600040]\n",
      "[Epoch 172/200] [Batch 3000/4688] [D loss: 0.092651] [G loss: 0.732377]\n",
      "[Epoch 172/200] [Batch 4000/4688] [D loss: 0.101019] [G loss: 1.125278]\n",
      "[Epoch 173/200] [Batch 0/4688] [D loss: 0.103839] [G loss: 0.834291]\n",
      "[Epoch 173/200] [Batch 1000/4688] [D loss: 0.068675] [G loss: 0.869223]\n",
      "[Epoch 173/200] [Batch 2000/4688] [D loss: 0.074514] [G loss: 0.866638]\n",
      "[Epoch 173/200] [Batch 3000/4688] [D loss: 0.111209] [G loss: 1.082523]\n",
      "[Epoch 173/200] [Batch 4000/4688] [D loss: 0.073966] [G loss: 0.710505]\n",
      "[Epoch 174/200] [Batch 0/4688] [D loss: 0.076386] [G loss: 0.692477]\n",
      "[Epoch 174/200] [Batch 1000/4688] [D loss: 0.073049] [G loss: 0.724373]\n",
      "[Epoch 174/200] [Batch 2000/4688] [D loss: 0.073937] [G loss: 0.864747]\n",
      "[Epoch 174/200] [Batch 3000/4688] [D loss: 0.088401] [G loss: 0.759969]\n",
      "[Epoch 174/200] [Batch 4000/4688] [D loss: 0.063746] [G loss: 0.751793]\n",
      "[Epoch 175/200] [Batch 0/4688] [D loss: 0.086187] [G loss: 0.628354]\n",
      "[Epoch 175/200] [Batch 1000/4688] [D loss: 0.092589] [G loss: 0.939588]\n",
      "[Epoch 175/200] [Batch 2000/4688] [D loss: 0.075034] [G loss: 0.787645]\n",
      "[Epoch 175/200] [Batch 3000/4688] [D loss: 0.068346] [G loss: 0.769843]\n",
      "[Epoch 175/200] [Batch 4000/4688] [D loss: 0.051606] [G loss: 0.852901]\n",
      "[Epoch 176/200] [Batch 0/4688] [D loss: 0.108914] [G loss: 0.441989]\n",
      "[Epoch 176/200] [Batch 1000/4688] [D loss: 0.076409] [G loss: 0.813492]\n",
      "[Epoch 176/200] [Batch 2000/4688] [D loss: 0.108946] [G loss: 0.891044]\n",
      "[Epoch 176/200] [Batch 3000/4688] [D loss: 0.089079] [G loss: 0.820615]\n",
      "[Epoch 176/200] [Batch 4000/4688] [D loss: 0.088900] [G loss: 0.592544]\n",
      "[Epoch 177/200] [Batch 0/4688] [D loss: 0.121410] [G loss: 0.423392]\n",
      "[Epoch 177/200] [Batch 1000/4688] [D loss: 0.066720] [G loss: 0.688287]\n",
      "[Epoch 177/200] [Batch 2000/4688] [D loss: 0.079057] [G loss: 0.888017]\n",
      "[Epoch 177/200] [Batch 3000/4688] [D loss: 0.087856] [G loss: 0.713549]\n",
      "[Epoch 177/200] [Batch 4000/4688] [D loss: 0.079776] [G loss: 0.664161]\n",
      "[Epoch 178/200] [Batch 0/4688] [D loss: 0.064764] [G loss: 0.807017]\n",
      "[Epoch 178/200] [Batch 1000/4688] [D loss: 0.079889] [G loss: 0.787449]\n",
      "[Epoch 178/200] [Batch 2000/4688] [D loss: 0.080890] [G loss: 0.755694]\n",
      "[Epoch 178/200] [Batch 3000/4688] [D loss: 0.077594] [G loss: 0.786997]\n",
      "[Epoch 178/200] [Batch 4000/4688] [D loss: 0.085476] [G loss: 0.800128]\n",
      "[Epoch 179/200] [Batch 0/4688] [D loss: 0.076204] [G loss: 0.891996]\n",
      "[Epoch 179/200] [Batch 1000/4688] [D loss: 0.080480] [G loss: 0.717487]\n",
      "[Epoch 179/200] [Batch 2000/4688] [D loss: 0.096258] [G loss: 0.544167]\n",
      "[Epoch 179/200] [Batch 3000/4688] [D loss: 0.117429] [G loss: 0.577571]\n",
      "[Epoch 179/200] [Batch 4000/4688] [D loss: 0.069355] [G loss: 0.714525]\n",
      "[Epoch 180/200] [Batch 0/4688] [D loss: 0.084699] [G loss: 1.083323]\n",
      "[Epoch 180/200] [Batch 1000/4688] [D loss: 0.081397] [G loss: 0.701183]\n",
      "[Epoch 180/200] [Batch 2000/4688] [D loss: 0.077649] [G loss: 0.892943]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9ab9e2339d4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mg_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0moptimizer_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;31m# ---------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(\"images/50\", exist_ok=True)\n",
    "\n",
    "\n",
    "digit_embeddings = np.load(\"digit_embeddings.npy\")\n",
    "\n",
    "class Arguments():\n",
    "  def __init__(self):\n",
    "    self.n_epochs = 200\n",
    "    self.batch_size = 64\n",
    "    self.lr = 0.0002\n",
    "    self.b1 = 0.5\n",
    "    self.b2 = 0.999\n",
    "    self.n_cpu = 8\n",
    "    self.latent_dim = 100\n",
    "    self.n_classes = 50\n",
    "    self.img_size = 32\n",
    "    self.channels = 1\n",
    "    self.sample_interval = 1000\n",
    "    self.embedding_size = 50\n",
    "\n",
    "opt = Arguments()\n",
    "\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size*2)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Linear(opt.embedding_size, opt.embedding_size)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.embedding_size, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Linear(opt.embedding_size, opt.embedding_size)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.embedding_size + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n",
    "\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Configure data loader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    CustomTensorDataset(tensors = (x, y), transform = transforms.Compose(\n",
    "            [transforms.ToPILImage(), transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "       )),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "#\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "#numbers = np.random.randint(0, 49, [10])\n",
    "#numbers = np.sort(numbers)\n",
    "numbers = [1, 22, 23, 26, 29, 33, 35, 37, 42, 48]\n",
    "print(\"Numbers\", numbers)\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    \"\"\"does not use n_row\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (10*10, opt.latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(10) for num in numbers])\n",
    "    gen_labels = Variable(FloatTensor(digit_embeddings[labels]))\n",
    "    gen_imgs = generator(z, gen_labels)\n",
    "    save_image(gen_imgs.data, \"images/50/%d.png\" % batches_done, nrow=10, normalize=True)\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(FloatTensor(digit_embeddings[labels]))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
    "        gen_labels = Variable(FloatTensor(digit_embeddings[np.random.randint(0, opt.n_classes, batch_size)]))\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "        \n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        validity_real = discriminator(real_imgs, labels)\n",
    "        d_real_loss = adversarial_loss(validity_real, valid)\n",
    "\n",
    "        # Loss for fake images\n",
    "        validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        d_fake_loss = adversarial_loss(validity_fake, fake)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        if i%1000==0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            sample_image(n_row=20, batches_done=batches_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
